# main.py
from __future__ import annotations

import datetime as dt
import json
import os
import re
import traceback
import random
from typing import Optional, List, Dict, Any, Tuple
import uuid

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from supabase import create_client, Client

import uuid
from ai_moderator import moderate_text
from llm_prompts import REPORT_SUMMARY_PROMPT, call_llm, get_system_prompt, TRIAGE_SYSTEM_PROMPT, FRIENDLY_SYSTEM_PROMPT 
from rule_based import rule_scoring
from srj5_constants import (
    CLUSTER_TO_DISPLAY_NAME, CLUSTERS, DEEP_DIVE_MAX_SCORES, EMOJI_ONLY_SCORE_CAP, FINAL_FUSION_WEIGHTS_NO_ICON, ICON_TO_CLUSTER, ONBOARDING_MAPPING,
    FINAL_FUSION_WEIGHTS, FINAL_FUSION_WEIGHTS_NO_TEXT,
    W_LLM, W_RULE, 
    SAFETY_LEMMAS, SAFETY_LEMMA_COMBOS, SAFETY_REGEX, SAFETY_FIGURATIVE
)


try:
    from kiwipiepy import Kiwi
    _kiwi = Kiwi()
except ImportError:
    _kiwi = None
    print("âš ï¸ kiwipiepy is not installed. Some safety features will be disabled.")


# --- í™˜ê²½ì„¤ì • ---
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
# BIND_HOST = os.getenv("BIND_HOST", "0.0.0.0")
# PORT = int(os.getenv("PORT", "8000"))
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
# Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
supabase: Optional[Client] = None


# --- FastAPI ì•± ì´ˆê¸°í™” ë° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
app = FastAPI(title="DailyMoji API v2 (Separated Logic)")

# supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@app.on_event("startup")
def startup_event():
    global supabase
    if SUPABASE_URL and SUPABASE_KEY:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… FastAPI server started and Supabase client initialized.")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- ë°ì´í„° ëª¨ë¸ (ë¶„ë¦¬ëœ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •) ---


# ì´ì „ ëŒ€í™” ê¸°ë¡(history)ì„ ë°›ê¸° ìœ„í•œ ëª¨ë¸ ìˆ˜ì •
class HistoryItem(BaseModel):
    sender: str
    content: str

# /analyze ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AnalyzeRequest(BaseModel):
    user_id: str
    text: str
    icon: Optional[str] = None
    language_code: Optional[str] = 'ko'
    timestamp: Optional[str] = None
    onboarding: Optional[Dict[str, Any]] = None
    character_personality: Optional[str] = None
    history: Optional[List[HistoryItem]] = None # history í•„ë“œ ì¶”ê°€


# /solutions/propose ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class SolutionRequest(BaseModel):
    user_id: str
    session_id: str
    top_cluster: str
    language_code: str = 'ko'


# /assessment/submit ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AssessmentSubmitRequest(BaseModel):
    user_id: str
    cluster: str  # "neg_low", "neg_high" ë“± í‰ê°€í•œ í´ëŸ¬ìŠ¤í„°
    responses: Dict[str, int] # {"NGL_01": 3, "NGL_02": 2, ...} í˜•íƒœì˜ ë‹µë³€


# Flutterì˜ PresetIdsì™€ ë™ì¼í•œ êµ¬ì¡°
class PresetIds:
    FRIENDLY_REPLY = "FRIENDLY_REPLY"
    SOLUTION_PROPOSAL = "SOLUTION_PROPOSAL"
    SAFETY_CRISIS_MODAL = "SAFETY_CRISIS_MODAL"
    EMOJI_REACTION = "EMOJI_REACTION"

# ======================================================================
# === kiwië¥¼ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ ì¥ì¹˜ ===
# ======================================================================

# --- ì•ˆì „ ì¥ì¹˜ ë¡œì§ ---

def _find_regex_matches(text: str, patterns: List[str]) -> List[str]:
    return [m.group(0) for pat in patterns for m in re.finditer(pat, text, flags=re.IGNORECASE)]

def _kiwi_detect_safety_lemmas(text: str) -> List[str]:
    if not _kiwi: return []
    try:
        tokens = _kiwi.tokenize(text)
        all_lemmas_in_text = {t.lemma for t in tokens}
        hits = {token.lemma for token in tokens if token.lemma in SAFETY_LEMMAS}
        for combo in SAFETY_LEMMA_COMBOS:
            if combo.issubset(all_lemmas_in_text):
                hits.update(combo)
        return list(hits)
    except Exception as e:
        print(f"Kiwi safety check error: {e}")
        return []

def is_safety_text(text: str, llm_json:  Optional[dict], debug_log: dict) -> Tuple[bool, dict]:
    """
    ì ìˆ˜ ì²´ê³„ ë° ì•ˆì „ ëª¨ë“œ ì„¤ëª…: ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì—ì„œ ìí•´/ìì‚´ ìœ„í—˜ì„ ë‹¤ë‹¨ê³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    - 1ë‹¨ê³„: "ì¡¸ë ¤ ì£½ê² ë‹¤"ì™€ ê°™ì´ ëª…ë°±íˆ ì•ˆì „í•œ ë¹„ìœ ì  í‘œí˜„ì„ ë¨¼ì € ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
    - 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„('ì£½ë‹¤', 'ìì‚´' ë“±)ê³¼ LLMì˜ ì˜ë„ ë¶„ì„('self_harm' í”Œë˜ê·¸)ìœ¼ë¡œ ìœ„í—˜ ì‹ í˜¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    - "ë‹¤ ë•Œë ¤ì¹˜ìš°ê³  ì‹¶ë‹¤"ì™€ ê°™ì€ ë¬¸ì¥ì€ ëª…ì‹œì ì¸ ìí•´ ë‹¨ì–´ê°€ ì—†ì–´ 1, 2ë‹¨ê³„ë¥¼ í†µê³¼í•  ìˆ˜ ìˆì§€ë§Œ,
      LLMì´ ë¬¸ì¥ì˜ ì ˆë§ì ì¸ ë‰˜ì•™ìŠ¤ë¥¼ 'self_harm: possible'ë¡œ íŒë‹¨í•˜ë©´ ì•ˆì „ ì¥ì¹˜ê°€ ë°œë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ì´ëŸ¬í•œ ì˜¤íƒì§€ëŠ” ëª¨ë¸ì˜ ë³´ìˆ˜ì ì¸ ì•ˆì „ ì„¤ê³„ ë•Œë¬¸ì´ë©°, ì§€ì†ì ì¸ í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """

    # 1ë‹¨ê³„: ë¹„ìœ ì /ê´€ìš©ì  í‘œí˜„ ìš°ì„  í•„í„°ë§ ("ì¡¸ë ¤ ì£½ê² ë‹¤" ë“±)
    # SAFETY_FIGURATIVEì— ë§¤ì¹˜ë˜ë©´, ìœ„í—˜í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ ì¢…ë£Œ
    figurative_matches = [m.group(0) for pat in SAFETY_FIGURATIVE for m in re.finditer(pat, text, flags=re.IGNORECASE)]
    if figurative_matches:
        debug_log["safety"] = {
            "triggered": False,
            "reason": "Figurative speech detected",
            "matches": figurative_matches
        }
        return False, {}

    # 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„ ë° LLM ì˜ë„ ë¶„ì„
    kiwi_lemma_hits = []
    if _kiwi:
        try:
            tokens = _kiwi.tokenize(text)
            lemmas_in_text = {t.lemma for t in tokens}
            hits = {t.lemma for t in tokens if t.lemma in SAFETY_LEMMAS}
            for combo in SAFETY_LEMMA_COMBOS:
                if combo.issubset(lemmas_in_text):
                    hits.update(combo)
            kiwi_lemma_hits = list(hits)
        except Exception as e:
            print(f"Kiwi safety check error: {e}")

    safety_llm_flag = (llm_json or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}
    
    # Kiwi ë˜ëŠ” LLM ì¤‘ í•˜ë‚˜ë¼ë„ ìœ„í—˜ ì‹ í˜¸ë¥¼ ê°ì§€í•˜ë©´ ì•ˆì „ì¥ì¹˜ ë°œë™
    triggered = bool(kiwi_lemma_hits) or safety_llm_flag

    debug_log["safety"] = {
        "kiwi_lemma_hits": kiwi_lemma_hits,
        "llm_intent_flag": safety_llm_flag,
        "triggered": triggered
    }

    if triggered:
        # ìœ„ê¸° ìƒí™©ì— ë§ëŠ” ê·¹ë‹¨ì ì¸ ì ìˆ˜ ë¶€ì—¬
        crisis_scores = {"neg_low": 0.95, "neg_high": 0.0, "adhd": 0.0, "sleep": 0.0, "positive": 0.0}
        return True, crisis_scores

    return False, {}



# ======================================================================
# === í•µì‹¬ ë¡œì§: ìŠ¤ì½”ì–´ë§ ë° ìœµí•© (Scoring & Fusion) ===
# ======================================================================

def calculate_final_scores(
    text_scores: dict,
    assessment_scores: dict,
    icon_scores: dict,
    has_icon: bool
) -> Tuple[dict, dict]:
    """í…ìŠ¤íŠ¸, ë§ˆìŒ ì ê²€, ì•„ì´ì½˜ ì ìˆ˜ë¥¼ ì¤‘ì•™ì—ì„œ ìœµí•©í•©ë‹ˆë‹¤."""
    if not has_icon:
        # CASE 1: í…ìŠ¤íŠ¸ë§Œ ì…ë ¥ ì‹œ -> ì•„ì´ì½˜ ê°€ì¤‘ì¹˜ë¥¼ ë¹„ë¡€ ë°°ë¶„
        w = FINAL_FUSION_WEIGHTS_NO_ICON
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": 0.0}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment']
        ) for c in CLUSTERS}
    else:
        # CASE 2: í…ìŠ¤íŠ¸ + ì•„ì´ì½˜ ì…ë ¥ ì‹œ -> ëª¨ë“  ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        w = FINAL_FUSION_WEIGHTS
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": w['icon']}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment'] +
            icon_scores.get(c, 0.0) * w['icon']
        ) for c in CLUSTERS}

    return final_scores, weights_used

def calculate_text_scores(text: str, llm_json: Optional[dict]) -> dict:
    """Rule-based ì ìˆ˜ì™€ LLM ì ìˆ˜ë¥¼ ìœµí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    rule_scores, _, _ = rule_scoring(text)
    
    text_if = {c: 0.0 for c in CLUSTERS}
    if llm_json and not llm_json.get("error"):
        I, F = llm_json.get("intensity", {}), llm_json.get("frequency", {})
        for c in CLUSTERS:
            In = clip01((I.get(c, 0.0) or 0.0) / 3.0)
            Fn = clip01((F.get(c, 0.0) or 0.0) / 3.0)
            text_if[c] = clip01(0.6 * In + 0.4 * Fn + 0.1 * rule_scores.get(c, 0.0))
    
    fused_scores = {c: clip01(W_RULE * rule_scores.get(c, 0.0) + W_LLM * text_if.get(c, 0.0)) for c in CLUSTERS}
    return fused_scores

# ======================================================================
# === í—¬í¼ í•¨ìˆ˜ (Helpers) ===
# ======================================================================

def _format_scores_for_print(scores: dict) -> str:
    """ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜"""
    if not isinstance(scores, dict):
        return str(scores)
    return json.dumps({k: round(v, 2) if isinstance(v, float) else v for k, v in scores.items()})

def clip01(x: float) -> float: return max(0.0, min(1.0, float(x)))

def g_score(final_scores: dict) -> float:
    w = {"neg_high": 1.0, "neg_low": 0.9, "sleep": 0.7, "adhd": 0.6, "positive": -0.3}
    g = sum(final_scores.get(k, 0.0) * w.get(k, 0.0) for k in CLUSTERS)
    return round(clip01((g + 1.0) / 2.0), 3)

def pick_profile(final_scores: dict, llm: Optional[dict]) -> int: # surveys íŒŒë¼ë¯¸í„° ì œê±°
    if (llm or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}: return 1
    if max(final_scores.get("neg_low", 0), final_scores.get("neg_high", 0)) > 0.85: return 1
    # if (surveys and (surveys.get("phq9", 0) >= 10 or surveys.get("gad7", 0) >= 10)) or max(final_scores.values()) > 0.60: return 2
    if max(final_scores.values()) > 0.60: return 2 # surveys í•„ë“œ ì œê±°
    if max(final_scores.values()) > 0.30: return 3
    return 0

def calculate_baseline_scores(onboarding_scores: Optional[Dict[str, int]]) -> Dict[str, float]:
    if not onboarding_scores: return {c: 0.0 for c in CLUSTERS}
    baseline = {c: 0.0 for c in CLUSTERS}
    for q_key, score in onboarding_scores.items():
        processed_score = 3 - score if q_key == 'q7' else score
        if q_key in ONBOARDING_MAPPING:
            normalized_score = processed_score / 3.0
            for mapping in ONBOARDING_MAPPING[q_key]:
                baseline[mapping["cluster"]] += normalized_score * mapping["w"]
    for c in CLUSTERS:
        baseline[c] = clip01(baseline.get(c, 0.0))
    return baseline

# ======================================================================
# === DB ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ ===
# ======================================================================

async def get_user_info(user_id: str) -> Tuple[str, str]:
    """ì‚¬ìš©ì ë‹‰ë„¤ì„ê³¼ ìºë¦­í„° ì´ë¦„ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not supabase: return "ì‚¬ìš©ì", "ëª¨ì§€"
    try:
        res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("user_nick_nm, character_nm")
            .eq("id", user_id).single().execute
        )
        if res.data:
            return res.data.get("user_nick_nm", "ì‚¬ìš©ì"), res.data.get("character_nm", "ëª¨ì§€")
    except Exception:
        pass
    return "ì‚¬ìš©ì", "ëª¨ì§€"


# ëª¨ë“  ë©˜íŠ¸ ì¡°íšŒë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def get_mention_from_db(mention_type: str, language_code: str, **kwargs) -> str:
    """DBì—ì„œ ì§€ì •ëœ íƒ€ì…ê³¼ ì¡°ê±´ì— ë§ëŠ” ìºë¦­í„° ë©˜íŠ¸ë¥¼ ëœë¤ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"--- âœ… get_mention_from_db í˜¸ì¶œë¨ (mention_type: {mention_type}) âœ… ---")

    default_messages = {
        "analysis": "ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
        "reaction": "ì–´ë–¤ ì¼ ë•Œë¬¸ì— ê·¸ë ‡ê²Œ ëŠë¼ì…¨ë‚˜ìš”?",
        "propose": "ì´ëŸ° í™œë™ì€ ì–´ë– ì„¸ìš”?",
        "home": "ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        "followup_user_closed": "ê´œì°®ì•„ìš”. ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°ˆê¹Œìš”?",
        "followup_video_ended": "ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        "decline_solution": "ì•Œê² ìŠµë‹ˆë‹¤. í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”."
    }
    default_message = default_messages.get(mention_type, "...")

    # .format()ì— ì‚¬ìš©ë  ì¸ìë“¤ì„ ë¯¸ë¦¬ ì¤€ë¹„í•©ë‹ˆë‹¤.
    format_args = kwargs.get("format_kwargs", kwargs)

    def _safe_format(text: str) -> str:
        """KeyError ì—†ì´ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            # format_argsê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ dictìœ¼ë¡œ ì²˜ë¦¬
            return text.format(**(format_args or {}))
        except KeyError:
            # í¬ë§·íŒ…ì— ì‹¤íŒ¨í•˜ë©´ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            return text.replace("{user_nick_nm}", "ì¹œêµ¬")

    if not supabase:
        return _safe_format(default_message)


    # if not supabase: return default_message.format(**kwargs) if kwargs else default_message
    try:
        query = supabase.table("character_mentions").select("text").eq("mention_type", mention_type).eq("language_code", language_code)
        
        valid_filter_keys = ["personality", "cluster", "level", "solution_variant"]
        for key, value in kwargs.items():
            if key in valid_filter_keys and value:
                query = query.eq(key, value)

        response = await run_in_threadpool(query.execute)
        scripts = [row['text'] for row in response.data]
        
        if not scripts:
            return _safe_format(default_message)
        
        selected_script = random.choice(scripts)
        # DBì—ì„œ ê°€ì ¸ì˜¨ ë©˜íŠ¸ë„ ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…
        return _safe_format(selected_script)

    except Exception as e:
        print(f"âŒ get_mention_from_db Error: {e}")
        return _safe_format(default_message)


# # ìˆ˜ì¹˜ë¥¼ ì£¼ê¸°ë³´ë‹¤ëŠ”, ì‹¬ê°ë„ 3ë‹¨ê³„ì— ë”°ë¼ ë©”ì‹œì§€ í•´ì„í•´ì£¼ëŠ”ê²Œ ë‹¬ë¼ì§(ìˆ˜ì¹˜í˜•x, ëŒ€í™”í˜•o)
# async def get_analysis_message(
#     scores: dict, 
#     personality: Optional[str], 
#     language_code: str
# ) -> str:
#     if not scores: return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ë” ë“¤ì—¬ë‹¤ë³´ê³  ìˆì–´ìš”."
#     top_cluster = max(scores, key=scores.get)
#     score_val = scores[top_cluster]
    
#     level = "low"
#     if score_val > 0.7: level = "high"
#     elif score_val > 0.4: level = "mid"
    
#     return await get_mention_from_db(
#         mention_type="analysis",
#         personality=personality,
#         language_code=language_code,
#         cluster=top_cluster,
#         level=level,
#         default_message="ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
#         format_kwargs={"emotion": top_cluster, "score": int(score_val * 100)}
#     )
    


async def save_analysis_to_supabase(
    payload: AnalyzeRequest, profile: int, g: float,
    intervention: dict, debug_log: dict, final_scores: dict
) -> Optional[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not supabase: return None
    try:
        user_id = payload.user_id

        # ì„¸ì…˜ì„ ì €ì¥í•˜ê¸° ì „, user_profilesì— í•´ë‹¹ ìœ ì €ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        profile_query = supabase.table("user_profiles").select("id").eq("id", user_id)
        profile_response = await run_in_threadpool(profile_query.execute)
        
        if not profile_response.data:
            print(f"âš ï¸ User profile for {user_id} not found. Creating a new one.")
            insert_query = supabase.table("user_profiles").insert({
                "id": user_id, 
                "user_nick_nm": "ì‚¬ìš©ì" 
            })
            await run_in_threadpool(insert_query.execute)


        session_row = {
            "user_id": user_id, "text": payload.text, "icon": payload.icon,
            "profile": int(profile), "g_score": float(g),
            "intervention": json.dumps(intervention, ensure_ascii=False),
            "debug_log": json.dumps(debug_log, ensure_ascii=False),
            "summary": (debug_log.get("llm") or {}).get("summary", ""),
        }
        
        session_insert_query = supabase.table("sessions").insert(session_row)
        response = await run_in_threadpool(session_insert_query.execute)
        
        session_data = response.data[0] if response.data else {}
        new_session_id = session_data.get('id')

        if not new_session_id:
            print("ğŸš¨ ERROR: Failed to save session, no ID returned.")
            return None
        
        print(f"âœ… Session saved successfully. session_id: {new_session_id}")

        if final_scores:
            score_rows = [
                {"session_id": new_session_id, "user_id": payload.user_id, "cluster": c, "score": v}
                for c, v in final_scores.items()
            ]
            if score_rows:
                for row in score_rows:
                    row["session_text"] = payload.text
                
                # ğŸ’¡ [ìˆ˜ì •] .executeë¥¼ ë¶„ë¦¬
                scores_insert_query = supabase.table("cluster_scores").insert(score_rows)
                await run_in_threadpool(scores_insert_query.execute)
        
        return new_session_id
    except Exception as e:
        print(f"ğŸš¨ Supabase save failed: {e}")
        traceback.print_exc()
        return None


# ---------- API Endpoints (ë¶„ë¦¬ëœ êµ¬ì¡°) ----------


# ======================================================================
# === /analyze ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ë¡œì§ (ë¶„ë¦¬ëœ í•¨ìˆ˜ë“¤) ===
# ======================================================================

async def _handle_moderation(text: str) -> bool:
    """OpenAI Moderation APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ í•´ ì½˜í…ì¸ ë¥¼ í™•ì¸í•˜ê³  ì°¨ë‹¨ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    is_flagged, categories = await moderate_text(text, OPENAI_KEY)
    if not is_flagged:
        return False

    allowed_categories = {'self-harm', 'self-harm/intent', 'self-harm/instructions', 'hate', 'harassment', 'violence'}
    should_block = any(cat not in allowed_categories and triggered for cat, triggered in categories.items())
    
    if should_block:
        print(f"ğŸš¨ [BLOCKED] Inappropriate content: '{text}', Categories: {categories}")
    else:
        print(f"âš ï¸ [PASSED] Delegating to internal safety check: '{text}', Categories: {categories}")
    
    return should_block

async def _handle_emoji_only_case(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """ì•„ì´ì½˜ë§Œ ì…ë ¥ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "EMOJI_REACTION"
    print("\n--- ğŸ§ EMOJI-ONLY ANALYSIS ---")

    selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower(), "neg_low")
    
    if selected_cluster == "neutral":
        intervention = {"preset_id": PresetIds.EMOJI_REACTION, "text": "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”?", "top_cluster": "neutral"}
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
        return {"session_id": session_id, "intervention": intervention}
    
    assessment_scores = calculate_baseline_scores(payload.onboarding) # ì´ëª¨ì§€ë§Œ ìˆì„ ë• ì˜¨ë³´ë”© ì ìˆ˜ ì‚¬ìš©
    icon_scores = {c: 1.0 if c == selected_cluster else 0.0 for c in CLUSTERS}
   
    w = FINAL_FUSION_WEIGHTS_NO_TEXT
    fused_scores = {c: clip01(assessment_scores.get(c, 0.0) * w['assessment'] + icon_scores.get(c, 0.0) * w['icon']) for c in CLUSTERS}
    
    # ì ìˆ˜ ìƒí•œì„ (Cap) ì ìš©
    final_scores = fused_scores.copy()
    if selected_cluster in final_scores:
        final_scores[selected_cluster] = min(final_scores[selected_cluster], EMOJI_ONLY_SCORE_CAP)
   
    g, profile = g_score(final_scores), pick_profile(final_scores, None)
    
    user_nick_nm, _ = await get_user_info(payload.user_id)
    reaction_text = await get_mention_from_db("reaction", payload.language_code, personality=payload.character_personality, cluster=selected_cluster, user_nick_nm=user_nick_nm)
    intervention = {"preset_id": PresetIds.EMOJI_REACTION, "top_cluster": selected_cluster, "empathy_text": reaction_text}
    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)
    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}


async def _handle_friendly_mode(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ì¹œêµ¬ ëª¨ë“œ'ì¼ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "FRIENDLY"
    print(f"\n--- ğŸ‘‹ FRIENDLY MODE: '{payload.text}' ---")

    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    system_prompt = get_system_prompt(
        mode='FRIENDLY', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )
    # ì´ì „ ëŒ€í™” ê¸°ì–µ: ì¹œêµ¬ ëª¨ë“œì—ì„œë„ ëŒ€í™” ê¸°ë¡ì„ user_contentì— í¬í•¨
    history_str = "\n".join([f"{h.sender}: {h.content}" for h in payload.history]) if payload.history else ""
    user_content = f"Previous conversation:\n{history_str}\n\nCurrent message: {payload.text}"

    llm_response = await call_llm(system_prompt, user_content, OPENAI_KEY, expect_json=False)

    # --- ğŸ‘‡ [ìˆ˜ì •] ---
    # LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì—ëŸ¬ì¸ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
    final_text = llm_response if not (isinstance(llm_response, dict) and 'error' in llm_response) else "ìŒ... ì§€ê¸ˆì€ ì ì‹œ ìƒê°í•  ì‹œê°„ì´ í•„ìš”í•´ìš”!ğŸ¥¹"
    intervention = {"preset_id": PresetIds.FRIENDLY_REPLY, "text": final_text}
    
    session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
    return {"session_id": session_id, "intervention": intervention}


async def _run_analysis_pipeline(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ë¶„ì„ ëª¨ë“œ'ì¼ ê²½ìš°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "ANALYSIS"
    print(f"\n--- ğŸ§ ANALYSIS MODE: '{payload.text}' ---")

     # 1. ì‚¬ìš©ìì˜ ìµœì‹  í‰ê°€ ì ìˆ˜(assessment_scores)ë¥¼ DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #    ì´ ì ìˆ˜ëŠ” ì˜¨ë³´ë”©ìœ¼ë¡œ ì‹œì‘í•´ì„œ, ì‹¬ì¸µ ë¶„ì„ì„ í•  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    
    profile_res = await run_in_threadpool(
        supabase.table("user_profiles")
        .select("latest_assessment_scores")
        .eq("id", payload.user_id).single().execute
    )
    
    assessment_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
    if not assessment_scores or not isinstance(assessment_scores, dict):
        # ìµœì‹  í‰ê°€ ì ìˆ˜ê°€ ì—†ìœ¼ë©´(ì˜ˆ: ì²« ì‚¬ìš©ì), ì˜¨ë³´ë”© ì ìˆ˜ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.
        print("âš ï¸ Latest assessment scores not found, using onboarding scores as baseline.")
        assessment_scores = calculate_baseline_scores(payload.onboarding)

    # --------------------------------------------------------------------------
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ 
    system_prompt = get_system_prompt(
        mode='ANALYSIS', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )

   # ì´ì „ ëŒ€í™” ê¸°ì–µ: ë¶„ì„ ëª¨ë“œì—ì„œë„ LLM í˜¸ì¶œ ì‹œ historyë¥¼ í¬í•¨
    history_for_llm = [h.dict() for h in payload.history] if payload.history else []
    llm_payload = {"user_message": payload.text, "baseline_scores": assessment_scores, "history": history_for_llm}
   
    # 2. LLM í˜¸ì¶œ ë° 2ì°¨ ì•ˆì „ ì¥ì¹˜
    llm_json = await call_llm(system_prompt, json.dumps(llm_payload, ensure_ascii=False), OPENAI_KEY)
    debug_log["llm"] = llm_json

    is_crisis, crisis_scores = is_safety_text(payload.text, llm_json, debug_log)
    if is_crisis:
        print(f"ğŸš¨ 2nd Safety Check Triggered: '{payload.text}'")
        g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
        intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "cluster": top_cluster,"solution_id": f"{top_cluster}_crisis_01"}
        session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
        return {"session_id": session_id, "final_scores": crisis_scores, "g_score": g, "profile": profile, "intervention": intervention}


    # 3. ëª¨ë“  ì ìˆ˜ ê³„ì‚° ë° ìœµí•©
    text_scores = calculate_text_scores(payload.text, llm_json)
    
    has_icon = payload.icon and ICON_TO_CLUSTER.get(payload.icon.lower()) != "neutral"
    icon_scores = {c: 0.0 for c in CLUSTERS}
    if has_icon:
        icon_scores[ICON_TO_CLUSTER.get(payload.icon.lower())] = 1.0


    final_scores, weights_used = calculate_final_scores(text_scores, assessment_scores, icon_scores, has_icon)
    debug_log["scores"] = {"weights_used": weights_used, "assessment_base": assessment_scores, "text": text_scores, "icon": icon_scores, "final": final_scores}
    print(f"Scores -> Assessment Base: {_format_scores_for_print(assessment_scores)}, Text: {_format_scores_for_print(text_scores)}, Icon: {_format_scores_for_print(icon_scores)}")
    print(f"Weights: {_format_scores_for_print(weights_used)} -> Final Scores: {_format_scores_for_print(final_scores)}")


    # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
    g, profile = g_score(final_scores), pick_profile(final_scores, llm_json)
    top_cluster = max(final_scores, key=final_scores.get, default="neg_low")
    print(f"G-Score: {g:.2f}, Profile: {profile}")
    
    empathy_text = (llm_json or {}).get("empathy_response", "ë§ˆìŒì„ ì‚´í”¼ëŠ” ì¤‘ì´ì—ìš”...")
    score_val = final_scores[top_cluster]
    level = "high" if score_val > 0.7 else "mid" if score_val > 0.4 else "low"
    
    # 'analysis' íƒ€ì…ì˜ ë©˜íŠ¸ë¥¼ DBì—ì„œ ê°€ì ¸ì˜´
    analysis_text = await get_mention_from_db(
        "analysis", 
        payload.language_code, 
        personality=payload.character_personality, 
        cluster=top_cluster, 
        level=level, 
        format_kwargs={"emotion": CLUSTER_TO_DISPLAY_NAME.get(top_cluster)}
    )
    
    # intervention ê°ì²´ ìƒì„± ë° DB ì €ì¥
    intervention = {
        "preset_id": PresetIds.SOLUTION_PROPOSAL, 
        "top_cluster": top_cluster, 
        "empathy_text": empathy_text, 
        "analysis_text": analysis_text
    }

    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)
    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}



# ======================================================================
# ===          ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
 
@app.post("/analyze")
async def analyze_emotion(payload: AnalyzeRequest):
    """ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì™€ ì•„ì´ì½˜ì„ ë°›ì•„ ê°ì •ì„ ë¶„ì„í•˜ê³  ìŠ¤ì½”ì–´ë§ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = (payload.text or "").strip()
    debug_log: Dict[str, Any] = {"input": payload.dict()}

    try:
        # --- íŒŒì´í”„ë¼ì¸ 0: ìœ í•´ ì½˜í…ì¸  ê²€ì—´ ---
        if await _handle_moderation(text):
            return JSONResponse(status_code=400, content={"error": "Inappropriate content detected."})

        # --- íŒŒì´í”„ë¼ì¸ 1: ğŸŒ¸ CASE 2 - ì´ëª¨ì§€ë§Œ ìˆëŠ” ê²½ìš° ---
        if payload.icon and not text:
            debug_log["mode"] = "EMOJI_REACTION"
            return await _handle_emoji_only_case(payload, debug_log)

        # --- íŒŒì´í”„ë¼ì¸ 2: 1ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM í˜¸ì¶œ ì „) ---
        is_crisis, crisis_scores = is_safety_text(text, None, debug_log)
        if is_crisis:
            print(f"ğŸš¨ 1st Safety Check Triggered: '{text}'")
            g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
            intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "cluster": top_cluster,"solution_id": f"{top_cluster}_crisis_01"}
            session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
            return {"session_id": session_id, "intervention": intervention}

        # --- íŒŒì´í”„ë¼ì¸ 3: Triage (ì¹œêµ¬ ëª¨ë“œ / ë¶„ì„ ëª¨ë“œ ë¶„ê¸°) ---
        rule_scores, _, _ = rule_scoring(text)
        is_simple_text = len(text) < 10 and max(rule_scores.values() or [0.0]) < 0.1
        
        if is_simple_text:
            triage_mode = 'FRIENDLY'
            debug_log["triage_decision"] = "Rule-based: Simple text"
        else:
            triage_mode = await call_llm(TRIAGE_SYSTEM_PROMPT, text, OPENAI_KEY, expect_json=False)
            debug_log["triage_decision"] = f"LLM Triage: {triage_mode}"

        # --- íŒŒì´í”„ë¼ì¸ 4: Triage ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ---
        if triage_mode == 'FRIENDLY':
            return await _handle_friendly_mode(payload, debug_log)
        else: # ANALYSIS
            return await _run_analysis_pipeline(payload, debug_log)

    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ UNHANDLED EXCEPTION in /analyze: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})

# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ (ë§ˆìŒ ì ê²€) ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    if not supabase: raise HTTPException(status_code=500, detail="Supabase client not initialized")
    try:
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score: raise HTTPException(status_code=400, detail=f"Invalid cluster: {payload.cluster}")
        
        normalized_score = clip01(total_score / max_score)
        
        profile_res = await run_in_threadpool(supabase.table("user_profiles").select("latest_assessment_scores").eq("id", payload.user_id).single().execute)
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}
        
        latest_scores[payload.cluster] = normalized_score
        
        await run_in_threadpool(supabase.table("user_profiles").update({"latest_assessment_scores": latest_scores}).eq("id", payload.user_id).execute)
        
        history_row = {"user_id": payload.user_id, "assessment_type": f"deep_dive_{payload.cluster}", "scores": {payload.cluster: normalized_score}, "raw_responses": payload.responses}
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)
        
        return {"message": "Assessment submitted successfully", "updated_scores": latest_scores}
    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})



# ======================================================================
# ===          ì†”ë£¨ì…˜ ì œì•ˆ ë° ìƒì„¸ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
   

@app.post("/solutions/propose")
async def propose_solution(payload: SolutionRequest): 
    """ë¶„ì„ ê²°ê³¼(top_cluster)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë§ëŠ” ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ëŠ” ë¡œì§"""
    if not supabase: raise HTTPException(status_code=500, detail="Supabase client not initialized")
        
    try:
        user_nick_nm, _ = await get_user_info(payload.user_id)

        solutions_res = await run_in_threadpool(
            supabase.table("solutions")
            .select("solution_id, text, context, solution_variant")
            .eq("cluster", payload.top_cluster)
            .execute
            )

        
        if not solutions_res.data:
            return {"proposal_text": "ì§€ê¸ˆì€ ì œì•ˆí•´ë“œë¦´ íŠ¹ë³„í•œ í™œë™ì´ ì—†ë„¤ìš”.", "solution_id": None}
                  
        # ê°€ì ¸ì˜¨ ì†”ë£¨ì…˜ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
        solution_data = random.choice(solutions_res.data)
        solution_id = solution_data.get("solution_id")
        solution_variant = solution_data.get("solution_variant") 


        # ì†”ë£¨ì…˜ IDê°€ ì—†ëŠ” ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ 
        if not solution_id:
            return {
                "proposal_text": "í¸ì•ˆí•˜ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆê¹Œìš”?", 
                "solution_id": None,
                "solution_details": None
            }
        
        # ì„ íƒëœ ì†”ë£¨ì…˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'propose'íƒ€ì…ì˜ ë©˜íŠ¸ë¥¼ DBì—ì„œ ì¡°íšŒ
        #    ì´ë•Œ solution_variantë¥¼ í•¨ê»˜ ë„˜ê²¨ í•„í„°ë§
        proposal_script = await get_mention_from_db(
            "propose",
            payload.language_code,
            cluster=payload.top_cluster,
            user_nick_nm=user_nick_nm,
            solution_variant=solution_variant 
        )

       # ìµœì¢… ì œì•ˆ í…ìŠ¤íŠ¸ë¥¼ ì¡°í•© (ë©˜íŠ¸ + ì†”ë£¨ì…˜ ìì²´ í…ìŠ¤íŠ¸)
        final_text = f"{proposal_script} {solution_data.get('text', '')}".strip()
      

        # ì œì•ˆ ì´ë ¥ì„ ë¡œê·¸ë¡œ ì €ì¥
        log_entry = {"session_id": payload.session_id, "type": "propose", "solution_id": solution_id}
        await run_in_threadpool(supabase.table("interventions_log").insert(log_entry).execute)

        return {"proposal_text": final_text, "solution_id": solution_id, "solution_details": solution_data}

    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})


# ======================================================================
# ===          ì†”ë£¨ì…˜ ì˜ìƒ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================

    # SolutionPageì—ì„œ ì˜ìƒ ë¡œë“œ
    # í•˜ë“œì½”ë”©ëœ SOLUTION_DETAILS_LIBRARYë¥¼ DB ì¡°íšŒë¡œ ëŒ€ì²´í–ˆìŒ!!
@app.get("/solutions/{solution_id}")
async def get_solution_details(solution_id: str):
    print(f"RIN: âœ… ì†”ë£¨ì…˜ ìƒì„¸ ì •ë³´ ìš”ì²­ ë°›ìŒ: {solution_id}")
    
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")
    
    try:
        # solutions í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¡°íšŒ
        response = await run_in_threadpool(
            supabase.table("solutions")
            .select("url, start_at, end_at")
            .eq("solution_id", solution_id)
            .single()
            .execute
        )
        
        
        if not response.data:
            raise HTTPException(status_code=404, detail="Solution not found")

        # ìœ íŠœë¸Œ ë¶ˆëŸ¬ì˜¬ë•Œ startAt, endAt (camelCase)ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ í‚¤ë¥¼ ë³€í™˜í•´ì¤Œ
        # supabaseëŠ” snake_caseë¡œ ì €ì¥í•´ì•¼ í•œë‹¤ê³  í•¨.
        return {
            'url': response.data.get('url'), 
            'startAt': response.data.get('start_at'), 
            'endAt': response.data.get('end_at')
            }
        
    except Exception as e:
        print(f"RIN: âŒ í•´ë‹¹ ì†”ë£¨ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {solution_id}, ì—ëŸ¬: {e}")
        raise HTTPException(status_code=404, detail="Solution not found")
    

# ======================================================================
# ===          ìƒí™©ë³„ ëŒ€ì‚¬ ì œê³µ ì—”ë“œí¬ì¸íŠ¸ (`/dialogue/*`)         ===
# ======================================================================
@app.get("/dialogue/home")
async def get_home_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko',
    emotion: Optional[str] = None 
):
    """í™ˆ í™”ë©´ì— í‘œì‹œí•  ëŒ€ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if emotion:
        # ì´ëª¨ì§€ê°€ ì„ íƒëœ ê²½ìš°: 'reaction' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "reaction"
        cluster = ICON_TO_CLUSTER.get(emotion.lower(), "common")
    else:
        # ì´ëª¨ì§€ê°€ ì—†ëŠ” ì´ˆê¸° ìƒíƒœ: 'home' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "home"
        cluster = "common"

    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        language_code=language_code,
        personality=personality,
        cluster=cluster,
        default_message=f"ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}
    
#  ì†”ë£¨ì…˜ ì™„ë£Œ í›„ í›„ì† ì§ˆë¬¸ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸  
@app.get("/dialogue/solution-followup")
async def get_solution_followup_dialogue(
    reason: str, # 'user_closed' ë˜ëŠ” 'video_ended'
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ì´ ëë‚œ í›„ì˜ ìƒí™©(reason)ê³¼ ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” í›„ì† ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # ì´ìœ (reason)ì— ë”°ë¼ DBì—ì„œ ì¡°íšŒí•  mention_typeì„ ê²°ì •í•©ë‹ˆë‹¤.
    if reason == 'user_closed':
        mention_type = "followup_user_closed"
    else: # 'video_ended' ë˜ëŠ” ê¸°íƒ€
        mention_type = "followup_video_ended"

    # get_mention_from_db í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        personality=personality,
        language_code=language_code,
        cluster="common", 
        default_message="ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí–ˆì„ ë•Œì˜ ë©˜íŠ¸ë¥¼ ì„±í–¥ë³„ë¡œ ì£¼ê¸° 
@app.get("/dialogue/decline-solution")
async def get_decline_solution_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí•˜ê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³  ì‹¶ì–´í•  ë•Œì˜ ë°˜ì‘ ë©˜íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    dialogue_text = await get_mention_from_db(
        mention_type="decline_solution",
        personality=personality,
        language_code=language_code,
        cluster="common",
        default_message="ì•Œê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ìš”. ì €ì—ê²Œ í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”. ê·€ ê¸°ìš¸ì—¬ ë“£ê³  ìˆì„ê²Œìš”.",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ======================================================================
# ===     ë¦¬í¬íŠ¸ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
class DailyReportRequest(BaseModel):
    user_id: str
    date: str # "YYYY-MM-DD" í˜•ì‹
    language_code: str = 'ko'


async def create_and_save_summary_for_user(user_id: str, date_str: str):
    """
    íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¹ì • ë‚ ì§œì— ëŒ€í•œ ìš”ì•½ë¬¸ì„ ìƒì„±í•˜ê³  DBì— ì €ì¥(Upsert)í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìŠ¤ì¼€ì¤„ë§ëœ ì‘ì—…(/tasks/generate-summaries)ì— ì˜í•´ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    print(f"----- [Job Start] User: {user_id}, Date: {date_str} -----")
    
    # Supabase ë˜ëŠ” OpenAI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.
    if not supabase or not OPENAI_KEY:
        print("Error: Supabase client or OpenAI key not initialized. Skipping summary generation.")
        return

    try:
        # --- 1. ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ---
        user_nick_nm, character_nm = await get_user_info(user_id)
        start_of_day = f"{date_str}T00:00:00+00:00"
        end_of_day = f"{date_str}T23:59:59+00:00"

        # --- 2. ê·¸ë‚ ì˜ ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ê¸°ë¡ ë° í‰ê·  ê³„ì‚° ---
        score_query = supabase.table("cluster_scores").select("cluster, score") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day)
        score_res = await run_in_threadpool(score_query.execute)
        
        all_scores_today = score_res.data
        # ìœ ì €ê°€ ì•±ì„ ì¼°ì§€ë§Œ ìœ ì˜ë¯¸í•œ ê°ì • ë¶„ì„ ê¸°ë¡(cluster_scores)ì´ ì—†ëŠ” ê²½ìš° íŒ¨ìŠ¤
        if not all_scores_today:
            print(f"Info: No score data for user {user_id} on {date_str}. Skipping.")
            return
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ê¸°ë¡(entry)ì„ ì°¾ìŒ
        top_score_entry = max(all_scores_today, key=lambda x: x['score'])
        top_cluster_name = top_score_entry['cluster']


        # ì˜¤ëŠ˜ì˜ ëŒ€í‘œ í´ëŸ¬ìŠ¤í„°(top_cluster_name)ì— í•´ë‹¹í•˜ëŠ” 'í‘œì‹œìš© ì´ë¦„'ì„ ì¡°íšŒ
        top_cluster_display_name = CLUSTER_TO_DISPLAY_NAME.get(top_cluster_name, "ì£¼ìš” ê°ì •")


        # ê·¸ë‚  ìµœê³  ì ìˆ˜ë¥¼ ê¸°ë¡í•œ ì„¸ì…˜ì˜ ì ìˆ˜
        top_score = top_score_entry['score']
        top_score_for_llm = int(top_score * 100)
        
        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ì ìˆ˜ë¥¼ ë‹¤ì‹œ ëª¨ì•„ì„œ 
        scores_for_top_cluster = [item['score'] for item in all_scores_today if item['cluster'] == top_cluster_name]
        # í‰ê· ì„ ê³„ì‚°í•¨
        average_score = sum(scores_for_top_cluster) / len(scores_for_top_cluster) if scores_for_top_cluster else 0
        top_score_for_llm = int(average_score * 100)

        # --- 3. ê·¸ë‚ ì˜ ëª¨ë“  ëŒ€í™” ìš”ì•½(summary) ê°€ì ¸ì˜¤ê¸° ---
        session_query = supabase.table("sessions").select("summary") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .not_.is_("summary", "null")
        session_res = await run_in_threadpool(session_query.execute)
        dialogue_summaries = [s['summary'] for s in session_res.data if s.get('summary')]

        # --- 4. ê·¸ë‚  ì œê³µëœ ì†”ë£¨ì…˜ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ---
        user_sessions_query = supabase.table("sessions").select("id") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day)
        user_sessions_res = await run_in_threadpool(user_sessions_query.execute)
        
        solution_contexts = []
        if user_sessions_res.data:
            session_ids = [s['id'] for s in user_sessions_res.data]
            log_query = supabase.table("interventions_log").select("solution_id") \
                .in_("session_id", session_ids) \
                .eq("type", "propose")
            log_res = await run_in_threadpool(log_query.execute)
            if log_res.data:
                solution_ids = list(set([log['solution_id'] for log in log_res.data]))
                solution_query = supabase.table("solutions").select("context").in_("solution_id", solution_ids)
                solution_res = await run_in_threadpool(solution_query.execute)
                solution_contexts = [s['context'] for s in solution_res.data if s.get('context')]

        # --- 5. ëŒ€í‘œ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ì¡°ì–¸ ê°€ì ¸ì˜¤ê¸° ---
        advice_text = await get_mention_from_db(
            "analysis", "ko", cluster=top_cluster_name, level="high"
        )

        # --- 6. LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ì¡°í•© ---
        llm_context = {
            "user_nick_nm": user_nick_nm,
            # "top_cluster_today": top_cluster_name, # ê¸°ì¡´ ë‚´ë¶€ í‚¤ëŠ” ì´ì œ LLMì— í•„ìš” ì—†ìŒ
            "top_cluster_display_name": top_cluster_display_name, # ì‚¬ìš©ìì—ê²Œ í‘œì‹œìš© ì´ë¦„ì„ ì „ë‹¬
            "top_score_today": top_score_for_llm,
            "user_dialogue_summary": " ".join(dialogue_summaries) or "íŠ¹ë³„í•œ ëŒ€í™”ëŠ” ì—†ì—ˆì–´ìš”.",
            "solution_context": ", ".join(solution_contexts) or "ì œê³µëœ ì†”ë£¨ì…˜ì´ ì—†ì—ˆì–´ìš”.",
            "cluster_advice": advice_text
        }
        
        system_prompt = REPORT_SUMMARY_PROMPT

        # --- 7. LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ë¬¸ ìƒì„± ---
        summary_json = await call_llm(
            system_prompt=system_prompt,
            user_content=json.dumps(llm_context, ensure_ascii=False),
            openai_key=OPENAI_KEY
        )
        
        daily_summary_text = summary_json.get("daily_summary")
        if not daily_summary_text:
            print(f"Warning: LLM failed to generate summary for user {user_id} on {date_str}.")
            return

        # --- 8. ìƒì„±ëœ ìš”ì•½ë¬¸ì„ `daily_summaries` í…Œì´ë¸”ì— ì €ì¥ (Upsert) ---
        summary_data = {
            "user_id": user_id,
            "date": date_str,
            "summary_text": daily_summary_text,
            "top_cluster": top_cluster_name,
            "avg_score": top_score_for_llm
        }
        
        # upsert: user_idì™€ dateê°€ ë™ì¼í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        upsert_query = supabase.table("daily_summaries").upsert(summary_data, on_conflict="user_id,date")
        await run_in_threadpool(upsert_query.execute)
        
        print(f"Success: Saved summary for user {user_id} on {date_str}.")

    except Exception as e:
        print(f"Error: Failed to generate summary for user {user_id} on {date_str}. Reason: {e}")
        traceback.print_exc()

    finally:
        print(f"----- [Job End] User: {user_id}, Date: {date_str} -----")

#  ------- daily_summaries í…Œì´ë¸”ì—ì„œ ìš”ì•½ë¬¸ ê°„ë‹¨íˆ ì¡°íšŒ ---------

@app.post("/report/summary")
async def get_daily_report_summary(request: DailyReportRequest):
    """ë¯¸ë¦¬ ìƒì„±ëœ ì¼ì¼ ìš”ì•½ë¬¸ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        query = supabase.table("daily_summaries").select("summary_text") \
            .eq("user_id", request.user_id) \
            .eq("date", request.date) \
            .limit(1)
            
        response = await run_in_threadpool(query.execute)

        if response.data:
            summary = response.data[0].get("summary_text", "ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"summary": summary}
        else:
            return {"summary": "í•´ë‹¹ ë‚ ì§œì˜ ìš”ì•½ ê¸°ë¡ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ì–´ìš”."}

    except Exception as e:
        print(f"ğŸ”¥ EXCEPTION in /report/summary (read): {e}")
        raise HTTPException(status_code=500, detail="ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    

# ======================================================================
# ===     ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ë§ ì‘ì—…ìš© ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/tasks/generate-summaries")
async def handle_generate_summaries_task():
    """
    Supabase Cron Jobì— ì˜í•´ í˜¸ì¶œë  ì—”ë“œí¬ì¸íŠ¸.
    ì–´ì œ í™œë™í•œ ëª¨ë“  ì‚¬ìš©ìì˜ ì¼ì¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # ì–´ì œ ë‚ ì§œ ê³„ì‚° (UTC ê¸°ì¤€)
    yesterday = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=1)
    yesterday_str = yesterday.strftime('%Y-%m-%d')
    
    start_of_yesterday = f"{yesterday_str}T00:00:00+00:00"
    end_of_yesterday = f"{yesterday_str}T23:59:59+00:00"

    print(f"Starting daily summary generation task for date: {yesterday_str}")

    # ì–´ì œ í™œë™í•œ ìœ ì € ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
    active_users_query = supabase.table("sessions").select("user_id", count='exact') \
        .gte("created_at", start_of_yesterday) \
        .lte("created_at", end_of_yesterday)
    
    active_users_res = await run_in_threadpool(active_users_query.execute)
    # ì–´ì œ ì•±ì„ ì‚¬ìš©í•œ ìœ ì €ê°€ ë‹¨ í•œ ëª…ë„ ì—†ë‹¤ë©´, ì¦‰ì‹œ "ì–´ì œ í™œë™í•œ ìœ ì € ì—†ìŒ" ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì‘ì—…ì„ ì¢…ë£Œ
    if not active_users_res.data:
        message = "No active users yesterday. Task finished."
        print(message)
        return {"message": message}

    # í™œë™ ìœ ì €ê°€ ìˆì„ ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰
    user_ids = list(set([item['user_id'] for item in active_users_res.data]))
    
    print(f"Found {len(user_ids)} active users. Starting summary generation for each user...")

    # ê° ìœ ì €ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì•½ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    for user_id in user_ids:
        await create_and_save_summary_for_user(user_id, yesterday_str)

    message = f"Summary generation task complete for {len(user_ids)} users."
    print(message)
    return {"message": message}


# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    """ì£¼ê¸°ì  ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ DBì— ì €ì¥í•˜ê³ , ì‚¬ìš©ìì˜ ìµœì‹  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        # 1. ì œì¶œëœ ë‹µë³€ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° ë° ì •ê·œí™”
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score:
            raise HTTPException(status_code=400, detail=f"Invalid cluster: {payload.cluster}")
        
        normalized_score = clip01(total_score / max_score)

        # 2. user_profiles í…Œì´ë¸”ì—ì„œ í˜„ì¬ ìµœì‹  ì ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
        profile_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("latest_assessment_scores")
            .eq("id", payload.user_id).single().execute
        )
        
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}


        # 3. ì´ë²ˆì— í‰ê°€í•œ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ë§Œ ì—…ë°ì´íŠ¸
        latest_scores[payload.cluster] = normalized_score
        
        # 4. ì—…ë°ì´íŠ¸ëœ ì ìˆ˜ë¥¼ ë‹¤ì‹œ user_profiles í…Œì´ë¸”ì— ì €ì¥
        update_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .update({"latest_assessment_scores": latest_scores})
            .eq("id", payload.user_id).execute
        )

        # assessment_history í…Œì´ë¸”ì— ì›ë³¸ ê¸°ë¡ ì €ì¥ (ì¶”í›„ ìƒì„¸ ë¶„ì„ìš©)
        history_row = {
            "user_id": payload.user_id,
            "assessment_type": f"deep_dive_{payload.cluster}",
            "scores": {payload.cluster: normalized_score},
            "raw_responses": payload.responses,
        }
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)

        return {"message": "Assessment submitted successfully", "updated_scores": latest_scores}

    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    
