# main.py
from __future__ import annotations

import asyncio
import datetime as dt
import json
import os
import re
import traceback
import random
from typing import Optional, List, Dict, Any, Tuple
import uuid
from localization import DEFAULT_LANG, get_translation, translations

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse
import numpy as np
from pydantic import BaseModel
from supabase import create_client, Client

from ai_moderator import moderate_text
from llm_prompts import (
    REPORT_SUMMARY_PROMPT, WEEKLY_REPORT_SUMMARY_PROMPT_STANDARD, WEEKLY_REPORT_SUMMARY_PROMPT_NEURO,
    call_llm, get_adhd_breakdown_prompt, get_system_prompt, TRIAGE_SYSTEM_PROMPT, FRIENDLY_SYSTEM_PROMPT
)
from rule_based import rule_scoring
from srj5_constants import (
    ASSESSMENT_SCORE_CAP, CLUSTER_TO_DISPLAY_NAME, CLUSTERS, DEEP_DIVE_MAX_SCORES, EMOJI_ONLY_SCORE_CAP, FINAL_FUSION_WEIGHTS_NO_ICON, ICON_TO_CLUSTER, ONBOARDING_MAPPING,
    FINAL_FUSION_WEIGHTS, FINAL_FUSION_WEIGHTS_NO_TEXT,
    W_LLM, W_RULE, 
    SAFETY_LEMMAS, SAFETY_LEMMA_COMBOS, SAFETY_REGEX, SAFETY_FIGURATIVE
)


try:
    from kiwipiepy import Kiwi
    _kiwi = Kiwi()
except ImportError:
    _kiwi = None
    print(get_translation("log_error_kiwi_not_installed", DEFAULT_LANG))


# --- í™˜ê²½ì„¤ì • ---
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
# BIND_HOST = os.getenv("BIND_HOST", "0.0.0.0")
# PORT = int(os.getenv("PORT", "8000"))
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
# Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
supabase: Optional[Client] = None


# --- FastAPI ì•± ì´ˆê¸°í™” ë° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
app = FastAPI(title="DailyMoji API v2 (Separated Logic)")

# supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@app.on_event("startup")
def startup_event():
    global supabase
    if SUPABASE_URL and SUPABASE_KEY:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print(get_translation("log_startup_success", DEFAULT_LANG))

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- ë°ì´í„° ëª¨ë¸ (ë¶„ë¦¬ëœ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •) ---


# ì´ì „ ëŒ€í™” ê¸°ë¡(history)ì„ ë°›ê¸° ìœ„í•œ ëª¨ë¸ ìˆ˜ì •
class HistoryItem(BaseModel):
    sender: str
    content: str

# /analyze ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AnalyzeRequest(BaseModel):
    user_id: str
    text: str
    icon: Optional[str] = None
    language_code: Optional[str] = 'ko'
    timestamp: Optional[str] = None
    onboarding: Optional[Dict[str, Any]] = None
    character_personality: Optional[str] = None
    history: Optional[List[HistoryItem]] = None 
    # ADHD ë¶„ê¸° ë¡œì§ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒíƒœ ì •ë³´
    adhd_context: Optional[Dict[str, Any]] = None


# /solutions/propose ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class SolutionRequest(BaseModel):
    user_id: str
    session_id: str
    top_cluster: str
    language_code: str = 'ko'

      
class FeedbackRequest(BaseModel):
    user_id: str
    solution_id: str
    session_id: Optional[str] = None
    solution_type: str
    feedback: str
    language_code: Optional[str] = 'ko'


class BackfillRequest(BaseModel):
    start_date: str  # "YYYY-MM-DD" í˜•ì‹
    end_date: str    # "YYYY-MM-DD" í˜•ì‹
    user_id: Optional[str] = None  # íŠ¹ì • ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)
    language_code: Optional[str] = 'ko'


# /assessment/submit ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AssessmentSubmitRequest(BaseModel):
    user_id: str
    cluster: str  # "neg_low", "neg_high" ë“± í‰ê°€í•œ í´ëŸ¬ìŠ¤í„°
    responses: Dict[str, int] # {"NGL_01": 3, "NGL_02": 2, ...} í˜•íƒœì˜ ë‹µë³€
    language_code: Optional[str] = 'ko'
class DailyReportRequest(BaseModel):
    user_id: str
    date: str
    language_code: str = 'ko'

class WeeklyReportRequest(BaseModel):
    user_id: str
    language_code: Optional[str] = 'ko'

# Flutterì˜ PresetIdsì™€ ë™ì¼í•œ êµ¬ì¡°
class PresetIds:
    FRIENDLY_REPLY = "FRIENDLY_REPLY"
    SOLUTION_PROPOSAL = "SOLUTION_PROPOSAL"
    SAFETY_CRISIS_MODAL = "SAFETY_CRISIS_MODAL"
    EMOJI_REACTION = "EMOJI_REACTION"
    ADHD_PRE_SOLUTION_QUESTION = "ADHD_PRE_SOLUTION_QUESTION"
    ADHD_AWAITING_TASK_DESCRIPTION = "ADHD_AWAITING_TASK_DESCRIPTION"
    ADHD_TASK_BREAKDOWN = "ADHD_TASK_BREAKDOWN"



# ======================================================================
# === kiwië¥¼ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ ì¥ì¹˜ ===
# ======================================================================

# --- ì•ˆì „ ì¥ì¹˜ ë¡œì§ ---

def _find_regex_matches(text: str, patterns: List[str]) -> List[str]:
    return [m.group(0) for pat in patterns for m in re.finditer(pat, text, flags=re.IGNORECASE)]

def _kiwi_detect_safety_lemmas(text: str) -> List[str]:
    if not _kiwi: return []
    try:
        tokens = _kiwi.tokenize(text)
        all_lemmas_in_text = {t.lemma for t in tokens}
        hits = {token.lemma for token in tokens if token.lemma in SAFETY_LEMMAS}
        for combo in SAFETY_LEMMA_COMBOS:
            if combo.issubset(all_lemmas_in_text):
                hits.update(combo)
        return list(hits)
    except Exception as e:
        print(f"Kiwi safety check error: {e}")
        return []

def is_safety_text(text: str, llm_json:  Optional[dict], debug_log: dict) -> Tuple[bool, dict]:
    """
    ì ìˆ˜ ì²´ê³„ ë° ì•ˆì „ ëª¨ë“œ ì„¤ëª…: ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì—ì„œ ìí•´/ìì‚´ ìœ„í—˜ì„ ë‹¤ë‹¨ê³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    - 1ë‹¨ê³„: "ì¡¸ë ¤ ì£½ê² ë‹¤"ì™€ ê°™ì´ ëª…ë°±íˆ ì•ˆì „í•œ ë¹„ìœ ì  í‘œí˜„ì„ ë¨¼ì € ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
    - 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„('ì£½ë‹¤', 'ìì‚´' ë“±)ê³¼ LLMì˜ ì˜ë„ ë¶„ì„('self_harm' í”Œë˜ê·¸)ìœ¼ë¡œ ìœ„í—˜ ì‹ í˜¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    - "ë‹¤ ë•Œë ¤ì¹˜ìš°ê³  ì‹¶ë‹¤"ì™€ ê°™ì€ ë¬¸ì¥ì€ ëª…ì‹œì ì¸ ìí•´ ë‹¨ì–´ê°€ ì—†ì–´ 1, 2ë‹¨ê³„ë¥¼ í†µê³¼í•  ìˆ˜ ìˆì§€ë§Œ,
      LLMì´ ë¬¸ì¥ì˜ ì ˆë§ì ì¸ ë‰˜ì•™ìŠ¤ë¥¼ 'self_harm: possible'ë¡œ íŒë‹¨í•˜ë©´ ì•ˆì „ ì¥ì¹˜ê°€ ë°œë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ì´ëŸ¬í•œ ì˜¤íƒì§€ëŠ” ëª¨ë¸ì˜ ë³´ìˆ˜ì ì¸ ì•ˆì „ ì„¤ê³„ ë•Œë¬¸ì´ë©°, ì§€ì†ì ì¸ í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """

    # 1ë‹¨ê³„: ë¹„ìœ ì /ê´€ìš©ì  í‘œí˜„ ìš°ì„  í•„í„°ë§ ("ì¡¸ë ¤ ì£½ê² ë‹¤" ë“±)
    # SAFETY_FIGURATIVEì— ë§¤ì¹˜ë˜ë©´, ìœ„í—˜í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ ì¢…ë£Œ
    figurative_matches = [m.group(0) for pat in SAFETY_FIGURATIVE for m in re.finditer(pat, text, flags=re.IGNORECASE)]
    if figurative_matches:
        debug_log["safety"] = {
            "triggered": False,
            "reason": get_translation("log_safety_figurative", DEFAULT_LANG),
            "matches": figurative_matches
        }
        return False, {}

    # 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„ ë° LLM ì˜ë„ ë¶„ì„
    kiwi_lemma_hits = []
    if _kiwi:
        try:
            tokens = _kiwi.tokenize(text)
            lemmas_in_text = {t.lemma for t in tokens}
            hits = {t.lemma for t in tokens if t.lemma in SAFETY_LEMMAS}
            for combo in SAFETY_LEMMA_COMBOS:
                if combo.issubset(lemmas_in_text):
                    hits.update(combo)
            kiwi_lemma_hits = list(hits)
        except Exception as e:
            print(f"Kiwi safety check error: {e}")

    safety_llm_flag = (llm_json or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}
    
    # Kiwi ë˜ëŠ” LLM ì¤‘ í•˜ë‚˜ë¼ë„ ìœ„í—˜ ì‹ í˜¸ë¥¼ ê°ì§€í•˜ë©´ ì•ˆì „ì¥ì¹˜ ë°œë™
    triggered = bool(kiwi_lemma_hits) or safety_llm_flag

    debug_log["safety"] = {
        "kiwi_lemma_hits": kiwi_lemma_hits,
        "llm_intent_flag": safety_llm_flag,
        "triggered": triggered
    }

    if triggered:
        # ìœ„ê¸° ìƒí™©ì— ë§ëŠ” ê·¹ë‹¨ì ì¸ ì ìˆ˜ ë¶€ì—¬
        crisis_scores = {"neg_low": 0.95, "neg_high": 0.0, "adhd": 0.0, "sleep": 0.0, "positive": 0.0}
        return True, crisis_scores

    return False, {}



# ======================================================================
# === í•µì‹¬ ë¡œì§: ìŠ¤ì½”ì–´ë§ ë° ìœµí•© (Scoring & Fusion) ===
# ======================================================================

def calculate_final_scores(
    text_scores: dict,
    assessment_scores: dict,
    icon_scores: dict,
    has_icon: bool
) -> Tuple[dict, dict]:
    """í…ìŠ¤íŠ¸, ë§ˆìŒ ì ê²€, ì•„ì´ì½˜ ì ìˆ˜ë¥¼ ì¤‘ì•™ì—ì„œ ìœµí•©í•©ë‹ˆë‹¤."""
    if not has_icon:
        # CASE 1: í…ìŠ¤íŠ¸ë§Œ ì…ë ¥ ì‹œ -> ì•„ì´ì½˜ ê°€ì¤‘ì¹˜ë¥¼ ë¹„ë¡€ ë°°ë¶„
        w = FINAL_FUSION_WEIGHTS_NO_ICON
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": 0.0}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment']
        ) for c in CLUSTERS}
    else:
        # CASE 2: í…ìŠ¤íŠ¸ + ì•„ì´ì½˜ ì…ë ¥ ì‹œ -> ëª¨ë“  ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        w = FINAL_FUSION_WEIGHTS
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": w['icon']}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment'] +
            icon_scores.get(c, 0.0) * w['icon']
        ) for c in CLUSTERS}

    return final_scores, weights_used

def calculate_text_scores(text: str, llm_json: Optional[dict]) -> dict:
    """Rule-based ì ìˆ˜ì™€ LLM ì ìˆ˜ë¥¼ ìœµí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    rule_scores, _, _ = rule_scoring(text)
    
    text_if = {c: 0.0 for c in CLUSTERS}
    if llm_json and not llm_json.get("error"):
        I, F = llm_json.get("intensity", {}), llm_json.get("frequency", {})
        for c in CLUSTERS:
            In = clip01((I.get(c, 0.0) or 0.0) / 3.0)
            Fn = clip01((F.get(c, 0.0) or 0.0) / 3.0)
            text_if[c] = clip01(0.6 * In + 0.4 * Fn + 0.1 * rule_scores.get(c, 0.0))
    
    fused_scores = {c: clip01(W_RULE * rule_scores.get(c, 0.0) + W_LLM * text_if.get(c, 0.0)) for c in CLUSTERS}
    return fused_scores

# ======================================================================
# === í—¬í¼ í•¨ìˆ˜ (Helpers) ===
# ======================================================================

def _format_scores_for_print(scores: dict) -> str:
    """ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜"""
    if not isinstance(scores, dict):
        return str(scores)
    return json.dumps({k: round(v, 2) if isinstance(v, float) else v for k, v in scores.items()})

def clip01(x: float) -> float: return max(0.0, min(1.0, float(x)))

def g_score(final_scores: dict) -> float:
    w = {"neg_high": 1.0, "neg_low": 0.9, "sleep": 0.7, "adhd": 0.6, "positive": -0.3}
    g = sum(final_scores.get(k, 0.0) * w.get(k, 0.0) for k in CLUSTERS)
    return round(clip01((g + 1.0) / 2.0), 3)

def pick_profile(final_scores: dict, llm: Optional[dict]) -> int: # surveys íŒŒë¼ë¯¸í„° ì œê±°
    if (llm or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}: return 1
    if max(final_scores.get("neg_low", 0), final_scores.get("neg_high", 0)) > 0.85: return 1
    # if (surveys and (surveys.get("phq9", 0) >= 10 or surveys.get("gad7", 0) >= 10)) or max(final_scores.values()) > 0.60: return 2
    if max(final_scores.values()) > 0.60: return 2 # surveys í•„ë“œ ì œê±°
    if max(final_scores.values()) > 0.30: return 3
    return 0

def calculate_baseline_scores(onboarding_scores: Optional[Dict[str, int]]) -> Dict[str, float]:
    if not onboarding_scores: return {c: 0.0 for c in CLUSTERS}
    baseline = {c: 0.0 for c in CLUSTERS}
    for q_key, score in onboarding_scores.items():
        processed_score = 3 - score if q_key == 'q7' else score
        if q_key in ONBOARDING_MAPPING:
            normalized_score = processed_score / 3.0
            for mapping in ONBOARDING_MAPPING[q_key]:
                baseline[mapping["cluster"]] += normalized_score * mapping["w"]
    for c in CLUSTERS:
        baseline[c] = clip01(baseline.get(c, 0.0))
    return baseline

# ======================================================================
# === DB ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ ===
# ======================================================================
async def get_user_info(user_id: str, lang_code: str = 'ko') -> Tuple[str, str]:
    """ì‚¬ìš©ì ë‹‰ë„¤ì„ê³¼ ìºë¦­í„° ì´ë¦„ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    default_user = get_translation("default_user_name", lang_code) # localization.pyì— ì¶”ê°€ í•„ìš”
    default_char = get_translation("default_char_name", lang_code) # localization.pyì— ì¶”ê°€ í•„ìš”

    if not supabase:
        return default_user, default_char
    
    try:
        res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("user_nick_nm, character_nm")
            .eq("id", user_id).single().execute
        )
        if res.data:
            user_nick = res.data.get("user_nick_nm") or default_user
            char_name = res.data.get("character_nm") or default_char
            return user_nick, char_name
        else:
             # ì‚¬ìš©ìê°€ DBì— ìˆì§€ë§Œ ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° (ê±°ì˜ ì—†ìŒ)
             return default_user, default_char
    except Exception as e: 
        print(get_translation("log_error_fetch_user_info", DEFAULT_LANG, user_id=user_id, error=str(e)))
        return default_user, default_char


# ëª¨ë“  ë©˜íŠ¸ ì¡°íšŒë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def get_mention_from_db(mention_type: str, language_code: str, **kwargs) -> str:
    """DBì—ì„œ ì§€ì •ëœ íƒ€ì…ê³¼ ì¡°ê±´ì— ë§ëŠ” ìºë¦­í„° ë©˜íŠ¸ë¥¼ ëœë¤ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"--- âœ… get_mention_from_db (type: {mention_type}, lang: {language_code}) âœ… ---")

    default_keys = {
        "analysis": "default_analysis_message",
        "reaction": "default_reaction_message",
        "propose": "default_propose_message",
        "home": "default_home_message",
        "followup_user_closed": "default_followup_user_closed",
        "followup_video_ended": "default_followup_video_ended",
        "decline_solution": "default_decline_solution",
        "adhd_question": "adhd_ask_task"
    }
    default_key = default_keys.get(mention_type, "...")

    # .format()ì— ì‚¬ìš©ë  ì¸ìë“¤ì„ ë¯¸ë¦¬ ì¤€ë¹„í•©ë‹ˆë‹¤.
    format_args = kwargs.get("format_kwargs", kwargs)

    default_message = get_translation(default_key, language_code, **(format_args or {}))

    def _safe_format(text: str) -> str:
        """KeyError ì—†ì´ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            # format_argsê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ dictìœ¼ë¡œ ì²˜ë¦¬
            return text.format(**(format_args or {}))
        except KeyError as e:
            print(get_translation("log_warn_missing_format_key", DEFAULT_LANG, key=str(e), lang=language_code, text=text))
            try:
                placeholders = re.findall(r'\{([^}]+)\}', text)
                safe_kwargs = {**{p: f"{{{p}}}" for p in placeholders}, **(format_args or {})}
                return text.format(**safe_kwargs)
            except:
                return text # Final fallback

    if not supabase:
        return default_message


    # if not supabase: return default_message.format(**kwargs) if kwargs else default_message
    try:
        query = supabase.table("character_mentions").select("text").eq("mention_type", mention_type).eq("language_code", language_code)
        
        valid_filter_keys = ["personality", "cluster", "level", "solution_variant"]
        for key, value in kwargs.items():
            if key in valid_filter_keys and value:
                query = query.eq(key, value)

        response = await run_in_threadpool(query.execute)
        scripts = [row['text'] for row in response.data]
        
        if not scripts:
            print(get_translation("log_warn_no_mention_found", DEFAULT_LANG, mention_type=mention_type, lang=language_code, filters=kwargs))
            return default_message # Return already formatted default
        
        selected_script = random.choice(scripts)
        # DBì—ì„œ ê°€ì ¸ì˜¨ ë©˜íŠ¸ë„ ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…
        return _safe_format(selected_script)

    except Exception as e:
        print(get_translation("log_error_get_mention", DEFAULT_LANG, error=str(e)))
        return default_message


# # ìˆ˜ì¹˜ë¥¼ ì£¼ê¸°ë³´ë‹¤ëŠ”, ì‹¬ê°ë„ 3ë‹¨ê³„ì— ë”°ë¼ ë©”ì‹œì§€ í•´ì„í•´ì£¼ëŠ”ê²Œ ë‹¬ë¼ì§(ìˆ˜ì¹˜í˜•x, ëŒ€í™”í˜•o)
# async def get_analysis_message(
#     scores: dict, 
#     personality: Optional[str], 
#     language_code: str
# ) -> str:
#     if not scores: return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ë” ë“¤ì—¬ë‹¤ë³´ê³  ìˆì–´ìš”."
#     top_cluster = max(scores, key=scores.get)
#     score_val = scores[top_cluster]
    
#     level = "low"
#     if score_val > 0.7: level = "high"
#     elif score_val > 0.4: level = "mid"
    
#     return await get_mention_from_db(
#         mention_type="analysis",
#         personality=personality,
#         language_code=language_code,
#         cluster=top_cluster,
#         level=level,
#         default_message="ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
#         format_kwargs={"emotion": top_cluster, "score": int(score_val * 100)}
#     )
    


async def save_analysis_to_supabase(
    payload: AnalyzeRequest, profile: int, g: float,
    intervention: dict, debug_log: dict, final_scores: dict,
    lang_code: str = 'ko'
) -> Optional[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not supabase: return None
    try:
        user_id = payload.user_id

        # ì„¸ì…˜ì„ ì €ì¥í•˜ê¸° ì „, user_profilesì— í•´ë‹¹ ìœ ì €ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        profile_query = supabase.table("user_profiles").select("id").eq("id", user_id)
        profile_response = await run_in_threadpool(profile_query.execute)
        
        if not profile_response.data:
            print(get_translation("error_user_profile_not_found_creating", lang_code, user_id=user_id))            
            insert_query = supabase.table("user_profiles").insert({
                "id": user_id, 
                "user_nick_nm": get_translation("default_user_name", lang_code)
            })
            await run_in_threadpool(insert_query.execute)


        session_row = {
            "user_id": user_id, "text": payload.text, "icon": payload.icon,
            "profile": int(profile), "g_score": float(g),
            "intervention": json.dumps(intervention, ensure_ascii=False),
            "debug_log": json.dumps(debug_log, ensure_ascii=False),
            "summary": (debug_log.get("llm") or {}).get("summary", ""),
        }
        
        session_insert_query = supabase.table("sessions").insert(session_row)
        response = await run_in_threadpool(session_insert_query.execute)
        
        session_data = response.data[0] if response.data else {}
        new_session_id = session_data.get('id')

        if not new_session_id:
            print(get_translation("error_failed_to_save_session", lang_code))
            return None
        
        print(get_translation("log_session_saved", lang_code, session_id=new_session_id))

        if final_scores:
            score_rows = [
                {"session_id": new_session_id, "user_id": payload.user_id, "cluster": c, "score": v}
                for c, v in final_scores.items()
            ]
            if score_rows:
                for row in score_rows:
                    row["session_text"] = payload.text
                
                # .executeë¥¼ ë¶„ë¦¬
                scores_insert_query = supabase.table("cluster_scores").insert(score_rows)
                await run_in_threadpool(scores_insert_query.execute)
        
        return new_session_id
    except Exception as e:
        print(get_translation("error_supabase_save_failed", lang_code, error=str(e)))
        traceback.print_exc()
        return None


# RIN: ADHD ì§ˆë¬¸ì— ëŒ€í•œ ì‚¬ìš©ì ë‹µë³€ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def _handle_adhd_response(payload: AnalyzeRequest, debug_log: dict, lang_code: str = 'ko'):
    user_response = payload.text
    adhd_context = payload.adhd_context or {}
    current_step = adhd_context.get("step")
    user_nick_nm, _ = await get_user_info(payload.user_id, lang_code) 

    # --- ì‹œë‚˜ë¦¬ì˜¤ 1: "ìˆì–´!" / "ì—†ì–´!" ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ ---
    if current_step == "awaiting_choice":
        # "ìˆì–´!" / "ì—†ì–´!" ë²„íŠ¼ì— ëŒ€í•œ ì‘ë‹µ ì²˜ë¦¬
        if "adhd_has_task" in user_response:
            # ë‹¤ìŒ ë‹¨ê³„: í•  ì¼ì´ ë¬´ì—‡ì¸ì§€ ë¬¼ì–´ë³´ê¸°
            question_text = await get_mention_from_db(
                mention_type="adhd_ask_task",
                language_code=lang_code,
                personality=payload.character_personality,
                format_kwargs={"user_nick_nm": user_nick_nm}
            )
            return {
                "intervention": {
                    "preset_id": PresetIds.ADHD_AWAITING_TASK_DESCRIPTION,
                    "text": question_text,
                    "adhd_context": {"step": "awaiting_task_description"}
                }
            }
        else: # "adhd_no_task"
         # "ì—†ì–´!"ë¥¼ ëˆ„ë¥¸ ê²½ìš° -> í˜¸í¡ ë° ì§‘ì¤‘ë ¥ í›ˆë ¨ ë§ˆìŒ ê´€ë¦¬ íŒ ì œì•ˆ
            
            # 1. 'ì§‘ì¤‘ë ¥ í›ˆë ¨' ë§ˆìŒ ê´€ë¦¬ íŒì„ DBì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
            focus_solution_query = supabase.table("solutions").select("solution_id, solution_type").eq("cluster", "adhd").eq("solution_variant", "focus_training").limit(1)
            focus_solution_res = await run_in_threadpool(focus_solution_query.execute)
            focus_solution_data = focus_solution_res.data[0] if focus_solution_res.data else {}

            # 2. 'í˜¸í¡' ë§ˆìŒ ê´€ë¦¬ íŒ - í”„ë¡ íŠ¸ì—”ë“œ ë¼ìš°íŒ…ì„ ìœ„í•´ì„œ!
            breathing_solution_data = {
            "solution_id": "breathing_default", 
            "solution_type": "breathing"
            }
            
              # 3. ì œì•ˆ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            proposal_text = await get_mention_from_db(
                "propose", 
                lang_code,
                cluster="adhd", 
                personality=payload.character_personality,
                format_kwargs={"user_nick_nm": user_nick_nm}
            )     

            # ë§ˆìŒ ê´€ë¦¬ íŒ ì œì•ˆ ì‹œì ì— session ìƒì„±
            intervention_for_db = { "preset_id": PresetIds.SOLUTION_PROPOSAL, "proposal_text": proposal_text}
            session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention_for_db, debug_log, {}, lang_code)
        

            return {
                "intervention": {
                    "preset_id": PresetIds.SOLUTION_PROPOSAL,
                    "proposal_text": proposal_text,
                    "options": [
                         { "label": get_translation("label_breathing", lang_code), # ğŸ¥‘ Use get_translation
                           "action": "accept_solution", "solution_id": breathing_solution_data.get("solution_id"), "solution_type": "breathing" },
                         { "label": get_translation("label_focus_training", lang_code), # ğŸ¥‘ Needs key "label_focus_training"
                           "action": "accept_solution", "solution_id": focus_solution_data.get("solution_id"), "solution_type": focus_solution_data.get("solution_type") },
                    ],
                    "session_id": session_id
                }
            }

        
            # --- ì‹œë‚˜ë¦¬ì˜¤ 2: ì‚¬ìš©ìê°€ í•  ì¼ì„ ì…ë ¥í–ˆì„ ë•Œ ---
    elif current_step == "awaiting_task_description":
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í•  ì¼ ë‚´ìš©ì„ ë°›ì•„ ì²˜ë¦¬
        user_nick_nm, _ = await get_user_info(payload.user_id, lang_code)
        
        # ì„±ê²©ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        prompt_template = get_adhd_breakdown_prompt(payload.character_personality)
        
        # ê°€ì ¸ì˜¨ í…œí”Œë¦¿ì— ë³€ìˆ˜ë¥¼ ì±„ì›Œ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
        final_prompt = prompt_template.format(user_nick_nm=user_nick_nm, user_message=user_response)
        
        breakdown_result = await call_llm(
            system_prompt=final_prompt, # ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ system_promptë¡œ ì‚¬ìš©
            user_content="", # user_contentëŠ” ë¹„ì›Œë‘ê¸°
            openai_key=OPENAI_KEY, 
            lang_code=lang_code,
            expect_json=True
        )
        
        coaching_text = breakdown_result.get("coaching_text", get_translation("adhd_fallback_coaching", lang_code))
        mission_text = breakdown_result.get("mission_text", get_translation("adhd_fallback_mission", lang_code))

         # ë½€ëª¨ë„ë¡œ ë§ˆìŒ ê´€ë¦¬ íŒ ì •ë³´ ì¡°íšŒ
        solution_query = supabase.table("solutions").select("solution_id, solution_type").eq("cluster", "adhd").eq("solution_variant", "pomodoro").limit(1)
        solution_res = await run_in_threadpool(solution_query.execute)
        solution_data = solution_res.data[0] if solution_res.data else {}

        # DBì— ì €ì¥í•  intervention ê°ì²´ ë¨¼ì € ìƒì„±
        intervention_for_db = { 
            "preset_id": PresetIds.ADHD_TASK_BREAKDOWN, 
            "coaching_text": coaching_text, 
            "mission_text": mission_text 
        }

        # ë½€ëª¨ë„ë¡œ ì œì•ˆ ì‹œì ì— session ìƒì„±
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention_for_db, debug_log, {}, lang_code)

        # intervention ê°ì²´ ì•ˆì— optionsì™€ session_idë¥¼ í¬í•¨ì‹œì¼œ í•œë²ˆì— ë°˜í™˜í•©ë‹ˆë‹¤.
        intervention_for_client = intervention_for_db.copy()
        intervention_for_client["options"] = [
            {
                "label": get_translation("label_pomodoro_mission", lang_code),
                "action": "accept_solution",
                "solution_id": solution_data.get("solution_id"),
                "solution_type": solution_data.get("solution_type")
            }
        ]
        intervention_for_client["session_id"] = session_id

        return { "intervention": intervention_for_client }

# ---------- API Endpoints (ë¶„ë¦¬ëœ êµ¬ì¡°) ----------


# ======================================================================
# === /analyze ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ë¡œì§ (ë¶„ë¦¬ëœ í•¨ìˆ˜ë“¤) ===
# ======================================================================

async def _handle_moderation(text: str, lang_code: str = 'ko') -> bool:
    """OpenAI Moderation APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ í•´ ì½˜í…ì¸ ë¥¼ í™•ì¸í•˜ê³  ì°¨ë‹¨ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    is_flagged, categories = await moderate_text(text, OPENAI_KEY, lang_code)
    if not is_flagged:
        return False

    allowed_categories = {'self-harm', 'self-harm/intent', 'self-harm/instructions', 'hate', 'harassment', 'violence'}
    should_block = any(cat not in allowed_categories and triggered for cat, triggered in categories.items())
    
    if should_block:
        print(get_translation("log_moderation_blocked", DEFAULT_LANG, text=text, categories=categories))
    else:
        print(get_translation("log_moderation_passed", DEFAULT_LANG, text=text, categories=categories))
    
    return should_block

async def _handle_emoji_only_case(payload: AnalyzeRequest, debug_log: dict, lang_code: str = 'ko'):
    """ì•„ì´ì½˜ë§Œ ì…ë ¥ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "EMOJI_REACTION"
    print(f"\n--- ğŸ§ EMOJI-ONLY ANALYSIS (Lang: {lang_code}) ---")

    selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower(), "neg_low")
    
    if selected_cluster == "neutral":
        intervention = {"preset_id": PresetIds.EMOJI_REACTION, "text": get_translation("neutral_emoji_response", lang_code), "top_cluster": "neutral"}        
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {}, lang_code)

        return {"session_id": session_id, "intervention": intervention}
    
    assessment_scores = calculate_baseline_scores(payload.onboarding) # ì´ëª¨ì§€ë§Œ ìˆì„ ë• ì˜¨ë³´ë”© ì ìˆ˜ ì‚¬ìš©
    icon_scores = {c: 1.0 if c == selected_cluster else 0.0 for c in CLUSTERS}
   
    w = FINAL_FUSION_WEIGHTS_NO_TEXT
    fused_scores = {c: clip01(assessment_scores.get(c, 0.0) * w['assessment'] + icon_scores.get(c, 0.0) * w['icon']) for c in CLUSTERS}
    
    # ì ìˆ˜ ìƒí•œì„ (Cap) ì ìš©
    final_scores = fused_scores.copy()
    if selected_cluster in final_scores:
        final_scores[selected_cluster] = min(final_scores[selected_cluster], EMOJI_ONLY_SCORE_CAP)
   
    g, profile = g_score(final_scores), pick_profile(final_scores, None)
    
    user_nick_nm, _ = await get_user_info(payload.user_id, lang_code)
    reaction_text = await get_mention_from_db("reaction", lang_code, personality=payload.character_personality, cluster=selected_cluster, user_nick_nm=user_nick_nm)
    intervention = {"preset_id": PresetIds.EMOJI_REACTION, "top_cluster": selected_cluster, "empathy_text": reaction_text}
    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores, lang_code)
    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}


async def _handle_friendly_mode(payload: AnalyzeRequest, debug_log: dict, lang_code: str = 'ko') -> dict:
    """Triage ê²°ê³¼ê°€ 'ì¹œêµ¬ ëª¨ë“œ'ì¼ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "FRIENDLY"
    print(f"\n--- ğŸ‘‹ FRIENDLY MODE (Lang: {lang_code}): '{payload.text}' ---")

    user_nick_nm, character_nm = await get_user_info(payload.user_id, lang_code)
    system_prompt = get_system_prompt(
        mode='FRIENDLY', personality=payload.character_personality, language_code=lang_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )
    # ì´ì „ ëŒ€í™” ê¸°ì–µ: ì¹œêµ¬ ëª¨ë“œì—ì„œë„ ëŒ€í™” ê¸°ë¡ì„ user_contentì— í¬í•¨
    history_str = "\n".join([f"{h.sender}: {h.content}" for h in payload.history]) if payload.history else ""
    user_content = f"Previous conversation:\n{history_str}\n\nCurrent message: {payload.text}"

    llm_response = await call_llm(system_prompt, user_content, OPENAI_KEY, lang_code=lang_code, expect_json=False)

    # LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì—ëŸ¬ì¸ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
    final_text = llm_response if not (isinstance(llm_response, dict) and 'error' in llm_response) else get_translation("default_llm_friendly_fallback", lang_code)
    intervention = {"preset_id": PresetIds.FRIENDLY_REPLY, "text": final_text}
    
    session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {}, lang_code)    
    return {"session_id": session_id, "intervention": intervention}


async def _run_analysis_pipeline(payload: AnalyzeRequest, debug_log: dict, lang_code: str = 'ko') -> dict: 
    """Triage ê²°ê³¼ê°€ 'ë¶„ì„ ëª¨ë“œ'ì¼ ê²½ìš°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "ANALYSIS"
    print(f"\n--- ğŸ§ ANALYSIS MODE (Lang: {lang_code}): '{payload.text}' ---")

     # 1. ì‚¬ìš©ìì˜ ìµœì‹  í‰ê°€ ì ìˆ˜(assessment_scores)ë¥¼ DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #    ì´ ì ìˆ˜ëŠ” ì˜¨ë³´ë”©ìœ¼ë¡œ ì‹œì‘í•´ì„œ, ì‹¬ì¸µ ë¶„ì„ì„ í•  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
    user_nick_nm, character_nm = await get_user_info(payload.user_id, lang_code)    
   
    profile_res = await run_in_threadpool(
        supabase.table("user_profiles")
        .select("latest_assessment_scores")
        .eq("id", payload.user_id).single().execute
    )
    
    assessment_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
    if not assessment_scores or not isinstance(assessment_scores, dict):
        # ìµœì‹  í‰ê°€ ì ìˆ˜ê°€ ì—†ìœ¼ë©´(ì˜ˆ: ì²« ì‚¬ìš©ì), ì˜¨ë³´ë”© ì ìˆ˜ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.
        print(get_translation("log_warn_no_assessment_scores", DEFAULT_LANG))
        assessment_scores = calculate_baseline_scores(payload.onboarding)
    # assessment_scoresì— ìƒí•œì„ (Cap)ì„ ì ìš©
    for cluster in assessment_scores:
        assessment_scores[cluster] = min(assessment_scores.get(cluster, 0.0), ASSESSMENT_SCORE_CAP)

    # --------------------------------------------------------------------------
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ 
    system_prompt = get_system_prompt(
        mode='ANALYSIS', personality=payload.character_personality, language_code=lang_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )

   # ì´ì „ ëŒ€í™” ê¸°ì–µ: ë¶„ì„ ëª¨ë“œì—ì„œë„ LLM í˜¸ì¶œ ì‹œ historyë¥¼ í¬í•¨
    history_for_llm = [h.dict() for h in payload.history] if payload.history else []
    llm_payload = {"user_message": payload.text, "baseline_scores": assessment_scores, "history": history_for_llm}
   
    # 2. LLM í˜¸ì¶œ ë° 2ì°¨ ì•ˆì „ ì¥ì¹˜
    llm_json = await call_llm(system_prompt, json.dumps(llm_payload, ensure_ascii=False), OPENAI_KEY, lang_code=lang_code)
    debug_log["llm"] = llm_json

    is_crisis, crisis_scores = is_safety_text(payload.text, llm_json, debug_log)
    if is_crisis:
        print(get_translation("log_safety_triggered_2nd", DEFAULT_LANG, text=payload.text))
        g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
        intervention = {
            "preset_id": PresetIds.SAFETY_CRISIS_MODAL,
            "analysis_text": get_translation("safety_crisis_text", lang_code), # ğŸ¥‘ Use translation
            "cluster": top_cluster,
            "solution_id": f"{top_cluster}_crisis_01"
        }        
        session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores, lang_code)
        return {"session_id": session_id, "final_scores": crisis_scores, "g_score": g, "profile": profile, "intervention": intervention}


    # 3. ëª¨ë“  ì ìˆ˜ ê³„ì‚° ë° ìœµí•©
    text_scores = calculate_text_scores(payload.text, llm_json)
    
    has_icon = payload.icon and ICON_TO_CLUSTER.get(payload.icon.lower()) != "neutral"
    icon_scores = {c: 0.0 for c in CLUSTERS}
    if has_icon:
        icon_scores[ICON_TO_CLUSTER.get(payload.icon.lower())] = 1.0


    final_scores, weights_used = calculate_final_scores(text_scores, assessment_scores, icon_scores, has_icon)
    debug_log["scores"] = {"weights_used": weights_used, "assessment_base": assessment_scores, "text": text_scores, "icon": icon_scores, "final": final_scores}
    print(f"Scores -> Assessment Base: {_format_scores_for_print(assessment_scores)}, Text: {_format_scores_for_print(text_scores)}, Icon: {_format_scores_for_print(icon_scores)}")
    print(f"Weights: {_format_scores_for_print(weights_used)} -> Final Scores: {_format_scores_for_print(final_scores)}")


    # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
    g, profile = g_score(final_scores), pick_profile(final_scores, llm_json)
    top_cluster = max(final_scores, key=final_scores.get, default="neg_low")
    print(f"G-Score: {g:.2f}, Profile: {profile}")
    
    empathy_text = (llm_json or {}).get("empathy_response", get_translation("default_empathy_fallback", lang_code))
    score_val = final_scores[top_cluster]
    level = "high" if score_val > 0.7 else "mid" if score_val > 0.4 else "low"
 
    cluster_display_name = get_translation(f"cluster_{top_cluster}", lang_code)

    # 'analysis' íƒ€ì…ì˜ ë©˜íŠ¸ë¥¼ DBì—ì„œ ê°€ì ¸ì˜´
    analysis_text = await get_mention_from_db(
        "analysis", 
        lang_code,
        personality=payload.character_personality, 
        cluster=top_cluster, 
        level=level, 
        format_kwargs={"emotion": cluster_display_name, "user_nick_nm": user_nick_nm}
    )
    
    # intervention ê°ì²´ ìƒì„± ë° DB ì €ì¥
    intervention = {
        "preset_id": PresetIds.SOLUTION_PROPOSAL, 
        "top_cluster": top_cluster, 
        "empathy_text": empathy_text, 
        "analysis_text": analysis_text
    }

    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores, lang_code)
    
    # --- ì €ì¥ í›„ ìµœì¢… ë°˜í™˜ ê°ì²´ êµ¬ì„± ---
    analysis_result = {
        "session_id": session_id,
        "final_scores": final_scores,
        "g_score": g,
        "profile": profile,
        "intervention": intervention # ì¼ë‹¨ ê¸°ë³¸ intervention í• ë‹¹
    }
    # intervention ê°ì²´ì— session_id ì¶”ê°€ (ADHD ë¶„ê¸°ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
    analysis_result["intervention"]["session_id"] = session_id


    # --- ğŸ¥‘ ADHD ë¶„ê¸° ë¡œì§ (ì„¸ì…˜ ì €ì¥ *í›„*, ìµœì¢… ë°˜í™˜ *ì „*) ---
    if top_cluster == "adhd":
        print(get_translation("log_adhd_detected", lang_code)) # ë¡œê·¸ ë²ˆì—­ ì‚¬ìš©

        question_text_template = await get_mention_from_db(
            mention_type="adhd_question",
            language_code=lang_code,
            personality=payload.character_personality,
            format_kwargs={"user_nick_nm": user_nick_nm}
        )

        # ê³µê° ë©˜íŠ¸ê°€ ìˆìœ¼ë©´ ì§ˆë¬¸ ì•ì— ë¶™ì—¬ì¤Œ
        final_question_text = f"{empathy_text} {question_text_template}".strip()

        # analysis_resultì˜ intervention ë‚´ìš©ì„ ADHD ì§ˆë¬¸ í˜•íƒœë¡œ *ë®ì–´ì“°ê¸°*
        analysis_result["intervention"] = {
            "preset_id": PresetIds.ADHD_PRE_SOLUTION_QUESTION,
            "text": final_question_text,
            "options": [
                {"label": get_translation("label_adhd_has_task", lang_code), "action": "adhd_has_task"},
                {"label": get_translation("label_adhd_no_task", lang_code), "action": "adhd_no_task"}
            ],
            "adhd_context": { "step": "awaiting_choice" },
            "session_id": session_id # session_idëŠ” ìœ ì§€
        }
        # top_cluster, empathy_text, analysis_textëŠ” ADHD ì§ˆë¬¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ì œê±°

    # --- ìµœì¢… ê²°ê³¼ ë°˜í™˜ ---
    return analysis_result


# ======================================================================
# ===          ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
 
@app.post("/analyze")
async def analyze_emotion(payload: AnalyzeRequest):
    """ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì™€ ì•„ì´ì½˜ì„ ë°›ì•„ ê°ì •ì„ ë¶„ì„í•˜ê³  ìŠ¤ì½”ì–´ë§ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = (payload.text or "").strip()
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG
    debug_log: Dict[str, Any] = {"input": payload.dict()}

    try:
        # --- íŒŒì´í”„ë¼ì¸ 0: ìœ í•´ ì½˜í…ì¸  ê²€ì—´ ---
        if await _handle_moderation(text, lang_code): 
            return JSONResponse(status_code=400, content={"error": get_translation("error_inappropriate_content", lang_code)})
       
        # --- ADHD Context Handling ---
        # ADHD ì»¨í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ë©´, ë‹¤ë¥¸ ëª¨ë“  ë¶„ì„ì„ ê±´ë„ˆë›°ê³  ADHD ë‹µë³€ ì²˜ë¦¬ ë¡œì§ìœ¼ë¡œ ë°”ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        if payload.adhd_context and "step" in payload.adhd_context:
            return await _handle_adhd_response(payload, debug_log, lang_code)


        # --- íŒŒì´í”„ë¼ì¸ 1: ğŸŒ¸ CASE 2 - ì´ëª¨ì§€ë§Œ ìˆëŠ” ê²½ìš° ---
        if payload.icon and not text:
            debug_log["mode"] = "EMOJI_REACTION"
            return await _handle_emoji_only_case(payload, debug_log, lang_code)

        # --- íŒŒì´í”„ë¼ì¸ 2: 1ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM í˜¸ì¶œ ì „) ---
        is_crisis, crisis_scores = is_safety_text(text, None, debug_log)
        if is_crisis:
            print(get_translation("log_safety_triggered_1st", DEFAULT_LANG, text=text))
            g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
            intervention = {
                "preset_id": PresetIds.SAFETY_CRISIS_MODAL,
                "analysis_text": get_translation("safety_crisis_text", lang_code), # ğŸ¥‘ ë²ˆì—­ ì‚¬ìš©
                "cluster": top_cluster,
                "solution_id": f"{top_cluster}_crisis_01"
            }
            session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores, lang_code)
            intervention["session_id"] = session_id
            return {"session_id": session_id, "final_scores": crisis_scores, "g_score": g, "profile": profile, "intervention": intervention}
       
        # --- íŒŒì´í”„ë¼ì¸ 3: Triage (ì¹œêµ¬ ëª¨ë“œ / ë¶„ì„ ëª¨ë“œ ë¶„ê¸°) ---
        rule_scores, _, _ = rule_scoring(text)
        is_simple_text = len(text) < 10 and max(rule_scores.values() or [0.0]) < 0.1
        
        if is_simple_text:
            triage_mode = 'FRIENDLY'
            debug_log["triage_decision"] = "Rule-based: Simple text"
        else:
            triage_system_prompt = get_system_prompt(mode='TRIAGE', personality=None, language_code=lang_code)
            triage_mode = await call_llm(triage_system_prompt, text, OPENAI_KEY, lang_code=lang_code, expect_json=False)
            debug_log["triage_decision"] = f"LLM Triage: {triage_mode}"

        # --- íŒŒì´í”„ë¼ì¸ 4: Triage ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ---
        if triage_mode == 'FRIENDLY':
            return await _handle_friendly_mode(payload, debug_log, lang_code)
        elif triage_mode == 'ANALYSIS':
            analysis_result = await _run_analysis_pipeline(payload, debug_log, lang_code)
            
            if not analysis_result or "error" in analysis_result:
                 # ì—ëŸ¬ ìƒí™© ì²˜ë¦¬ ë˜ëŠ” ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì˜ˆ: ì¹œêµ¬ ëª¨ë“œ ì‘ë‹µ)
                 print(f"âš ï¸ Analysis pipeline failed or returned error. Result: {analysis_result}")
                 # í•„ìš” ì‹œ ì—ëŸ¬ ì‘ë‹µì„ í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬í•˜ê±°ë‚˜ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
                 return await _handle_friendly_mode(payload, debug_log, lang_code) # ì˜ˆì‹œ: ì¹œêµ¬ ëª¨ë“œë¡œ ëŒ€ì²´

            # # --- ADHD ë¶„ê¸° ë¡œì§ (ì´ì œ analysis_result ì•ˆì—ì„œ ì²˜ë¦¬ë¨) ---
            # # _run_analysis_pipeline í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ADHD ë¶„ê¸° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ,
            # # ì—¬ê¸°ì„œëŠ” analysis_resultë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.
            # intervention = analysis_result.get("intervention", {})
            # top_cluster = intervention.get("top_cluster")
            # empathy_text = intervention.get("empathy_text", "")
            # user_nick_nm, _ = await get_user_info(payload.user_id)
            
            
            # # ë§Œì•½ ë¶„ì„ ê²°ê³¼ top_clusterê°€ ADHDë¼ë©´, ë§ˆìŒ ê´€ë¦¬ íŒì„ ë°”ë¡œ ì œì•ˆí•˜ì§€ ì•Šê³  ì§ˆë¬¸ì„ ë˜ì§
            # if top_cluster == "adhd":
            #     print("ğŸ§  ADHD cluster detected. Switching to pre-solution question flow.")
                
            #     question_text_template = await get_mention_from_db(
            #         mention_type="adhd_question", # Assuming key exists
            #         language_code=lang_code, # ğŸ¥‘ Pass lang_code
            #         personality=payload.character_personality,
            #         format_kwargs={"user_nick_nm": user_nick_nm}
            #     )

            #     final_question_text = f"{empathy_text} {question_text_template}"

                
            #     # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì§ˆë¬¸ê³¼ ë‹¤ìŒ ìš”ì²­ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
            #     analysis_result["intervention"] = {
            #         "preset_id": PresetIds.ADHD_PRE_SOLUTION_QUESTION,
            #         "text": final_question_text.strip(), # ìµœì¢… ì¡°í•©ëœ í…ìŠ¤íŠ¸
            #         "options": [
            #             {"label": "ìˆì–´! ë­ë¶€í„° í•˜ë©´ ì¢‹ì„ê¹Œ?", "action": "adhd_has_task"},
            #             {"label": "ì—†ì–´! ì§‘ì¤‘ë ¥ í›ˆë ¨ í• ë˜", "action": "adhd_no_task"}
            #         ],
            #         "adhd_context": { "step": "awaiting_choice" }
            #     }

            return analysis_result
        
        else: # Handle unexpected triage result
            print(get_translation("log_warn_unexpected_triage", DEFAULT_LANG, mode=triage_mode))
            return await _handle_friendly_mode(payload, debug_log, lang_code)

    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/analyze"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        error_msg = get_translation("error_occurred", lang_code, error=str(e))
        raise HTTPException(status_code=500, detail={"error": error_msg, "trace": tb if os.getenv("DEBUG") else None})
    
# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ (ë§ˆìŒ ì ê²€) ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG
    if not supabase: raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))
    try:
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score: raise HTTPException(status_code=400, detail=get_translation("error_invalid_cluster", lang_code, cluster=payload.cluster))

        normalized_score = clip01(total_score / max_score)
        
        profile_res = await run_in_threadpool(supabase.table("user_profiles").select("latest_assessment_scores").eq("id", payload.user_id).single().execute)
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}
        
        latest_scores[payload.cluster] = normalized_score
        
        await run_in_threadpool(supabase.table("user_profiles").update({"latest_assessment_scores": latest_scores}).eq("id", payload.user_id).execute)
        
        history_row = {"user_id": payload.user_id, "assessment_type": f"deep_dive_{payload.cluster}", "scores": {payload.cluster: normalized_score}, "raw_responses": payload.responses}
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)
        
        return {"message": get_translation("assessment_success", lang_code), "updated_scores": latest_scores}
    
    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/assessment/submit"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        raise HTTPException(status_code=500, detail={"error": get_translation("error_occurred", lang_code, error=str(e)), "trace": tb if os.getenv("DEBUG") else None})


# ======================================================================
# ===          ë§ˆìŒ ê´€ë¦¬ íŒ ì œì•ˆ ë° ìƒì„¸ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
   

@app.post("/solutions/propose")
async def propose_solution(payload: SolutionRequest): 
    """
    ë¶„ì„ ê²°ê³¼(top_cluster)ì— ë§ëŠ” í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì œì•ˆí•  ë§ˆìŒ ê´€ë¦¬ íŒ íƒ€ì… ëª©ë¡ì„ ëª…í™•íˆ ì •ì˜í•˜ê³ , í•´ë‹¹ íƒ€ì…ì˜ ë§ˆìŒ ê´€ë¦¬ íŒë§Œ ì°¾ì•„ 
    ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì˜µì…˜ ëª©ë¡ê³¼, ëŒ€í‘œ ì œì•ˆ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    neg_low, sleep: í˜¸í¡, ì˜ìƒ, í–‰ë™ë¯¸ì…˜
    neg_high, positive: í˜¸í¡, ì˜ìƒë§Œ
    adhdëŠ” í• ê±° ìˆëƒì—†ëƒ ë¬¼ì–´ë³´ê³  ìˆìœ¼ë©´ ë½€ëª¨ë„ë¡œ, ì—†ìœ¼ë©´ í˜¸í¡, ì˜ìƒ
    """    
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG

    if not supabase: raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))

    try:
        user_nick_nm, _ = await get_user_info(payload.user_id, lang_code)
        top_cluster = payload.top_cluster

         # 0. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì œì•ˆí•  ë§ˆìŒ ê´€ë¦¬ íŒ íƒ€ì… ëª©ë¡ì„ ì •ì˜í•´ì•¼í•¨
        solution_types_by_cluster = {
            "neg_low": ["breathing", "video", "action"],
            "sleep": ["breathing", "video", "action"],
            "neg_high": ["breathing", "video"],
            "positive": ["breathing", "video"],
            # ADHDëŠ” ë³„ë„ íë¦„ì„ íƒ€ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ë§Œ ì •ì˜
            "adhd": ["breathing", "video"] 
        }
        
        # í˜„ì¬ top_clusterì— í•´ë‹¹í•˜ëŠ” ë§ˆìŒ ê´€ë¦¬ íŒ íƒ€ì… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        target_solution_types = solution_types_by_cluster.get(top_cluster, ["video"])


        # 1. ì‚¬ìš©ìì˜ ê±°ë¶€ íƒœê·¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        profile_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("negative_tags")
            .eq("id", payload.user_id)
            .single().execute
        )
        negative_tags = (profile_res.data or {}).get("negative_tags", [])

        # 2. ì œì•ˆí•  í›„ë³´ ë§ˆìŒ ê´€ë¦¬ íŒ ì „ì²´ë¥¼ DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
        all_candidates_res = await run_in_threadpool(
            supabase.table("solutions")
            .select("*")
            .eq("cluster", top_cluster)
            .execute
        )
        all_candidates = all_candidates_res.data
        
        if not all_candidates:
            return {"proposal_text": get_translation("no_solution_proposal", lang_code), "options": []}

        # # 3. ê±°ë¶€ íƒœê·¸ê°€ í¬í•¨ëœ ë§ˆìŒ ê´€ë¦¬ íŒì€ í›„ë³´ì—ì„œ ì œì™¸
        # if negative_tags:
        #     filtered_candidates = [
        #         sol for sol in all_candidates
        #         if not any(tag in (sol.get("tags") or []) for tag in negative_tags)
        #     ]
        # else:
        #     filtered_candidates = all_candidates

        # 3. í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŒ ê´€ë¦¬ íŒ í•„í„°ë§(1/3 í™•ë¥ ë¡œ ë‚˜ì˜¤ë„ë¡!)
        probabilistically_filtered_candidates = []
        if negative_tags:
            for sol in all_candidates:
                solution_tags = set(sol.get("tags") or [])
                # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                if not solution_tags.isdisjoint(negative_tags):
                    # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ìˆë‹¤ë©´, 1/3 í™•ë¥ ë¡œë§Œ ëª©ë¡ì— ì¶”ê°€
                    if random.random() < (1/3):
                        probabilistically_filtered_candidates.append(sol)
                else:
                    # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ì—†ë‹¤ë©´, ë¬´ì¡°ê±´ ëª©ë¡ì— ì¶”ê°€
                    probabilistically_filtered_candidates.append(sol)
        else:
            # negative_tagsê°€ ì—†ìœ¼ë©´ ëª¨ë“  í›„ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            probabilistically_filtered_candidates = all_candidates
        
        # í•„í„°ë§ í›„ í›„ë³´êµ°ì´ ì—†ìœ¼ë©´ ëª¨ë“  í›„ë³´ë¥¼ ë‹¤ì‹œ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
        if not probabilistically_filtered_candidates:
            probabilistically_filtered_candidates = all_candidates


        # 4. ê° ë§ˆìŒ ê´€ë¦¬ íŒ íƒ€ì…ë³„ë¡œ ëŒ€í‘œ ë§ˆìŒ ê´€ë¦¬ íŒì„ í•˜ë‚˜ì”© ëœë¤ ì„ íƒ
        options = []
        labels = {
            "breathing": get_translation("label_breathing", lang_code),
            "video": get_translation("label_video", lang_code),
            "action": get_translation("label_mission", lang_code)
        }
        default_label = get_translation("label_tip", lang_code)

        # í…ìŠ¤íŠ¸ ì¡°í•©ì„ ìœ„í•´ ì²« ë²ˆì§¸ ë§ˆìŒ ê´€ë¦¬ íŒì˜ ì„¤ëª…ì„ ì €ì¥í•  ë³€ìˆ˜
        first_solution_text = ""

        for sol_type in target_solution_types:

            # 'breathing' íƒ€ì…ì€ DB ì¡°íšŒ ì—†ì´ ê³ ì •ëœ ì˜µì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            if sol_type == 'breathing':
                options.append({
                    "label": labels.get(sol_type),
                    "action": "accept_solution",
                    "solution_id": "breathing_default",
                    "solution_type": "breathing"
                })
                continue

            # 'sleep' í´ëŸ¬ìŠ¤í„°ì˜ 'action' íƒ€ì…ì€ ìˆ˜ë©´ìœ„ìƒ íŒìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
            elif top_cluster == 'sleep' and sol_type == 'action':
                options.append({
                    "label": labels.get(sol_type), 
                    "action": "accept_solution",
                    "solution_id": "sleep_hygiene_tip_random",
                    "solution_type": "action"
                })
                continue
            
            # ê·¸ ì™¸ ëª¨ë“  ê²½ìš°ëŠ” DBì—ì„œ ë§ˆìŒ ê´€ë¦¬ íŒì„ ì°¾ìŠµë‹ˆë‹¤.
            type_candidates = [s for s in probabilistically_filtered_candidates if s.get("solution_type") == sol_type]
            if type_candidates:
                chosen_solution = random.choice(type_candidates)
                
                # 4-1. í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬í•  ë²„íŠ¼ ì˜µì…˜ ëª©ë¡
                options.append({
                    "label": labels.get(sol_type, default_label),
                    "action": "accept_solution",
                    "solution_id": chosen_solution["solution_id"],
                    "solution_type": chosen_solution["solution_type"]
                })

                # 4-2. ì²« ë²ˆì§¸ë¡œ ì„ íƒëœ ë§ˆìŒ ê´€ë¦¬ íŒì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì €ì¥ 
                if not first_solution_text:
                    first_solution_text = chosen_solution.get("text", "")

        if not options:
            return {"proposal_text": get_translation("no_solution_proposal_talk", lang_code), "options": []}
        
        # 5. ì œì•ˆ ë©˜íŠ¸ì™€ ëŒ€í‘œ ë§ˆìŒ ê´€ë¦¬ íŒ ì„¤ëª…ì„ ì¡°í•©í•˜ì—¬ ìµœì¢… ì œì•ˆ í…ìŠ¤íŠ¸ ìƒì„±
        proposal_script = await get_mention_from_db(
            mention_type="propose",
            language_code=lang_code,
            cluster=top_cluster,
            format_kwargs={"user_nick_nm": user_nick_nm}
        )
        final_text = f"{proposal_script} {first_solution_text}".strip()
      
        # 6. ë¡œê·¸ ì €ì¥ ë° ìµœì¢… ê²°ê³¼ ë°˜í™˜
        log_entry = {
            "session_id": payload.session_id, 
            "type": "propose", 
            "solution_id": f"multiple_options_{top_cluster}"
        }
        await run_in_threadpool(supabase.table("interventions_log").insert(log_entry).execute)

        return {"proposal_text": final_text, "options": options}

    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/solutions/propose"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        raise HTTPException(status_code=500, detail={"error": get_translation("error_occurred", lang_code, error=str(e)), "trace": tb if os.getenv("DEBUG") else None})
    
# ======================================================================
# ===          ë§ˆìŒ ê´€ë¦¬ íŒ ì˜ìƒ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================

    # SolutionPageì—ì„œ ì˜ìƒ ë¡œë“œ
    # í•˜ë“œì½”ë”©ëœ SOLUTION_DETAILS_LIBRARYë¥¼ DB ì¡°íšŒë¡œ ëŒ€ì²´í–ˆìŒ!!
@app.get("/solutions/{solution_id}")
async def get_solution_details(solution_id: str, language_code: Optional[str] = 'ko'): 
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    print(f"RIN: âœ… ë§ˆìŒ ê´€ë¦¬ íŒ ìƒì„¸ ì •ë³´ ìš”ì²­ ë°›ìŒ: {solution_id}")
    
    if not supabase:
        raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))    
    try:
        # solutions í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¡°íšŒ
        response = await run_in_threadpool(
            supabase.table("solutions")
            .select("url, start_at, end_at, text") 
            .eq("solution_id", solution_id)
            .single()
            .execute
        )
        
        
        if not response.data:
            raise HTTPException(status_code=404, detail=get_translation("error_solution_not_found", lang_code))

        # ìœ íŠœë¸Œ ë¶ˆëŸ¬ì˜¬ë•Œ startAt, endAt (camelCase)ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ í‚¤ë¥¼ ë³€í™˜í•´ì¤Œ
        # supabaseëŠ” snake_caseë¡œ ì €ì¥í•´ì•¼ í•œë‹¤ê³  í•¨.
        return {
            'url': response.data.get('url'), 
            'startAt': response.data.get('start_at'), 
            'endAt': response.data.get('end_at'),
            'text': response.data.get('text')
            }
        
    except Exception as e:
        print(get_translation("log_error_solution_not_found", DEFAULT_LANG, solution_id=solution_id, error=str(e)))
        raise HTTPException(status_code=404, detail=get_translation("error_solution_not_found", lang_code))
    

# ======================================================================
# ===          ìƒí™©ë³„ ëŒ€ì‚¬ ì œê³µ ì—”ë“œí¬ì¸íŠ¸ (`/dialogue/*`)         ===
# ======================================================================
@app.get("/dialogue/home")
async def get_home_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = None,
    language_code: Optional[str] = 'ko',
    emotion: Optional[str] = None 
):
    """í™ˆ í™”ë©´ì— í‘œì‹œí•  ëŒ€ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    user_name_to_use = user_nick_nm or get_translation("default_user_name", lang_code)

    if emotion:
        # ì´ëª¨ì§€ê°€ ì„ íƒëœ ê²½ìš°: 'reaction' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "reaction"
        cluster = ICON_TO_CLUSTER.get(emotion.lower(), "common")
    else:
        # ì´ëª¨ì§€ê°€ ì—†ëŠ” ì´ˆê¸° ìƒíƒœ: 'home' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "home"
        cluster = "common"

    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        language_code=lang_code,
        personality=personality,
        cluster=cluster,
        format_kwargs={"user_nick_nm": user_name_to_use}
    )
    
    return {"dialogue": dialogue_text}
    
#  ë§ˆìŒ ê´€ë¦¬ íŒ ì™„ë£Œ í›„ í›„ì† ì§ˆë¬¸ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸  
@app.get("/dialogue/solution-followup")
async def get_solution_followup_dialogue(
    reason: str, # 'user_closed' ë˜ëŠ” 'video_ended'
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = None,
    language_code: Optional[str] = 'ko'
):
    """ë§ˆìŒ ê´€ë¦¬ íŒì´ ëë‚œ í›„ì˜ ìƒí™©(reason)ê³¼ ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” í›„ì† ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    user_name_to_use = user_nick_nm or get_translation("default_user_name", lang_code)

    # ì´ìœ (reason)ì— ë”°ë¼ DBì—ì„œ ì¡°íšŒí•  mention_typeì„ ê²°ì •í•©ë‹ˆë‹¤.
    if reason == 'user_closed':
        mention_type = "followup_user_closed"
    else: # 'video_ended' ë˜ëŠ” ê¸°íƒ€
        mention_type = "followup_video_ended"

    # get_mention_from_db í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        personality=personality,
        language_code=lang_code,
        cluster="common", 
        format_kwargs={"user_nick_nm": user_name_to_use}
    )
    
    return {"dialogue": dialogue_text}


# ë§ˆìŒ ê´€ë¦¬ íŒ ì œì•ˆì„ ê±°ì ˆí–ˆì„ ë•Œì˜ ë©˜íŠ¸ë¥¼ ì„±í–¥ë³„ë¡œ ì£¼ê¸° 
@app.get("/dialogue/decline-solution")
async def get_decline_solution_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = None,
    language_code: Optional[str] = 'ko'
):
    """ë§ˆìŒ ê´€ë¦¬ íŒ ì œì•ˆì„ ê±°ì ˆí•˜ê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³  ì‹¶ì–´í•  ë•Œì˜ ë°˜ì‘ ë©˜íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    user_name_to_use = user_nick_nm or get_translation("default_user_name", lang_code)
    
    dialogue_text = await get_mention_from_db(
        mention_type="decline_solution",
        personality=personality,
        language_code=lang_code,
        cluster="common",
        format_kwargs={"user_nick_nm": user_name_to_use}
    )
    
    return {"dialogue": dialogue_text}

# ======================================================================
# ===     ë¦¬í¬íŠ¸ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================



async def create_and_save_summary_for_user(user_id: str, date_str: str, lang_code: str = 'ko'): 
    """
    ê·¸ë‚ ì˜ 'ìµœê³ ì  ê°ì •'ê³¼ 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„ì˜ ê°ì •'ì„ ëª¨ë‘ ì°¾ì•„ LLMì— ì „ë‹¬í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìŠ¤ì¼€ì¤„ë§ëœ ì‘ì—…(/tasks/generate-summaries)ì— ì˜í•´ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    print(get_translation("log_daily_summary_start", DEFAULT_LANG, user_id=user_id, date_str=date_str))
    
    # Supabase ë˜ëŠ” OpenAI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.
    if not supabase or not OPENAI_KEY:
        print(get_translation("log_error_keys_not_set", DEFAULT_LANG))
        return

    try:
        start_of_day = f"{date_str}T00:00:00+00:00"
        end_of_day = f"{date_str}T23:59:59+00:00"

        # --- 1. 'ê·¸ë‚  ê°€ì¥ ë†’ì•˜ë˜ ë‹¨ì¼ ê°ì •' ì°¾ê¸° (ê¸°ì¤€ì  1) ---
        top_score_query = supabase.table("cluster_scores") \
            .select("cluster, score, sessions(summary)") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .order("score", desc=True) \
            .limit(1)
        top_score_res = await run_in_threadpool(top_score_query.execute)

        if not top_score_res.data:
            print(get_translation("log_daily_summary_no_scores", DEFAULT_LANG, user_id=user_id, date_str=date_str))
            return

        top_score_entry = top_score_res.data[0]
        headline_cluster = top_score_entry['cluster']
        headline_score = int(top_score_entry['score'] * 100)
        headline_summary = (top_score_entry.get('sessions') or {}).get('summary', get_translation("placeholder_no_dialogue", lang_code)) 

        # --- 2. 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„(g_score ìµœê³ ì )ì˜ ê°ì •' ì°¾ê¸° (ê¸°ì¤€ì  2) ---
        top_g_score_session_query = supabase.table("sessions") \
            .select("id, summary, g_score, cluster_scores(cluster, score)") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .order("g_score", desc=True) \
            .limit(1)
        top_g_score_res = await run_in_threadpool(top_g_score_session_query.execute)
        
        difficult_moment_context = None
        if top_g_score_res.data:
            top_g_score_session = top_g_score_res.data[0]
            if top_g_score_session.get('cluster_scores'):
                # í•´ë‹¹ ì„¸ì…˜ ë‚´ì—ì„œ ê°€ì¥ ë†’ì•˜ë˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                top_cluster_in_g_session = max(top_g_score_session['cluster_scores'], key=lambda x: x['score'])
                
                # 'ìµœê³ ì  ê°ì •'ê³¼ 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„ì˜ ê°ì •'ì´ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ì¶”ê°€ ì •ë³´ êµ¬ì„±
                if top_cluster_in_g_session['cluster'] != headline_cluster:
                    cluster_name_display = get_translation(f"cluster_{top_cluster_in_g_session['cluster']}", lang_code)
                    difficult_moment_context = {
                    "cluster_name": cluster_name_display,
                    "score": int(top_cluster_in_g_session['score'] * 100),
                    "reason": get_translation("reason_difficult_moment", lang_code)
                    }

        # --- 3. LLMì— ì „ë‹¬í•  ì •ë³´ êµ¬ì„± ---
        user_nick_nm, _ = await get_user_info(user_id, lang_code)
        llm_context = {
            "user_nick_nm": user_nick_nm,
            "headline_emotion": {
                "cluster_name": get_translation(f"cluster_{headline_cluster}", lang_code),
                "score": headline_score,
                "dialogue_summary": headline_summary
            },
            "difficult_moment": difficult_moment_context
        }
        
        recent_summaries_query = supabase.table("daily_summaries").select("summary_text").eq("user_id", user_id).order("date", desc=True).limit(5)
        recent_summaries_res = await run_in_threadpool(recent_summaries_query.execute)
        llm_context["previous_summaries"] = [s['summary_text'] for s in recent_summaries_res.data]

        # --- LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ë¬¸ ìƒì„± ---
        summary_json = await call_llm(
            system_prompt=REPORT_SUMMARY_PROMPT,
            user_content=json.dumps(llm_context, ensure_ascii=False),
            openai_key=OPENAI_KEY,
            lang_code=lang_code
        )
        
        daily_summary_text = summary_json.get("daily_summary")
        if not daily_summary_text:
            print(get_translation("log_daily_summary_llm_fail", DEFAULT_LANG, user_id=user_id, date_str=date_str))
            return


        # --- 8. ìƒì„±ëœ ìš”ì•½ë¬¸ì„ `daily_summaries` í…Œì´ë¸”ì— ì €ì¥ (Upsert) ---
        summary_data = {
            "user_id": user_id,
            "date": date_str,
            "summary_text": daily_summary_text,
            "top_cluster": headline_cluster,
            "top_score": headline_score 
        }
        await run_in_threadpool(supabase.table("daily_summaries").upsert(summary_data, on_conflict="user_id,date").execute)

        print(get_translation("log_daily_summary_success", DEFAULT_LANG, user_id=user_id, date_str=date_str))
    

    except Exception as e:
        job_name = "create_and_save_summary_for_user"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=traceback.format_exc()))
    finally:
        print(get_translation("log_daily_summary_end", DEFAULT_LANG, user_id=user_id, date_str=date_str))


# 2ì£¼ ì°¨íŠ¸ ìš”ì•½ ìƒì„± í•¨ìˆ˜
async def create_and_save_weekly_summary_for_user(user_id: str, date_str: str, lang_code: str = 'ko'):
    print(get_translation("log_weekly_summary_start", DEFAULT_LANG, user_id=user_id, date_str=date_str))
    if not supabase or not OPENAI_KEY:
        print(get_translation("log_error_keys_not_set", DEFAULT_LANG))
        return
    
    try:
        # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ìš”ì¼ í™•ì¸
        today_dt = dt.datetime.strptime(date_str, '%Y-%m-%d')
        # (ì›”ìš”ì¼=0, í™”ìš”ì¼=1, ..., ì¼ìš”ì¼=6)
        is_sunday = today_dt.weekday() == 6

        # ìš”ì¼ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if is_sunday:
            system_prompt = WEEKLY_REPORT_SUMMARY_PROMPT_NEURO
            print(get_translation("log_weekly_summary_sunday", DEFAULT_LANG))
        else:
            system_prompt = WEEKLY_REPORT_SUMMARY_PROMPT_STANDARD
            print(get_translation("log_weekly_summary_standard", DEFAULT_LANG))
            
        today = dt.datetime.strptime(date_str, '%Y-%m-%d').replace(tzinfo=dt.timezone.utc)
        start_date = today - dt.timedelta(days=13)
        end_date = today + dt.timedelta(days=1)

        # 14ì¼ê°„ì˜ ì„¸ì…˜ ë° í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ë°ì´í„° í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        sessions_res = supabase.table("sessions").select("id, created_at, g_score").eq("user_id", user_id).gte("created_at", start_date.isoformat()).lt("created_at", end_date.isoformat()).execute()
        if not sessions_res.data:
            print(get_translation("log_weekly_summary_no_session", DEFAULT_LANG, user_id=user_id))
            return # ë°ì´í„° ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
        
        # ê¸°ë¡ì´ ìˆëŠ” ë‚ ì§œ ìˆ˜ ê³„ì‚°
        recorded_days = set()
        for session in sessions_res.data:
            try:
                # íƒ€ì„ì¡´ ì •ë³´ ì œê±°í•˜ê³  ë‚ ì§œë§Œ ì¶”ì¶œ
                day_str = dt.datetime.fromisoformat(session['created_at'].split('+')[0]).strftime('%Y-%m-%d')
                recorded_days.add(day_str)
            except Exception as e:
                print(get_translation("log_weekly_summary_parse_error", DEFAULT_LANG, user_id=user_id, date_str=session['created_at'], error=str(e)))
                continue # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ì„¸ì…˜ ê±´ë„ˆë›°ê¸°

        MIN_DAYS_REQUIRED = 3 # ìµœì†Œ í•„ìš” ì¼ìˆ˜
        if len(recorded_days) < MIN_DAYS_REQUIRED:
            print(get_translation("log_weekly_summary_insufficient_data", DEFAULT_LANG, days_found=len(recorded_days), days_required=MIN_DAYS_REQUIRED, user_id=user_id))
            # ë°ì´í„° ë¶€ì¡± ì‹œ, DBì— placeholder ì €ì¥í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ ì¢…ë£Œ
            return
        # [ìˆ˜ì • ë] ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰



        
        session_ids = [s['id'] for s in sessions_res.data]
        scores_res = supabase.table("cluster_scores").select("session_id, created_at, cluster, score").in_("session_id", session_ids).execute()
        
        sessions_with_scores = []
        scores_by_session_id = {sid: [] for sid in session_ids}
        for score in scores_res.data:
            scores_by_session_id.setdefault(score['session_id'], []).append(score)

        for session in sessions_res.data:
            session['cluster_scores'] = scores_by_session_id.get(session['id'], [])
            sessions_with_scores.append(session)

        if not sessions_with_scores:
            print(get_translation("log_weekly_summary_no_session", DEFAULT_LANG, user_id=user_id))
            return


 # --- ë°ì´í„° ê°€ê³µ: íŠ¸ë Œë“œ ë¶„ì„ ë¡œì§ ì‹œì‘ ---

        # 1. ì¼ë³„ ë°ì´í„° êµ¬ì¡°í™”
        daily_data = {}
        # 14ì¼ê°„ì˜ ëª¨ë“  ë‚ ì§œ í‚¤ë¥¼ ë¯¸ë¦¬ ìƒì„±
        for i in range(14):
            day_key = (start_date + dt.timedelta(days=i)).strftime('%Y-%m-%d')
            daily_data[day_key] = {'g_scores': [], 'clusters': {c: [] for c in CLUSTERS}}
        for session in sessions_with_scores:
            created_at_str = session['created_at'].split('+')[0]
            try: day = dt.datetime.strptime(created_at_str, "%Y-%m-%dT%H:%M:%S.%f").strftime('%Y-%m-%d')
            except ValueError: day = dt.datetime.strptime(created_at_str, "%Y-%m-%dT%H:%M:%S").strftime('%Y-%m-%d')
            if day in daily_data:
                if session['g_score'] is not None: daily_data[day]['g_scores'].append(session['g_score'])
                for score_data in session.get('cluster_scores', []):
                    if score_data['cluster'] in daily_data[day]['clusters']: daily_data[day]['clusters'][score_data['cluster']].append(score_data['score'])
       
        # 2. í†µê³„ ì§€í‘œ ê³„ì‚°
        g_scores = [np.mean(day['g_scores']) for day in daily_data.values() if day['g_scores']]
        
        cluster_stats = {}
        all_scores = []
        for c in CLUSTERS:
            # í•˜ë£¨ì— ì—¬ëŸ¬ ê¸°ë¡ì´ ìˆìœ¼ë©´ í‰ê· ì„ ë‚´ì–´ ê·¸ë‚ ì˜ ëŒ€í‘œ ì ìˆ˜ë¡œ ì‚¬ìš©
            daily_avgs = [np.mean(day['clusters'][c]) for day in daily_data.values() if day['clusters'][c]]
            
            if not daily_avgs: cluster_stats[c] = {"avg": 0, "std": 0, "trend": "stable"}; continue
            all_scores.extend([(c, s) for s in daily_avgs])
           
            # ì¶”ì„¸ ë¶„ì„ (ê°„ë‹¨í•œ ê¸°ìš¸ê¸° ê³„ì‚°)
            x = np.arange(len(daily_avgs)); slope = np.polyfit(x, daily_avgs, 1)[0] if len(daily_avgs) > 1 else 0
            # í•˜ë£¨ í‰ê·  5ì ì”© ì ìˆ˜ê°€ ìƒìŠ¹/í•˜ë½ í•˜ëŠ” ì¶”ì„¸ì¼ ë•Œ í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë³€í™”ë¡œ ì¹¨
            trend = "increasing" if slope > 0.05 else "decreasing" if slope < -0.05 else "stable"

            cluster_stats[c] = {
                "avg": int(np.mean(daily_avgs) * 100), 
                "std": int(np.std(daily_avgs) * 100), 
                "trend": trend
                }

        # 3. ì£¼ìš” í´ëŸ¬ìŠ¤í„° ë° ìƒê´€ê´€ê³„ ë¶„ì„

        # ìƒê´€ê´€ê³„ ë¶„ì„ ë¡œì§ (ëª¨ë“  í´ëŸ¬ìŠ¤í„° ëŒ€ìƒ)
        correlations = []
            #  "í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ 2ì£¼ í‰ê·  ì ìˆ˜ê°€ 'ë‚®ìŒ' ìˆ˜ì¤€ì„ ë„˜ì–´, 'ì¤‘ê°„' ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ê¾¸ì¤€íˆ ë‚˜íƒ€ë‚¬ë‹¤"
        
        # [ê¸ì •ì  ìƒê´€ê´€ê³„: Aê°€ ë†’ì„ ë•Œ Bë„ ë†’ìŒ]
        if cluster_stats['sleep']['avg'] > 40 and cluster_stats['neg_low']['avg'] > 40:
            correlations.append(get_translation("corr_sleep_neglow", lang_code))
        if cluster_stats['neg_high']['avg'] > 40 and cluster_stats['sleep']['avg'] > 40:
            correlations.append(get_translation("corr_neghigh_sleep", lang_code))
        if cluster_stats['adhd']['avg'] > 50 and cluster_stats['neg_high']['avg'] > 50:
            correlations.append(get_translation("corr_adhd_neghigh", lang_code))

        # [ë¶€ì •ì /ë°˜ë¹„ë¡€ ìƒê´€ê´€ê³„: Aê°€ ë†’ì„ ë•Œ BëŠ” ë‚®ìŒ]
        if cluster_stats['neg_low']['avg'] > 50 and cluster_stats['positive']['avg'] < 30:
            correlations.append(get_translation("corr_neglow_positive", lang_code))
        if cluster_stats['neg_high']['avg'] > 50 and cluster_stats['positive']['avg'] < 30:
            correlations.append(get_translation("corr_neghigh_positive", lang_code))

        # [ì¶”ì„¸ ê¸°ë°˜ ë°˜ë¹„ë¡€ ìƒê´€ê´€ê³„: Aê°€ ê°œì„ ë  ë•Œ Bë„ ê°œì„ ë¨]
        if cluster_stats['sleep']['trend'] == 'decreasing' and cluster_stats['neg_low']['trend'] == 'decreasing':
            correlations.append(get_translation("corr_trend_sleep_neglow", lang_code))
        if cluster_stats['neg_low']['trend'] == 'decreasing' and cluster_stats['positive']['trend'] == 'increasing':
            correlations.append(get_translation("corr_trend_neglow_positive", lang_code))

        # 4. ì£¼ìš” í´ëŸ¬ìŠ¤í„° ì‹ë³„
        # ì§€ë‚œ 2ì£¼ê°„ ë°œìƒí•œ ëª¨ë“  ê°ì • ê¸°ë¡ ì¤‘ì—ì„œ, ì ìˆ˜ê°€ ê°€ì¥ ë†’ì•˜ë˜ ìˆœê°„ Top 2ë¥¼ ì°¾ì•„ë‚´ë¼
        dominant_clusters_keys = list(set([item[0] for item in sorted(all_scores, key=lambda item: item[1], reverse=True)[:2]]))        
        # í´ëŸ¬ìŠ¤í„° ì´ë¦„ ë³€í™˜
        dominant_clusters_display = [get_translation(f"cluster_{c}", lang_code) for c in dominant_clusters_keys]
        # ìµœì¢… LLM ì „ë‹¬ ë°ì´í„° êµ¬ì¡°
        trend_data = {
            "g_score_stats": {"avg": int(np.mean(g_scores)*100) if g_scores else 0, 
                              "std": int(np.std(g_scores)*100) if g_scores else 0}, 
            "cluster_stats": cluster_stats, 
            "dominant_clusters": dominant_clusters_display,
            "correlations": correlations
            }

        # 5. LLM í˜¸ì¶œ ë° ê²°ê³¼ ì €ì¥
        # ë¶„ì„í•œ íŠ¸ë Œë“œ llmì— ë„£ê¸°
        user_nick_nm, _ = await get_user_info(user_id, lang_code)
        llm_context = { "user_nick_nm": user_nick_nm, "trend_data": trend_data }
        summary_json = await call_llm(system_prompt, json.dumps(llm_context, ensure_ascii=False), OPENAI_KEY, lang_code=lang_code)

        if not summary_json or "error" in summary_json:
            print(get_translation("log_weekly_summary_llm_fail", DEFAULT_LANG, user_id=user_id))
            return
            
        summary_data = { "user_id": user_id, "summary_date": date_str, **summary_json }
        await run_in_threadpool(supabase.table("weekly_summaries").upsert(summary_data, on_conflict="user_id,summary_date").execute)
        print(get_translation("log_weekly_summary_success", DEFAULT_LANG, user_id=user_id, date_str=date_str))
    except Exception as e:
        job_name = "create_and_save_weekly_summary_for_user"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=traceback.format_exc()))



#  ------- daily_summaries í…Œì´ë¸”ì—ì„œ ìš”ì•½ë¬¸ ê°„ë‹¨íˆ ì¡°íšŒ ---------
# ëª¨ì§€ ë‹¬ë ¥ì—ì„œ íŠ¹ì • ë‚ ì§œë¥¼ íƒ­í–ˆì„ ë•Œ, í•´ë‹¹ ë‚ ì§œì˜ 'ì¼ì¼ ìš”ì•½ë¬¸' í•˜ë‚˜ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ëŠ” ì—­í• 
@app.post("/report/summary")
async def get_daily_report_summary(request: DailyReportRequest):
    """ë¯¸ë¦¬ ìƒì„±ëœ ì¼ì¼ ìš”ì•½ë¬¸ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    lang_code = request.language_code if request.language_code in translations else DEFAULT_LANG

    if not supabase:
        raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))

    try:
        query = supabase.table("daily_summaries").select("summary_text").eq("user_id", request.user_id).eq("date", request.date).limit(1)
        response = await run_in_threadpool(query.execute)

        if response.data:
            summary = response.data[0].get("summary_text", get_translation("summary_not_found", lang_code))
            return {"summary": summary}
        else:
            return {"summary": get_translation("placeholder_summary_no_data", lang_code)}

    except Exception as e:
        job_name = "/report/summary"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=traceback.format_exc()))
        raise HTTPException(status_code=500, detail=get_translation("error_loading_summary", lang_code))
    

# --- 2ì£¼ ì°¨íŠ¸ ìš”ì•½ë¬¸ì„ í”„ë¡ íŠ¸ì—”ë“œì— ì œê³µí•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ ---
# ëª¨ì§€ ì°¨íŠ¸ í˜ì´ì§€ì— ë“¤ì–´ê°”ì„ ë•Œ, '2ì£¼ ë¶„ì„ ë¦¬í¬íŠ¸' ì „ì²´(ì¢…í•©, í´ëŸ¬ìŠ¤í„°ë³„)ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì—­í• 


@app.post("/report/weekly-summary")
async def get_weekly_report_summary(request: WeeklyReportRequest):
    lang_code = request.language_code if request.language_code in translations else DEFAULT_LANG
    if not supabase: raise HTTPException(500, get_translation("error_supabase_init", lang_code))

    # ê¸°ë³¸ í”Œë ˆì´ìŠ¤í™€ë” ë©”ì‹œì§€ ì •ì˜
    placeholder_no_data = get_translation("placeholder_weekly_summary_no_data", lang_code)
    placeholder_error = get_translation("placeholder_weekly_summary_error", lang_code)

    try:
        # Supabase ì¿¼ë¦¬ ê°ì²´ ìƒì„±
        query = (
            supabase.table("weekly_summaries")
            .select("*")
            .eq("user_id", request.user_id)
            .order("summary_date", desc=True)
            .limit(1)
            .maybe_single()
        )

        # query ê°ì²´ì˜ execute ë©”ì„œë“œ ìì²´ë¥¼ ì „ë‹¬ (ê´„í˜¸ ì—†ìŒ!)
        response = await run_in_threadpool(query.execute)

        # response.dataê°€ Noneì´ ì•„ë‹ˆê³ , ë‚´ìš©ì´ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
        if response and response.data and response.data.get("overall_summary"):
            print(f"âœ… Found weekly summary for user {request.user_id}")
            return response.data
        else:
            print(f"âš ï¸ No weekly summary data found for user {request.user_id}. Returning placeholder.")
            return {
                "overall_summary": placeholder_no_data,
                "neg_low_summary": placeholder_no_data,
                "neg_high_summary": placeholder_no_data,
                "adhd_summary": placeholder_no_data,
                "sleep_summary": placeholder_no_data,
                "positive_summary": placeholder_no_data
            }
    except Exception as e:
        job_name = "/report/weekly-summary"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=traceback.format_exc()))
        return {
            "overall_summary": placeholder_error,
            "neg_low_summary": placeholder_error,
            "neg_high_summary": placeholder_error,
            "adhd_summary": placeholder_error,
            "sleep_summary": placeholder_error,
            "positive_summary": placeholder_error
        }
    
# ======================================================================
# ===     ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ë§ ì‘ì—…ìš© ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/tasks/generate-summaries")
async def handle_generate_summaries_task(language_code: Optional[str] = 'ko'): 
    """
    Supabase Cron Jobì— ì˜í•´ í˜¸ì¶œë  ì—”ë“œí¬ì¸íŠ¸.
    ì–´ì œ í™œë™í•œ ëª¨ë“  ì‚¬ìš©ìì˜ ì¼ì¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    
    # ì–´ì œ ë‚ ì§œ ê³„ì‚° (UTC ê¸°ì¤€)
    yesterday = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=1)
    yesterday_str = yesterday.strftime('%Y-%m-%d')
    
    start_of_yesterday = f"{yesterday_str}T00:00:00+00:00"
    end_of_yesterday = f"{yesterday_str}T23:59:59+00:00"

    print(get_translation("log_task_start", DEFAULT_LANG, job_name="Daily/Weekly Summaries", date_str=yesterday_str))

    # ì–´ì œ í™œë™í•œ ìœ ì € ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
    active_users_query = supabase.table("sessions").select("user_id", count='exact') \
        .gte("created_at", start_of_yesterday) \
        .lte("created_at", end_of_yesterday)
    
    active_users_res = await run_in_threadpool(active_users_query.execute)
    # ì–´ì œ ì•±ì„ ì‚¬ìš©í•œ ìœ ì €ê°€ ë‹¨ í•œ ëª…ë„ ì—†ë‹¤ë©´, ì¦‰ì‹œ "ì–´ì œ í™œë™í•œ ìœ ì € ì—†ìŒ" ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì‘ì—…ì„ ì¢…ë£Œ
    if not active_users_res.data:
        message = get_translation("log_task_no_active_users", DEFAULT_LANG)
        print(message)
        return {"message": message}

    # í™œë™ ìœ ì €ê°€ ìˆì„ ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰
    user_ids = list(set([item['user_id'] for item in active_users_res.data]))
    
    print(get_translation("log_task_found_users", DEFAULT_LANG, user_count=len(user_ids)))

    # ê° ìœ ì €ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì•½ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    profiles_res = await run_in_threadpool(supabase.table("user_profiles").select("id, language_code").in_("id", user_ids).execute)
    user_lang_map = {p['id']: p.get('language_code', DEFAULT_LANG) for p in profiles_res.data}

    for user_id in user_ids:
        user_lang = user_lang_map.get(user_id, DEFAULT_LANG) # ğŸ¥‘ í•´ë‹¹ ìœ ì €ì˜ ì–¸ì–´ ì„¤ì • ì‚¬ìš©
        await create_and_save_summary_for_user(user_id, yesterday_str, user_lang)
        await create_and_save_weekly_summary_for_user(user_id, yesterday_str, user_lang)

    message = get_translation("log_task_complete", DEFAULT_LANG, user_count=len(user_ids))
    print(message)
    return {"message": message}


# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    """ì£¼ê¸°ì  ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ DBì— ì €ì¥í•˜ê³ , ì‚¬ìš©ìì˜ ìµœì‹  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG

    if not supabase:
        raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))

    try:
        # 1. ì œì¶œëœ ë‹µë³€ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° ë° ì •ê·œí™”
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score:
            raise HTTPException(status_code=400, detail=get_translation("error_invalid_cluster", lang_code, cluster=payload.cluster))
        
        normalized_score = clip01(total_score / max_score)

        # 2. user_profiles í…Œì´ë¸”ì—ì„œ í˜„ì¬ ìµœì‹  ì ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
        profile_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("latest_assessment_scores")
            .eq("id", payload.user_id).single().execute
        )
        
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}


        # 3. ì´ë²ˆì— í‰ê°€í•œ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ë§Œ ì—…ë°ì´íŠ¸
        latest_scores[payload.cluster] = normalized_score
        
        # 4. ì—…ë°ì´íŠ¸ëœ ì ìˆ˜ë¥¼ ë‹¤ì‹œ user_profiles í…Œì´ë¸”ì— ì €ì¥
        update_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .update({"latest_assessment_scores": latest_scores})
            .eq("id", payload.user_id).execute
        )

        # assessment_history í…Œì´ë¸”ì— ì›ë³¸ ê¸°ë¡ ì €ì¥ (ì¶”í›„ ìƒì„¸ ë¶„ì„ìš©)
        history_row = {
            "user_id": payload.user_id,
            "assessment_type": f"deep_dive_{payload.cluster}",
            "scores": {payload.cluster: normalized_score},
            "raw_responses": payload.responses,
        }
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)

        return {"message": get_translation("assessment_success", lang_code), "updated_scores": latest_scores}

    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/assessment/submit"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        raise HTTPException(status_code=500, detail={"error": get_translation("error_occurred", lang_code, error=str(e)), "trace": tb if os.getenv("DEBUG") else None})    


# ======================================================================
# ===     ìˆ˜ë©´ìœ„ìƒ íŒ ì œê³µ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.get("/dialogue/sleep-tip")
async def get_sleep_tip(
    personality: Optional[str] = None,
    user_nick_nm: Optional[str] = None,
    language_code: Optional[str] = 'ko'
):
    # get_mention_from_db ëŒ€ì‹  ì§ì ‘ ì¿¼ë¦¬ (ë³„ë„ í…Œì´ë¸”ì´ë¯€ë¡œ)
    lang_code = language_code if language_code in translations else DEFAULT_LANG
    user_name_to_use = user_nick_nm or get_translation("default_user_name", lang_code)
    default_tip = get_translation("default_sleep_tip", lang_code)

    """ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” ìˆ˜ë©´ìœ„ìƒ íŒì„ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not supabase:
        return {"tip": default_tip}
    try:
        query = supabase.table("sleep_hygiene_tips").select("text").eq("language_code", lang_code)
        if personality:
            query = query.eq("personality", personality)
        
        # SQLì˜ ORDER BY random() LIMIT 1ê³¼ ìœ ì‚¬í•œ íš¨ê³¼
        response = await run_in_threadpool(query.execute)
        tips = [row['text'] for row in response.data]
        
        if not tips:
            # í•´ë‹¹ ì„±ê²©ì˜ íŒì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒ ë°˜í™˜
            fallback_res = await run_in_threadpool(supabase.table("sleep_hygiene_tips").select("text").eq("language_code", lang_code).eq("personality", "prob_solver").execute)
            tips = [row['text'] for row in fallback_res.data]

        selected_tip = random.choice(tips) if tips else default_tip

        try:
            return {"tip": selected_tip.format(user_nick_nm=user_name_to_use)}
        except KeyError:
            return {"tip": selected_tip.replace("{user_nick_nm}", user_name_to_use)}
        
    except Exception as e:
        print(get_translation("log_error_get_sleep_tip", DEFAULT_LANG, error=str(e)))
        return {"tip": default_tip}


# ======================================================================
# ===     í–‰ë™ í™œì„±í™” ë¯¸ì…˜ ì œê³µ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.get("/dialogue/action-mission")
async def get_action_mission(
    personality: Optional[str] = None,
    user_nick_nm: Optional[str] = None,
    language_code: Optional[str] = 'ko'
):
    """ìš°ìš¸(neg_low) í´ëŸ¬ìŠ¤í„°ë¥¼ ìœ„í•œ í–‰ë™ ë¯¸ì…˜ì„ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""

    lang_code = language_code if language_code in translations else DEFAULT_LANG
    user_name_to_use = user_nick_nm or get_translation("default_user_name", lang_code)
    default_mission = get_translation("default_action_mission", lang_code)

    if not supabase:
        return {"mission": default_mission}
    try:
        query = supabase.table("action_solutions").select("text").eq("language_code", lang_code)
        if personality:
            query = query.eq("personality", personality)
        
        response = await run_in_threadpool(query.execute)
        missions = [row['text'] for row in response.data]
        
        if not missions:
            fallback_res = await run_in_threadpool(supabase.table("action_solutions").select("text").eq("language_code", lang_code).eq("personality", "prob_solver").execute)
            missions = [row['text'] for row in fallback_res.data]

        selected_mission = random.choice(missions) if missions else default_mission
        
        try:
            return {"mission": selected_mission.format(user_nick_nm=user_name_to_use)}
        except KeyError:
            return {"mission": selected_mission.replace("{user_nick_nm}", user_name_to_use)}

    except Exception as e:
        print(get_translation("log_error_get_action_mission", DEFAULT_LANG, error=str(e)))
        return {"mission": default_mission}


# ======================================================================
# ===     í”¼ë“œë°± ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/solutions/feedback")
async def handle_solution_feedback(payload: FeedbackRequest):
    """
    ë§ˆìŒ ê´€ë¦¬ íŒì— ëŒ€í•œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ ,
    'not_helpful'ì¸ ê²½ìš° negative_tagsë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG
    if not supabase:
        raise HTTPException(status_code=500, detail=get_translation("error_supabase_init", lang_code))

    try:
        # 1. ë¨¼ì € solution_feedback í…Œì´ë¸”ì— í”¼ë“œë°± ê¸°ë¡ì„ ì‚½ì…í•©ë‹ˆë‹¤.
        feedback_insert_query = supabase.table("solution_feedback").insert({
            "user_id": payload.user_id,
            "solution_id": payload.solution_id,
            "session_id": payload.session_id,
            "solution_type": payload.solution_type,
            "feedback": payload.feedback
        })
        await run_in_threadpool(feedback_insert_query.execute)

        # 2. ë§Œì•½ í”¼ë“œë°±ì´ 'not_helpful'ì´ë¼ë©´, íƒœê·¸ ì—…ë°ì´íŠ¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        if payload.feedback == 'not_helpful':
            # 2-1. ì‹«ì–´ìš” ëˆ„ë¥¸ ë§ˆìŒ ê´€ë¦¬ íŒì˜ íƒœê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            solution_query = supabase.table("solutions").select("tags").eq("solution_id", payload.solution_id).single()
            solution_res = await run_in_threadpool(solution_query.execute)
            
            if solution_res.data and solution_res.data.get("tags"):
                solution_tags = solution_res.data["tags"]

                # 2-2. ì‚¬ìš©ìì˜ í˜„ì¬ negative_tagsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                profile_query = supabase.table("user_profiles").select("negative_tags").eq("id", payload.user_id).single()
                profile_res = await run_in_threadpool(profile_query.execute)
                
                current_tags = []
                if profile_res.data and profile_res.data.get("negative_tags"):
                    current_tags = profile_res.data["negative_tags"]

                # 2-3. ê¸°ì¡´ íƒœê·¸ì™€ ìƒˆë¡œìš´ íƒœê·¸ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
                updated_tags = list(set(current_tags) | set(solution_tags))
                
                # 2-4. user_profiles í…Œì´ë¸”ì— ì—…ë°ì´íŠ¸ëœ íƒœê·¸ ëª©ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.
                update_query = supabase.table("user_profiles").update({"negative_tags": updated_tags}).eq("id", payload.user_id)
                await run_in_threadpool(update_query.execute)
                
                print(get_translation("log_negative_tags_updated", DEFAULT_LANG, user_id=payload.user_id, tags=updated_tags))

        return {"message": get_translation("feedback_success", lang_code)}

    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/solutions/feedback"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        raise HTTPException(status_code=500, detail={"error": get_translation("error_occurred", lang_code, error=str(e)), "trace": tb if os.getenv("DEBUG") else None})
    

# ======================================================================
# === ë°±í•„ ìˆ˜ë™ ì‹¤í–‰ ===
# ======================================================================
@app.post("/jobs/backfill")
async def run_backfill(payload: BackfillRequest):
    lang_code = payload.language_code if payload.language_code in translations else DEFAULT_LANG
    """
    ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— ëŒ€í•´ ëª¨ë“  ì‚¬ìš©ì ë˜ëŠ” íŠ¹ì • ì‚¬ìš©ìì˜ ì¼ì¼/ì£¼ê°„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        payload: BackfillRequest
            - start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
            - end_date: ë ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
            - user_id: íŠ¹ì • ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­, Noneì´ë©´ ëª¨ë“  ì‚¬ìš©ì ì²˜ë¦¬)
    
    Returns:
        dict: ë°±í•„ ì‘ì—… ê²°ê³¼
    """
    try:
        from backfill_summaries import run_backfill as backfill_function
        print(get_translation("log_backfill_request", lang_code, endpoint="/jobs/backfill"))
        print(get_translation("log_backfill_range", lang_code, start_date=payload.start_date, end_date=payload.end_date))
        print(get_translation("log_backfill_check_logs", lang_code, main_py="main.py"))
        print(get_translation("log_backfill_wait", lang_code))
        
        # ë°±í•„ í•¨ìˆ˜ì—ë„ lang_code ì „ë‹¬ (ë§Œì•½ ì§€ì›í•œë‹¤ë©´)
        result = await backfill_function(payload.start_date, payload.end_date, payload.user_id, lang_code)
        
        print(get_translation("backfill_complete", lang_code))
        return result
    except ImportError:
        raise HTTPException(status_code=404, detail="Backfill script not found.")
    except Exception as e:
        tb = traceback.format_exc()
        job_name = "/jobs/backfill"
        print(get_translation("log_error_unhandled_exception", DEFAULT_LANG, job_name=job_name, error=str(e), trace=tb))
        raise HTTPException(status_code=500, detail={"error": get_translation("error_occurred", lang_code, error=str(e)), "trace": tb if os.getenv("DEBUG") else None})
    

