# main.py
from __future__ import annotations

import datetime as dt
import json
import os
import re
import traceback
import random
from typing import Optional, List, Dict, Any, Tuple
import uuid

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from supabase import create_client, Client

import uuid
from ai_moderator import moderate_text
from llm_prompts import REPORT_SUMMARY_PROMPT, call_llm, get_system_prompt, TRIAGE_SYSTEM_PROMPT, FRIENDLY_SYSTEM_PROMPT 
from rule_based import rule_scoring
from srj5_constants import (
    CLUSTERS, EMOJI_ONLY_SCORE_CAP, ICON_TO_CLUSTER, ONBOARDING_MAPPING,
    FINAL_FUSION_WEIGHTS, FINAL_FUSION_WEIGHTS_NO_TEXT,
    W_LLM, W_RULE, 
    SAFETY_LEMMAS, SAFETY_LEMMA_COMBOS, SAFETY_REGEX, SAFETY_FIGURATIVE
)


try:
    from kiwipiepy import Kiwi
    _kiwi = Kiwi()
except ImportError:
    _kiwi = None
    print("âš ï¸ kiwipiepy is not installed. Some safety features will be disabled.")


# --- í™˜ê²½ì„¤ì • ---
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
# BIND_HOST = os.getenv("BIND_HOST", "0.0.0.0")
# PORT = int(os.getenv("PORT", "8000"))
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
# Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
supabase: Optional[Client] = None


# --- FastAPI ì•± ì´ˆê¸°í™” ë° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
app = FastAPI(title="DailyMoji API v2 (Separated Logic)")

# supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@app.on_event("startup")
def startup_event():
    global supabase
    if SUPABASE_URL and SUPABASE_KEY:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… FastAPI server started and Supabase client initialized.")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- ë°ì´í„° ëª¨ë¸ (ë¶„ë¦¬ëœ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •) ---

# /analyze ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AnalyzeRequest(BaseModel):
    user_id: str
    text: str
    icon: Optional[str] = None
    language_code: Optional[str] = 'ko'
    timestamp: Optional[str] = None
    onboarding: Optional[Dict[str, Any]] = None
    character_personality: Optional[str] = None


# /solutions/propose ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class SolutionRequest(BaseModel):
    user_id: str
    session_id: str
    top_cluster: str
    language_code: str = 'ko'


# Flutterì˜ PresetIdsì™€ ë™ì¼í•œ êµ¬ì¡°
class PresetIds:
    FRIENDLY_REPLY = "FRIENDLY_REPLY"
    SOLUTION_PROPOSAL = "SOLUTION_PROPOSAL"
    SAFETY_CRISIS_MODAL = "SAFETY_CRISIS_MODAL"
    EMOJI_REACTION = "EMOJI_REACTION"

# ======================================================================
# === kiwië¥¼ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ ì¥ì¹˜ ===
# ======================================================================

# --- ì•ˆì „ ì¥ì¹˜ ë¡œì§ ---

def _find_regex_matches(text: str, patterns: List[str]) -> List[str]:
    return [m.group(0) for pat in patterns for m in re.finditer(pat, text, flags=re.IGNORECASE)]

def _kiwi_detect_safety_lemmas(text: str) -> List[str]:
    if not _kiwi: return []
    try:
        tokens = _kiwi.tokenize(text)
        all_lemmas_in_text = {t.lemma for t in tokens}
        hits = {token.lemma for token in tokens if token.lemma in SAFETY_LEMMAS}
        for combo in SAFETY_LEMMA_COMBOS:
            if combo.issubset(all_lemmas_in_text):
                hits.update(combo)
        return list(hits)
    except Exception as e:
        print(f"Kiwi safety check error: {e}")
        return []

def is_safety_text(text: str, llm_json:  Optional[dict], debug_log: dict) -> Tuple[bool, dict]:
    """
    ì‚¬ìš©ì í…ìŠ¤íŠ¸ì˜ ìí•´/ìì‚´ ìœ„í—˜ì„ ë‹¤ë‹¨ê³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    - 1ë‹¨ê³„: ëª…ë°±íˆ ì•ˆì „í•œ ë¹„ìœ ì  í‘œí˜„ì„ ë¨¼ì € ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
    - 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„ê³¼ LLMì˜ ì˜ë„ ë¶„ì„ìœ¼ë¡œ ìœ„í—˜ ì‹ í˜¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    """

    # 1ë‹¨ê³„: ë¹„ìœ ì /ê´€ìš©ì  í‘œí˜„ ìš°ì„  í•„í„°ë§ ("ì¡¸ë ¤ ì£½ê² ë‹¤" ë“±)
    # SAFETY_FIGURATIVEì— ë§¤ì¹˜ë˜ë©´, ìœ„í—˜í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ ì¢…ë£Œ
    figurative_matches = [m.group(0) for pat in SAFETY_FIGURATIVE for m in re.finditer(pat, text, flags=re.IGNORECASE)]
    if figurative_matches:
        debug_log["safety"] = {
            "triggered": False,
            "reason": "Figurative speech detected",
            "matches": figurative_matches
        }
        return False, {}

    # 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„ ë° LLM ì˜ë„ ë¶„ì„
    kiwi_lemma_hits = []
    if _kiwi:
        try:
            tokens = _kiwi.tokenize(text)
            lemmas_in_text = {t.lemma for t in tokens}
            hits = {t.lemma for t in tokens if t.lemma in SAFETY_LEMMAS}
            for combo in SAFETY_LEMMA_COMBOS:
                if combo.issubset(lemmas_in_text):
                    hits.update(combo)
            kiwi_lemma_hits = list(hits)
        except Exception as e:
            print(f"Kiwi safety check error: {e}")

    safety_llm_flag = (llm_json or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}
    
    # Kiwi ë˜ëŠ” LLM ì¤‘ í•˜ë‚˜ë¼ë„ ìœ„í—˜ ì‹ í˜¸ë¥¼ ê°ì§€í•˜ë©´ ì•ˆì „ì¥ì¹˜ ë°œë™
    triggered = bool(kiwi_lemma_hits) or safety_llm_flag

    debug_log["safety"] = {
        "kiwi_lemma_hits": kiwi_lemma_hits,
        "llm_intent_flag": safety_llm_flag,
        "triggered": triggered
    }

    if triggered:
        # ìœ„ê¸° ìƒí™©ì— ë§ëŠ” ê·¹ë‹¨ì ì¸ ì ìˆ˜ ë¶€ì—¬
        crisis_scores = {"neg_low": 0.95, "neg_high": 0.0, "adhd": 0.0, "sleep": 0.0, "positive": 0.0}
        return True, crisis_scores

    return False, {}



# ======================================================================
# === í•µì‹¬ ë¡œì§: ìŠ¤ì½”ì–´ë§ ë° ìœµí•© (Scoring & Fusion) ===
# ======================================================================
def calculate_final_scores(
    text_scores: dict,
    onboarding_scores: dict,
    icon_scores: dict,
    has_icon: bool
) -> Tuple[dict, dict]:
    """í…ìŠ¤íŠ¸, ì˜¨ë³´ë”©, ì•„ì´ì½˜ ì ìˆ˜ë¥¼ ìµœì¢… ê°€ì¤‘ì¹˜ì— ë”°ë¼ ìœµí•©í•©ë‹ˆë‹¤."""
    w = FINAL_FUSION_WEIGHTS
    
    if not has_icon:
        # CASE 1: í…ìŠ¤íŠ¸ë§Œ ì…ë ¥ ì‹œ -> ì•„ì´ì½˜ ê°€ì¤‘ì¹˜ë¥¼ í…ìŠ¤íŠ¸ì™€ ì˜¨ë³´ë”©ì— ë¹„ë¡€ ë°°ë¶„
        w_text = w['text'] + w['icon'] * (w['text'] / (w['text'] + w['onboarding']))
        w_onboarding = w['onboarding'] + w['icon'] * (w['onboarding'] / (w['text'] + w['onboarding']))
        w_icon = 0.0
    else:
        # CASE 3: í…ìŠ¤íŠ¸ + ì•„ì´ì½˜ ì…ë ¥ ì‹œ -> ëª¨ë“  ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        w_text, w_onboarding, w_icon = w['text'], w['onboarding'], w['icon']
    
    weights_used = {"text": w_text, "onboarding": w_onboarding, "icon": w_icon}

    final_scores = {c: clip01(
        text_scores.get(c, 0.0) * w_text +
        onboarding_scores.get(c, 0.0) * w_onboarding +
        icon_scores.get(c, 0.0) * w_icon
    ) for c in CLUSTERS}
    
    return final_scores, weights_used

def calculate_text_scores(text: str, llm_json: Optional[dict]) -> dict:
    """Rule-based ì ìˆ˜ì™€ LLM ì ìˆ˜ë¥¼ ìœµí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    rule_scores, _, _ = rule_scoring(text)
    
    text_if = {c: 0.0 for c in CLUSTERS}
    if llm_json and not llm_json.get("error"):
        I, F = llm_json.get("intensity", {}), llm_json.get("frequency", {})
        for c in CLUSTERS:
            In = clip01((I.get(c, 0.0) or 0.0) / 3.0)
            Fn = clip01((F.get(c, 0.0) or 0.0) / 3.0)
            text_if[c] = clip01(0.6 * In + 0.4 * Fn + 0.1 * rule_scores.get(c, 0.0))
    
    fused_scores = {c: clip01(W_RULE * rule_scores.get(c, 0.0) + W_LLM * text_if.get(c, 0.0)) for c in CLUSTERS}
    return fused_scores

# ======================================================================
# === í—¬í¼ í•¨ìˆ˜ (Helpers) ===
# ======================================================================

def _format_scores_for_print(scores: dict) -> str:
    """ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜"""
    if not isinstance(scores, dict):
        return str(scores)
    return json.dumps({k: round(v, 2) if isinstance(v, float) else v for k, v in scores.items()})

def clip01(x: float) -> float: return max(0.0, min(1.0, float(x)))

def g_score(final_scores: dict) -> float:
    w = {"neg_high": 1.0, "neg_low": 0.9, "sleep": 0.7, "adhd": 0.6, "positive": -0.3}
    g = sum(final_scores.get(k, 0.0) * w.get(k, 0.0) for k in CLUSTERS)
    return round(clip01((g + 1.0) / 2.0), 3)

def pick_profile(final_scores: dict, llm: Optional[dict]) -> int: # surveys íŒŒë¼ë¯¸í„° ì œê±°
    if (llm or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}: return 1
    if max(final_scores.get("neg_low", 0), final_scores.get("neg_high", 0)) > 0.85: return 1
    # if (surveys and (surveys.get("phq9", 0) >= 10 or surveys.get("gad7", 0) >= 10)) or max(final_scores.values()) > 0.60: return 2
    if max(final_scores.values()) > 0.60: return 2 # surveys í•„ë“œ ì œê±°
    if max(final_scores.values()) > 0.30: return 3
    return 0

def calculate_baseline_scores(onboarding_scores: Optional[Dict[str, int]]) -> Dict[str, float]:
    if not onboarding_scores: return {c: 0.0 for c in CLUSTERS}
    baseline = {c: 0.0 for c in CLUSTERS}
    for q_key, score in onboarding_scores.items():
        processed_score = 3 - score if q_key == 'q7' else score
        if q_key in ONBOARDING_MAPPING:
            normalized_score = processed_score / 3.0
            for mapping in ONBOARDING_MAPPING[q_key]:
                baseline[mapping["cluster"]] += normalized_score * mapping["w"]
    for c in CLUSTERS:
        baseline[c] = clip01(baseline.get(c, 0.0))
    return baseline

# ======================================================================
# === DB ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ ===
# ======================================================================

async def get_user_info(user_id: str) -> Tuple[str, str]:
    """ì‚¬ìš©ì ë‹‰ë„¤ì„ê³¼ ìºë¦­í„° ì´ë¦„ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not supabase: return "ì‚¬ìš©ì", "ëª¨ì§€"
    try:
        res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("user_nick_nm, character_nm")
            .eq("id", user_id).single().execute
        )
        if res.data:
            return res.data.get("user_nick_nm", "ì‚¬ìš©ì"), res.data.get("character_nm", "ëª¨ì§€")
    except Exception:
        pass
    return "ì‚¬ìš©ì", "ëª¨ì§€"


# ëª¨ë“  ë©˜íŠ¸ ì¡°íšŒë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def get_mention_from_db(mention_type: str, language_code: str, **kwargs) -> str:
    """DBì—ì„œ ì§€ì •ëœ íƒ€ì…ê³¼ ì¡°ê±´ì— ë§ëŠ” ìºë¦­í„° ë©˜íŠ¸ë¥¼ ëœë¤ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"--- âœ… get_mention_from_db í˜¸ì¶œë¨ (mention_type: {mention_type}) âœ… ---")

    default_messages = {
        "analysis": "ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
        "reaction": "ì–´ë–¤ ì¼ ë•Œë¬¸ì— ê·¸ë ‡ê²Œ ëŠë¼ì…¨ë‚˜ìš”?",
        "propose": "ì´ëŸ° í™œë™ì€ ì–´ë– ì„¸ìš”?",
        "home": "ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        "followup_user_closed": "ê´œì°®ì•„ìš”. ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°ˆê¹Œìš”?",
        "followup_video_ended": "ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        "decline_solution": "ì•Œê² ìŠµë‹ˆë‹¤. í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”."
    }
    default_message = default_messages.get(mention_type, "...")

    # .format()ì— ì‚¬ìš©ë  ì¸ìë“¤ì„ ë¯¸ë¦¬ ì¤€ë¹„í•©ë‹ˆë‹¤.
    format_args = kwargs.get("format_kwargs", kwargs)

    def _safe_format(text: str) -> str:
        """KeyError ì—†ì´ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            # format_argsê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ dictìœ¼ë¡œ ì²˜ë¦¬
            return text.format(**(format_args or {}))
        except KeyError:
            # í¬ë§·íŒ…ì— ì‹¤íŒ¨í•˜ë©´ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            return text.replace("{user_nick_nm}", "ì¹œêµ¬")

    if not supabase:
        return _safe_format(default_message)


    # if not supabase: return default_message.format(**kwargs) if kwargs else default_message
    try:
        query = supabase.table("character_mentions").select("text").eq("mention_type", mention_type).eq("language_code", language_code)
        
        valid_filter_keys = ["personality", "cluster", "level", "solution_variant"]
        for key, value in kwargs.items():
            if key in valid_filter_keys and value:
                query = query.eq(key, value)

        response = await run_in_threadpool(query.execute)
        scripts = [row['text'] for row in response.data]
        
        if not scripts:
            return _safe_format(default_message)
        
        selected_script = random.choice(scripts)
        # DBì—ì„œ ê°€ì ¸ì˜¨ ë©˜íŠ¸ë„ ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…
        return _safe_format(selected_script)

    except Exception as e:
        print(f"âŒ get_mention_from_db Error: {e}")
        return _safe_format(default_message)


# # ìˆ˜ì¹˜ë¥¼ ì£¼ê¸°ë³´ë‹¤ëŠ”, ì‹¬ê°ë„ 3ë‹¨ê³„ì— ë”°ë¼ ë©”ì‹œì§€ í•´ì„í•´ì£¼ëŠ”ê²Œ ë‹¬ë¼ì§(ìˆ˜ì¹˜í˜•x, ëŒ€í™”í˜•o)
# async def get_analysis_message(
#     scores: dict, 
#     personality: Optional[str], 
#     language_code: str
# ) -> str:
#     if not scores: return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ë” ë“¤ì—¬ë‹¤ë³´ê³  ìˆì–´ìš”."
#     top_cluster = max(scores, key=scores.get)
#     score_val = scores[top_cluster]
    
#     level = "low"
#     if score_val > 0.7: level = "high"
#     elif score_val > 0.4: level = "mid"
    
#     return await get_mention_from_db(
#         mention_type="analysis",
#         personality=personality,
#         language_code=language_code,
#         cluster=top_cluster,
#         level=level,
#         default_message="ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
#         format_kwargs={"emotion": top_cluster, "score": int(score_val * 100)}
#     )
    


async def save_analysis_to_supabase(
    payload: AnalyzeRequest, profile: int, g: float,
    intervention: dict, debug_log: dict, final_scores: dict
) -> Optional[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not supabase: return None
    try:
        user_id = payload.user_id

        # ì„¸ì…˜ì„ ì €ì¥í•˜ê¸° ì „, user_profilesì— í•´ë‹¹ ìœ ì €ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        profile_query = supabase.table("user_profiles").select("id").eq("id", user_id)
        profile_response = await run_in_threadpool(profile_query.execute)
        
        if not profile_response.data:
            print(f"âš ï¸ User profile for {user_id} not found. Creating a new one.")
            insert_query = supabase.table("user_profiles").insert({
                "id": user_id, 
                "user_nick_nm": "ì‚¬ìš©ì" 
            })
            await run_in_threadpool(insert_query.execute)


        session_row = {
            "user_id": user_id, "text": payload.text, "icon": payload.icon,
            "profile": int(profile), "g_score": float(g),
            "intervention": json.dumps(intervention, ensure_ascii=False),
            "debug_log": json.dumps(debug_log, ensure_ascii=False),
            "summary": (debug_log.get("llm") or {}).get("summary", ""),
        }
        
        session_insert_query = supabase.table("sessions").insert(session_row)
        response = await run_in_threadpool(session_insert_query.execute)
        
        session_data = response.data[0] if response.data else {}
        new_session_id = session_data.get('id')

        if not new_session_id:
            print("ğŸš¨ ERROR: Failed to save session, no ID returned.")
            return None
        
        print(f"âœ… Session saved successfully. session_id: {new_session_id}")

        if final_scores:
            score_rows = [
                {"session_id": new_session_id, "user_id": payload.user_id, "cluster": c, "score": v}
                for c, v in final_scores.items()
            ]
            if score_rows:
                for row in score_rows:
                    row["session_text"] = payload.text
                
                # ğŸ’¡ [ìˆ˜ì •] .executeë¥¼ ë¶„ë¦¬
                scores_insert_query = supabase.table("cluster_scores").insert(score_rows)
                await run_in_threadpool(scores_insert_query.execute)
        
        return new_session_id
    except Exception as e:
        print(f"ğŸš¨ Supabase save failed: {e}")
        traceback.print_exc()
        return None


# ---------- API Endpoints (ë¶„ë¦¬ëœ êµ¬ì¡°) ----------


# ======================================================================
# === /analyze ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ë¡œì§ (ë¶„ë¦¬ëœ í•¨ìˆ˜ë“¤) ===
# ======================================================================

async def _handle_moderation(text: str) -> bool:
    """OpenAI Moderation APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ í•´ ì½˜í…ì¸ ë¥¼ í™•ì¸í•˜ê³  ì°¨ë‹¨ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    is_flagged, categories = await moderate_text(text, OPENAI_KEY)
    if not is_flagged:
        return False

    allowed_categories = {'self-harm', 'self-harm/intent', 'self-harm/instructions', 'hate', 'harassment', 'violence'}
    should_block = any(cat not in allowed_categories and triggered for cat, triggered in categories.items())
    
    if should_block:
        print(f"ğŸš¨ [BLOCKED] Inappropriate content: '{text}', Categories: {categories}")
    else:
        print(f"âš ï¸ [PASSED] Delegating to internal safety check: '{text}', Categories: {categories}")
    
    return should_block

async def _handle_emoji_only_case(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """ì•„ì´ì½˜ë§Œ ì…ë ¥ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "EMOJI_REACTION"
    print("\n--- ğŸ§ EMOJI-ONLY ANALYSIS ---")

    selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower(), "neg_low")
    
    if selected_cluster == "neutral":
        intervention = {"preset_id": PresetIds.EMOJI_REACTION, "text": "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”?", "top_cluster": "neutral"}
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
        return {"session_id": session_id, "intervention": intervention}
    
    onboarding_scores = calculate_baseline_scores(payload.onboarding)
    icon_scores = {c: 1.0 if c == selected_cluster else 0.0 for c in CLUSTERS}

    w = FINAL_FUSION_WEIGHTS_NO_TEXT
    fused_scores = {c: clip01(onboarding_scores.get(c, 0.0) * w['onboarding'] + icon_scores.get(c, 0.0) * w['icon']) for c in CLUSTERS}
    
    # ì ìˆ˜ ìƒí•œì„ (Cap) ì ìš©
    final_scores = fused_scores.copy()
    if selected_cluster in final_scores:
        final_scores[selected_cluster] = min(final_scores[selected_cluster], EMOJI_ONLY_SCORE_CAP)

    g, profile = g_score(final_scores), pick_profile(final_scores, None)
    
    user_nick_nm, _ = await get_user_info(payload.user_id)
    reaction_text = await get_mention_from_db(
        "reaction", payload.language_code, 
        personality=payload.character_personality, cluster=selected_cluster, user_nick_nm=user_nick_nm
    )
    
    intervention = {"preset_id": PresetIds.EMOJI_REACTION, "empathy_text": reaction_text, "top_cluster": selected_cluster}
    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)

    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}

async def _handle_friendly_mode(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ì¹œêµ¬ ëª¨ë“œ'ì¼ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "FRIENDLY"
    print(f"\n--- ğŸ‘‹ FRIENDLY MODE: '{payload.text}' ---")

    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    system_prompt = get_system_prompt(
        mode='FRIENDLY', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )
    friendly_text = await call_llm(system_prompt, payload.text, OPENAI_KEY, expect_json=False)

    intervention = {"preset_id": PresetIds.FRIENDLY_REPLY, "text": friendly_text}
    session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
    
    return {"session_id": session_id, "intervention": intervention}

async def _run_analysis_pipeline(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ë¶„ì„ ëª¨ë“œ'ì¼ ê²½ìš°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "ANALYSIS"
    print(f"\n--- ğŸ§ ANALYSIS MODE: '{payload.text}' ---")

    # 1. ì‚¬ìš©ì ì •ë³´ ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    system_prompt = get_system_prompt(
        mode='ANALYSIS', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )
    onboarding_scores = calculate_baseline_scores(payload.onboarding)
    llm_payload = payload.dict()
    llm_payload["baseline_scores"] = onboarding_scores
    
    # 2. LLM í˜¸ì¶œ ë° 2ì°¨ ì•ˆì „ ì¥ì¹˜
    llm_json = await call_llm(system_prompt, json.dumps(llm_payload, ensure_ascii=False), OPENAI_KEY)
    debug_log["llm"] = llm_json

    is_crisis, crisis_scores = is_safety_text(payload.text, llm_json, debug_log)
    if is_crisis:
        print(f"ğŸš¨ 2nd Safety Check Triggered: '{payload.text}'")
        g, profile = g_score(crisis_scores), 1
        top_cluster = max(crisis_scores, key=crisis_scores.get)
        intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "solution_id": f"{top_cluster}_crisis_01", "cluster": top_cluster}
        session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
        return {"session_id": session_id, "final_scores": crisis_scores, "g_score": g, "profile": profile, "intervention": intervention}

    # 3. ëª¨ë“  ì ìˆ˜ ê³„ì‚° ë° ìœµí•©
    text_scores = calculate_text_scores(payload.text, llm_json)
    
    has_icon = payload.icon and ICON_TO_CLUSTER.get(payload.icon.lower()) != "neutral"
    icon_scores = {c: 0.0 for c in CLUSTERS}
    if has_icon:
        selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower())
        icon_scores[selected_cluster] = 1.0

    final_scores, weights_used = calculate_final_scores(text_scores, onboarding_scores, icon_scores, has_icon)
    debug_log["scores"] = {"weights_used": weights_used, "onboarding": onboarding_scores, "text": text_scores, "icon": icon_scores, "final": final_scores}
    print(f"Scores -> Onboarding: {_format_scores_for_print(onboarding_scores)}, Text: {_format_scores_for_print(text_scores)}, Icon: {_format_scores_for_print(icon_scores)}")
    print(f"Weights: {_format_scores_for_print(weights_used)} -> Final Scores: {_format_scores_for_print(final_scores)}")

    # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
    g, profile = g_score(final_scores), pick_profile(final_scores, llm_json)
    top_cluster = max(final_scores, key=final_scores.get, default="neg_low")
    print(f"G-Score: {g:.2f}, Profile: {profile}")
    
    empathy_text = (llm_json or {}).get("empathy_response", "ë§ˆìŒì„ ì‚´í”¼ëŠ” ì¤‘ì´ì—ìš”...")
    
    score_val = final_scores[top_cluster]
    level = "high" if score_val > 0.7 else "mid" if score_val > 0.4 else "low"
    analysis_text = await get_mention_from_db(
        "analysis", payload.language_code, personality=payload.character_personality,
        cluster=top_cluster, level=level,
        format_kwargs={"emotion": top_cluster, "score": int(score_val * 100)}
    )

    intervention = {"preset_id": PresetIds.SOLUTION_PROPOSAL, "empathy_text": empathy_text, "analysis_text": analysis_text, "top_cluster": top_cluster}
    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)

    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}



# ======================================================================
# ===          ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
 
@app.post("/analyze")
async def analyze_emotion(payload: AnalyzeRequest):
    """ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì™€ ì•„ì´ì½˜ì„ ë°›ì•„ ê°ì •ì„ ë¶„ì„í•˜ê³  ìŠ¤ì½”ì–´ë§ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = (payload.text or "").strip()
    debug_log: Dict[str, Any] = {"input": payload.dict()}

    try:
        # --- íŒŒì´í”„ë¼ì¸ 0: ìœ í•´ ì½˜í…ì¸  ê²€ì—´ ---
        if await _handle_moderation(text):
            return JSONResponse(status_code=400, content={"error": "Inappropriate content detected."})

        # --- íŒŒì´í”„ë¼ì¸ 1: ğŸŒ¸ CASE 2 - ì´ëª¨ì§€ë§Œ ìˆëŠ” ê²½ìš° ---
        if payload.icon and not text:
            debug_log["mode"] = "EMOJI_REACTION"
            return await _handle_emoji_only_case(payload, debug_log)

        # --- íŒŒì´í”„ë¼ì¸ 2: 1ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM í˜¸ì¶œ ì „) ---
        is_crisis, crisis_scores = is_safety_text(text, None, debug_log)
        if is_crisis:
            print(f"ğŸš¨ 1st Safety Check Triggered: '{text}'")
            g, profile = g_score(crisis_scores), 1
            top_cluster = max(crisis_scores, key=crisis_scores.get)
            intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "solution_id": f"{top_cluster}_crisis_01", "cluster": top_cluster}
            session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
            return {"session_id": session_id, "intervention": intervention}

        # --- íŒŒì´í”„ë¼ì¸ 3: Triage (ì¹œêµ¬ ëª¨ë“œ / ë¶„ì„ ëª¨ë“œ ë¶„ê¸°) ---
        rule_scores, _, _ = rule_scoring(text)
        is_simple_text = len(text) < 10 and max(rule_scores.values() or [0.0]) < 0.1
        
        if is_simple_text:
            triage_mode = 'FRIENDLY'
            debug_log["triage_decision"] = "Rule-based: Simple text"
        else:
            triage_mode = await call_llm(TRIAGE_SYSTEM_PROMPT, text, OPENAI_KEY, expect_json=False)
            debug_log["triage_decision"] = f"LLM Triage: {triage_mode}"

        # --- íŒŒì´í”„ë¼ì¸ 4: Triage ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ---
        if triage_mode == 'FRIENDLY':
            return await _handle_friendly_mode(payload, debug_log)
        else: # ANALYSIS
            return await _run_analysis_pipeline(payload, debug_log)

    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ UNHANDLED EXCEPTION in /analyze: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})




    #         top_cluster = ICON_TO_CLUSTER.get(payload.icon.lower(), "neg_low")

    #         if top_cluster == "neutral": 
    #         # RIN â™¥ : ë””í´íŠ¸ ì´ëª¨ì§€ëŠ” ë¶„ì„í•˜ì§€ ì•ŠìŒ 
    #         #  uiì—ì„œ ë§‰ì•„ë†“ê¸´ í• ê±´ë°, í˜¹ì‹œ ëª¨ë¥´ë‹ˆê¹Œ ì¼ë‹¨ êµ¬í˜„ 
    #             intervention = {
    #                 "preset_id": PresetIds.EMOJI_REACTION, 
    #                 "text": "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”?",
    #                 "top_cluster": "neutral"
    #             }
    #             session_id = await save_analysis_to_supabase(
    #                 payload, profile=0, g=0.5,
    #                 intervention=intervention,
    #                 debug_log=debug_log,
    #                 final_scores={}
    #             )
    #             if session_id:
    #                 intervention['session_id'] = session_id
    #             return {
    #                 "session_id": session_id,
    #                 "final_scores": {},  
    #                 "g_score": 0.5,
    #                 "profile": 0,
    #                 "intervention": intervention
    #             }
    #         pass
    #         print("\n--- ğŸ§ EMOJI-ONLY ANALYSIS DEBUG ğŸ§ ---")


    #         # 1. ì˜¨ë³´ë”© ì ìˆ˜ ê³„ì‚°
    #         onboarding_scores = calculate_baseline_scores(payload.onboarding or {})  
    #         print(f"Onboarding Scores: {_format_scores_for_print(onboarding_scores)}")

    #         # 2. ì´ëª¨ì§€ ì ìˆ˜ ìƒì„±
    #         icon_prior = {c: 0.0 for c in CLUSTERS}
    #         selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower())
    #         if selected_cluster in icon_prior:
    #             icon_prior[selected_cluster] = 1.0

    #         # 3. ì˜¨ë³´ë”© 0.2 + ì´ëª¨ì§€ 0.8 ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì°¨ ìœµí•©
    #         w = FINAL_FUSION_WEIGHTS_NO_TEXT
    #         final_scores = {c: clip01(
    #             onboarding_scores.get(c, 0.0) * w['onboarding'] +
    #             icon_prior.get(c, 0.0) * w['icon']
    #         ) for c in CLUSTERS}
    #         print(f"Final Scores (after fusion): {_format_scores_for_print(final_scores)}")

    #         # 4. ì ìˆ˜ ìƒí•œì„ (Cap) ì ìš© ë¡œì§ 
    #         # ì˜¨ë³´ë”©+ì´ëª¨ì§€ ì ìˆ˜ëŠ” ìµœëŒ€ 0.5ê°€ ë˜ë„ë¡ 
    #         capped_scores = final_scores.copy()
    #         if selected_cluster in capped_scores:
    #             # original_score ë³€ìˆ˜ë¥¼ capped_scoresì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •í•˜ì—¬ NameError í•´ê²°
    #             original_score = capped_scores[selected_cluster]
    #             capped_scores[selected_cluster] = min(original_score, EMOJI_ONLY_SCORE_CAP)
            
    #             print(f"Score Capping Applied for '{selected_cluster}': {original_score:.4f} -> {capped_scores[selected_cluster]:.4f}")

    #         # 5. ìµœì¢… ì ìˆ˜(g_score)
    #         g = g_score(capped_scores)   
    #         profile = pick_profile(capped_scores, None)


    #         print(f"Final Scores (after capping): {_format_scores_for_print(capped_scores)}")
    #         print(f"G-Score: {g:.2f}")
    #         print(f"Profile: {profile}")
    #         print("------â¤ï¸-------------â¤ï¸-----------â¤ï¸-------\n")   

    #         # --- EMOJI_ONLY - DB ì €ì¥ ë° ë°˜í™˜ ë¡œì§---
    #         # Supabaseì—ì„œ í•´ë‹¹ ì´ëª¨ì§€ í‚¤ë¥¼ ê°€ì§„ ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ëª¨ë‘ ê°€ì ¸ì˜´
    #         reaction_text = "ê¸°ë¶„ì„ ì•Œë ¤ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”!"
    #         # reaction_scripts ëŒ€ì‹  character_mentions ì¡°íšŒ
    #         reaction_text = await get_mention_from_db(
    #             mention_type="reaction",
    #             personality=payload.character_personality,
    #             language_code=payload.language_code,
    #             cluster=ICON_TO_CLUSTER.get(payload.icon.lower(), "common"),
    #             default_message="ì–´ë–¤ ì¼ ë•Œë¬¸ì— ê·¸ë ‡ê²Œ ëŠë¼ì…¨ë‚˜ìš”?",
    #             format_kwargs={"user_nick_nm": user_nick_nm}
    #         )

    #         intervention = {
    #             "preset_id": PresetIds.EMOJI_REACTION, 
    #             "empathy_text": reaction_text,
    #             "top_cluster": top_cluster
    #         }

    #         # g_score/score ì €ì¥ì„ baseline+prior ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
    #         session_id = await save_analysis_to_supabase(
    #             payload, profile=profile, g=g,
    #             intervention=intervention,
    #             debug_log=debug_log,
    #             final_scores=capped_scores
    #         )

    #         if session_id:
    #             intervention['session_id'] = session_id

    #         return {
    #             "session_id": session_id,
    #             "final_scores": capped_scores,  
    #             "g_score": g,
    #             "profile": profile,
    #             "intervention": intervention
    #         }
    #     pass


    #     # --- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ëª¨ë“  ê²½ìš° ---


    # # <<<<<<<     ì•ˆì „ì¥ì¹˜    >>>>>>>>
    #     # --- íŒŒì´í”„ë¼ì¸ 1: 1ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM ì—†ì´) ---
    #     is_safe, safety_scores = is_safety_text(text, None, debug_log)
    #     if is_safe:
    #         print(f"ğŸš¨ 1ì°¨ ì•ˆì „ ì¥ì¹˜ ë°œë™: '{text}'")
    #         profile, g = 1, g_score(safety_scores)
    #         top_cluster = "neg_low"
    #         intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "solution_id": f"{top_cluster}_crisis_01", "cluster": top_cluster}
    #         session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, safety_scores)
    #         return {"session_id": session_id, "intervention": intervention}
    #     pass

    #     # --- íŒŒì´í”„ë¼ì¸ 2: Triage (ì¹œêµ¬ ëª¨ë“œ / ë¶„ì„ ëª¨ë“œ ë¶„ê¸°) ---
    #     # 1ì°¨ í•„í„°: í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ì§§ê³ , ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš° LLM í˜¸ì¶œ ì—†ì´ ë°”ë¡œ 'FRIENDLY'ë¡œ íŒë‹¨
    #     rule_scores, _, _ = rule_scoring(text)
        
    #     # ì¡°ê±´: í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 10ì ë¯¸ë§Œì´ê³ , ëª¨ë“  ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ê°€ 0.1 ë¯¸ë§Œì¼ ë•Œ
    #     is_simple_text = len(text) < 10 and max(rule_scores.values() or [0.0]) < 0.1

    #     if is_simple_text:
    #         triage_mode = 'FRIENDLY'
    #         debug_log["triage_decision"] = "Rule-based filter: Simple text"
    #     else:
    #     # 2ì°¨ íŒë‹¨: 1ì°¨ í•„í„°ë¥¼ í†µê³¼í•œ ê²½ìš°ì—ë§Œ LLMìœ¼ë¡œ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ê°€ ë¶„ì„ì´ í•„ìš”í•œ ë‚´ìš©ì¸ì§€, ë‹¨ìˆœ ëŒ€í™”ì¸ì§€ ë¨¼ì € íŒë‹¨!!
    #         triage_mode = await call_llm(
    #             system_prompt=TRIAGE_SYSTEM_PROMPT,
    #             user_content=text,
    #             openai_key=OPENAI_KEY,
    #             expect_json=False # 'ANALYSIS' OR 'FRIENDLY'
    #         )
    #         debug_log["triage_decision"] = f"LLM Triage: {triage_mode}"
    #         debug_log["triage_mode"] = triage_mode

    #     # Triage ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°
    #     if triage_mode == 'FRIENDLY':
    #         debug_log["mode"] = "FRIENDLY"
    #         print(f"\n--- ğŸ‘‹ FRIENDLY MODE DEBUG ---")
    #         print(f"Input text: '{text}' -> Classified as FRIENDLY")
    #         print("------â¤ï¸-------------â¤ï¸-----------â¤ï¸-------\n")
        

    #         # ğŸ¤© RIN: ì¹œêµ¬ ëª¨ë“œì—ì„œë„ ìºë¦­í„° ì„±í–¥ì„ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©í•˜ê¸°
    #         system_prompt = get_system_prompt(
    #             mode='FRIENDLY',
    #             personality=payload.character_personality,
    #             language_code=payload.language_code,
    #             user_nick_nm=user_nick_nm,
    #             character_nm=character_nm
    #         )           
    #         friendly_text = await call_llm(system_prompt, text, OPENAI_KEY, expect_json=False)

    #         intervention = {"preset_id": PresetIds.FRIENDLY_REPLY, "text": friendly_text}
    #         # ì¹œê·¼í•œ ëŒ€í™”ë„ ì„¸ì…˜ì„ ë‚¨ê¸¸ ìˆ˜ ìˆìŒ (ìŠ¤ì½”ì–´ëŠ” ë¹„ì–´ìˆìŒ)
    #         session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
    #         return {"session_id": session_id, "intervention": intervention}

    #     else: # triage_mode == 'ANALYSIS' ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’
    #         # --- íŒŒì´í”„ë¼ì¸ 3: ë¶„ì„ ëª¨ë“œ ---
    #         debug_log["mode"] = "ANALYSIS"
    #         print("\n--- ğŸ§ TEXT ANALYSIS DEBUG ğŸ§ ---")

    #         # 3-1. ì˜¨ë³´ë”© ì ìˆ˜(Baseline) ê³„ì‚°
    #         onboarding_scores = calculate_baseline_scores(payload.onboarding or {})
    #         print(f"1. Onboarding Scores:\n{_format_scores_for_print(onboarding_scores)}")

    #         # 3-2. í…ìŠ¤íŠ¸ ë¶„ì„ ì ìˆ˜(fused_scores) ê³„ì‚° (LLM, Rule-based í¬í•¨)
    #         # rule_scores, _, _ = rule_scoring(text)
    #         # ğŸ¤© RIN: ë¶„ì„ ëª¨ë“œì—ì„œë„ ìºë¦­í„° ì„±í–¥ì„ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©í•˜ê¸°
    #         system_prompt = get_system_prompt(
    #             mode='ANALYSIS',
    #             personality=payload.character_personality,
    #             language_code=payload.language_code,
    #             user_nick_nm=user_nick_nm,
    #             character_nm=character_nm
    #         )      
    #         llm_payload = payload.dict()
    #         llm_payload["baseline_scores"] = onboarding_scores
    #         llm_json = await call_llm(system_prompt, json.dumps(llm_payload, ensure_ascii=False), OPENAI_KEY)
    #         debug_log["llm"] = llm_json
            
    #         # --- íŒŒì´í”„ë¼ì¸ 3.5: 2ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM ê²°ê³¼ ê¸°ë°˜) - ì ìˆ˜ ê³„ì‚° ì „ ì‹¤í–‰ ---
    #         is_safe_llm, crisis_scores_llm = is_safety_text(text, llm_json, debug_log)
    #         if is_safe_llm:
    #             print(f"ğŸš¨ 2ì°¨ ì•ˆì „ ì¥ì¹˜ ë°œë™: '{text}'")
    #             # ì•ˆì „ ëª¨ë“œ ë°œë™ ì‹œì—ëŠ” ìœ„ê¸° ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  DBì— ì €ì¥
    #             profile, g = 1, g_score(crisis_scores_llm)
    #             top_cluster = "neg_low"
    #             intervention = {
    #                 "preset_id": PresetIds.SAFETY_CRISIS_MODAL,
    #                 "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.",
    #                 "solution_id": f"{top_cluster}_crisis_01",
    #                 "cluster": top_cluster
    #             }
    #             # ì´ ê²½ìš°, ì‹¤ì œ ê³„ì‚°ëœ ì ìˆ˜ê°€ ì•„ë‹Œ ìœ„ê¸° ì ìˆ˜(crisis_scores_llm)ë¥¼ ì €ì¥
    #             session_id = await save_analysis_to_supabase(
    #                 payload, profile=profile, g=g, intervention=intervention,
    #                 debug_log=debug_log, final_scores=crisis_scores_llm
    #             )
    #             # ë°˜í™˜ê°’ë„ ìœ„ê¸° ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
    #             return {
    #                 "session_id": session_id,
    #                 "final_scores": crisis_scores_llm,
    #                 "g_score": g,
    #                 "profile": profile,
    #                 "intervention": intervention
    #             }

    #         # === ì•ˆì „ì¥ì¹˜ ëª¨ë‘ í†µê³¼ ì‹œ ===
    #         # --- íŒŒì´í”„ë¼ì¸ 4: ì „ì²´ ìŠ¤ì½”ì–´ë§ ë¡œì§ ---
    #         # 4-1. í…ìŠ¤íŠ¸ ë¶„ì„ ì ìˆ˜(fused_scores) ê³„ì‚° 
    #         rule_scores, _, _ = rule_scoring(text)
    #         text_if = {c: 0.0 for c in CLUSTERS}
    #         if llm_json and not llm_json.get("error"):
    #             I, F = llm_json.get("intensity", {}), llm_json.get("frequency", {})
    #             for c in CLUSTERS:
    #                 In = clip01((I.get(c, 0.0) or 0.0) / 3.0)
    #                 Fn = clip01((F.get(c, 0.0) or 0.0) / 3.0)
    #                 text_if[c] = clip01(0.6 * In + 0.4 * Fn + 0.1 * rule_scores.get(c, 0.0))
            
    #         fused_scores = {c: clip01(W_RULE * rule_scores.get(c, 0.0) + W_LLM * text_if.get(c, 0.0)) for c in CLUSTERS}
    #         print(f"2a. Rule-Based Scores:\n{_format_scores_for_print(rule_scores)}")
    #         print(f"2b. LLM-based Scores (I/F fusion):\n{_format_scores_for_print(text_if)}")
    #         print(f"2c. Fused Text Scores (Rule + LLM):\n{_format_scores_for_print(fused_scores)}")

    #         # 4-2. ì´ëª¨ì§€ ì ìˆ˜(icon_prior) ìƒì„±
    #         icon_prior = {c: 0.0 for c in CLUSTERS}
    #         has_icon = payload.icon and ICON_TO_CLUSTER.get(payload.icon.lower()) != "neutral"
    #         if has_icon:
    #             selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower())
    #             icon_prior[selected_cluster] = 1.0        
    #         print(f"3. Icon Prior Scores:\n{_format_scores_for_print(icon_prior)}")

            
    #         # --- ê°€ì¤‘ì¹˜ ì¬ì¡°ì • ë¡œì§ ---

    #         # 4-3. FINAL_FUSION_WEIGHTSë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì ìˆ˜ ìœµí•© (ê°€ì¤‘ì¹˜ ì¬ì¡°ì • í¬í•¨)
    #         w = FINAL_FUSION_WEIGHTS

    #         if not has_icon:
    #         # RIN ğŸŒ¸ CASE 1: í…ìŠ¤íŠ¸ë§Œ ì…ë ¥ ì‹œ (icon ì—†ìŒ) -> icon ê°€ì¤‘ì¹˜ë¥¼ textì™€ onboardingì— ë¹„ë¡€ ë°°ë¶„
    #             w_text = w['text'] + w['icon'] * (w['text'] / (w['text'] + w['onboarding']))
    #             w_onboarding = w['onboarding'] + w['icon'] * (w['onboarding'] / (w['text'] + w['onboarding']))
    #             w_icon = 0.0
    #         else:
    #         # RIN ğŸŒ¸ CASE 3: í…ìŠ¤íŠ¸ + ì´ëª¨ì§€ ì…ë ¥ ì‹œ -> ëª¨ë“  ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    #             w_text, w_onboarding, w_icon = w['text'], w['onboarding'], w['icon']

    #         weights_used = {"text": w_text, "onboarding": w_onboarding, "icon": w_icon}
    #         print(f"4. Final Fusion Weights:\n{_format_scores_for_print(weights_used)}")

    #         adjusted_scores = {c: clip01(
    #             fused_scores.get(c, 0.0) * w_text +
    #             onboarding_scores.get(c, 0.0) * w_onboarding +
    #             icon_prior.get(c, 0.0) * w_icon
    #         ) for c in CLUSTERS}
    #         print(f"5. Final Adjusted Scores (after fusion):\n{_format_scores_for_print(adjusted_scores)}")

    #         debug_log["scores"] = {
    #             "weights_used": {"text": w_text, "onboarding": w_onboarding, "icon": w_icon},
    #             "1_onboarding_scores": onboarding_scores,
    #             "2_text_fused_scores": fused_scores,
    #             "3_icon_prior": icon_prior,
    #             "4_final_adjusted_scores": adjusted_scores
    #         }
            
    #         # 5. ìµœì¢… ê²°ê³¼ ìƒì„±
    #         g = g_score(adjusted_scores)
    #         profile = pick_profile(adjusted_scores, llm_json)
    #         top_cluster = max(adjusted_scores, key=adjusted_scores.get, default="neg_low")
            
    #         print(f"G-Score: {g:.2f}")
    #         print(f"Profile: {profile}")
    #         print("------â¤ï¸-------------â¤ï¸-----------â¤ï¸-------\n")


    #         debug_log["scores"] = {
    #             "weights_used": {"text": w_text, "onboarding": w_onboarding, "icon": w_icon},
    #             "final_adjusted_scores": adjusted_scores
    #         }

    #         # LLMìœ¼ë¡œë¶€í„° ê³µê° ë©”ì‹œì§€ì™€ ë¶„ì„ ë©”ì‹œì§€ë¥¼ ê°ê° ê°€ì ¸ì˜´
    #         empathy_text = (llm_json or {}).get("empathy_response", "ë§ˆìŒì„ ì‚´í”¼ëŠ” ì¤‘ì´ì—ìš”...")
    #         # ğŸ¤© RIN: get_analysis_message í˜¸ì¶œ ì‹œ ìºë¦­í„° ì„±í–¥ì„ ë„˜ê²¨ì£¼ê³  DBì—ì„œ ë§ëŠ” ë©˜íŠ¸ ê°€ì ¸ì˜´
    #         analysis_text = await get_analysis_message(
    #             adjusted_scores, 
    #             payload.character_personality,
    #             payload.language_code
    #         )
                            
            
    #         # 4-4. Intervention ê°ì²´ ìƒì„± ë° ë°˜í™˜ 
    #         intervention = {
    #             "preset_id": PresetIds.SOLUTION_PROPOSAL,
    #             "empathy_text": empathy_text, 
    #             "analysis_text": analysis_text,
    #             "top_cluster": top_cluster
    #         }
            
    #         session_id = await save_analysis_to_supabase(
    #             payload, profile=profile, g=g, intervention=intervention,
    #             debug_log=debug_log, final_scores=adjusted_scores
    #         )
            
    #         if session_id:
    #             intervention['session_id'] = session_id


    #         return {
    #             "session_id": session_id,
    #             "final_scores": adjusted_scores,
    #             "g_score": g,
    #             "profile": profile,
    #             "intervention": intervention 
    #         }

    # except Exception as e:
    #     tb = traceback.format_exc()
    #     raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    #     pass

# ======================================================================
# ===          ì†”ë£¨ì…˜ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
   

@app.post("/solutions/propose")
async def propose_solution(payload: SolutionRequest): 
    """ë¶„ì„ ê²°ê³¼(top_cluster)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë§ëŠ” ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ëŠ” ë¡œì§"""
    if not supabase: raise HTTPException(status_code=500, detail="Supabase client not initialized")
        
    try:
        user_nick_nm, _ = await get_user_info(payload.user_id)
        # personalityëŠ” get_mention_from_db ë‚´ë¶€ì—ì„œ ì¿¼ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„  í•„ìš” ì—†ìŒ
        
        proposal_script = await get_mention_from_db(
            "propose", payload.language_code,
            cluster=payload.top_cluster, user_nick_nm=user_nick_nm
        )
            
        solutions_res = await run_in_threadpool(
            supabase.table("solutions").select("solution_id, text, url, start_at, end_at, context")
            .eq("cluster", payload.top_cluster).execute
        )
        
        if not solutions_res.data:
            return {"proposal_text": "ì§€ê¸ˆì€ ì œì•ˆí•´ë“œë¦´ íŠ¹ë³„í•œ í™œë™ì´ ì—†ë„¤ìš”.", "solution_id": None}
                  
        # 4. ê°€ì ¸ì˜¨ ì†”ë£¨ì…˜ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
        solution_data = random.choice(solutions_res.data)
        solution_id = solution_data.get("solution_id")

        # ì†”ë£¨ì…˜ IDê°€ ì—†ëŠ” ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ 
        if not solution_id:
            return {
                "proposal_text": "ì§€ê¸ˆì€ ì œì•ˆí•´ë“œë¦´ë§Œí•œ íŠ¹ë³„í•œ í™œë™ì´ ì—†ë„¤ìš”. ëŒ€ì‹ , í¸ì•ˆí•˜ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆê¹Œìš”?", 
                "solution_id": None,
                "solution_details": None
            }
        
       # 5. ìµœì¢… ì œì•ˆ í…ìŠ¤íŠ¸ë¥¼ ì¡°í•© (ë©˜íŠ¸ + ì†”ë£¨ì…˜ ìì²´ í…ìŠ¤íŠ¸)
        final_text = f"{proposal_script} {solution_data.get('text', '')}".strip()
        if solution_data and solution_data.get('text'):
            # ë©˜íŠ¸ì™€ ì†”ë£¨ì…˜ í…ìŠ¤íŠ¸ ì‚¬ì´ì— ìì—°ìŠ¤ëŸ¬ìš´ ê³µë°± ì¶”ê°€
            final_text = f"{proposal_script} {solution_data['text']}"


        # 4. ì œì•ˆ ì´ë ¥ì„ ë¡œê·¸ë¡œ ì €ì¥
        log_entry = {"session_id": payload.session_id, "type": "propose", "solution_id": solution_id}
        await run_in_threadpool(supabase.table("interventions_log").insert(log_entry).execute)

        return {"proposal_text": final_text, "solution_id": solution_id, "solution_details": solution_data}
    
    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})


# ======================================================================
# ===          í™ˆí™”ë©´ ëŒ€ì‚¬ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
# ğŸ¤© RIN: í™ˆ ëŒ€ì‚¬ë“¤ì„ ì„±í–¥ë³„ë¡œ ë¶ˆëŸ¬ì˜¤ë„ë¡ ë³€ê²½
@app.get("/dialogue/home")
async def get_home_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko',
    emotion: Optional[str] = None 
):
    """í™ˆ í™”ë©´ì— í‘œì‹œí•  ëŒ€ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if emotion:
        # ì´ëª¨ì§€ê°€ ì„ íƒëœ ê²½ìš°: 'reaction' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "reaction"
        cluster = ICON_TO_CLUSTER.get(emotion.lower(), "common")
    else:
        # ì´ëª¨ì§€ê°€ ì—†ëŠ” ì´ˆê¸° ìƒíƒœ: 'home' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "home"
        cluster = "common"

    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        personality=personality,
        language_code=language_code,
        cluster=cluster,
        default_message=f"ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}
    
# ======================================================================
# ===  ì†”ë£¨ì…˜ ì™„ë£Œ í›„ í›„ì† ì§ˆë¬¸ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸   ===
# ======================================================================
@app.get("/dialogue/solution-followup")
async def get_solution_followup_dialogue(
    reason: str, # 'user_closed' ë˜ëŠ” 'video_ended'
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ì´ ëë‚œ í›„ì˜ ìƒí™©(reason)ê³¼ ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” í›„ì† ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # ì´ìœ (reason)ì— ë”°ë¼ DBì—ì„œ ì¡°íšŒí•  mention_typeì„ ê²°ì •í•©ë‹ˆë‹¤.
    if reason == 'user_closed':
        mention_type = "followup_user_closed"
    else: # 'video_ended' ë˜ëŠ” ê¸°íƒ€
        mention_type = "followup_video_ended"

    # get_mention_from_db í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        personality=personality,
        language_code=language_code,
        cluster="common", 
        default_message="ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ======================================================================
# ===  ì†”ë£¨ì…˜ ì™„ë£Œ í›„ í›„ì† ì§ˆë¬¸ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸   ===
# ======================================================================

# ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí–ˆì„ ë•Œì˜ ë©˜íŠ¸ë¥¼ ì„±í–¥ë³„ë¡œ ì£¼ê¸° ìœ„í•´ ì¶”ê°€
@app.get("/dialogue/decline-solution")
async def get_decline_solution_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí•˜ê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³  ì‹¶ì–´í•  ë•Œì˜ ë°˜ì‘ ë©˜íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    dialogue_text = await get_mention_from_db(
        mention_type="decline_solution",
        personality=personality,
        language_code=language_code,
        cluster="common",
        default_message="ì•Œê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ìš”. ì €ì—ê²Œ í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”. ê·€ ê¸°ìš¸ì—¬ ë“£ê³  ìˆì„ê²Œìš”.",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ======================================================================
# ===          ì†”ë£¨ì…˜ ì˜ìƒ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================

    # SolutionPageì—ì„œ ì˜ìƒ ë¡œë“œ
    # í•˜ë“œì½”ë”©ëœ SOLUTION_DETAILS_LIBRARYë¥¼ DB ì¡°íšŒë¡œ ëŒ€ì²´í–ˆìŒ!!
@app.get("/solutions/{solution_id}")
async def get_solution_details(solution_id: str):
    print(f"RIN: âœ… ì†”ë£¨ì…˜ ìƒì„¸ ì •ë³´ ìš”ì²­ ë°›ìŒ: {solution_id}")
    
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")
    
    try:
        # solutions í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¡°íšŒ
        response = await run_in_threadpool(
            supabase.table("solutions")
            .select("url, start_at, end_at")
            .eq("solution_id", solution_id)
            .single()
            .execute
        )
        
        solution_data = response.data
        
        if not solution_data:
            raise HTTPException(status_code=404, detail="Solution not found")

        # ìœ íŠœë¸Œ ë¶ˆëŸ¬ì˜¬ë•Œ startAt, endAt (camelCase)ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ í‚¤ë¥¼ ë³€í™˜í•´ì¤Œ
        # supabaseëŠ” snake_caseë¡œ ì €ì¥í•´ì•¼ í•œë‹¤ê³  í•¨.
        response_data = {
            'url': solution_data.get('url'),
            'startAt': solution_data.get('start_at'),
            'endAt': solution_data.get('end_at')
        }
        
        print(f"RIN: âœ… ì†”ë£¨ì…˜ ì •ë³´ ë°˜í™˜: {response_data}")
        return response_data
        
    except Exception as e:
        print(f"RIN: âŒ í•´ë‹¹ ì†”ë£¨ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {solution_id}, ì—ëŸ¬: {e}")
        raise HTTPException(status_code=404, detail="Solution not found")
    

# ======================================================================
# ===     ë¦¬í¬íŠ¸ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
class DailyReportRequest(BaseModel):
    user_id: str
    date: str # "YYYY-MM-DD" í˜•ì‹
    language_code: str = 'ko'

@app.post("/report/summary")
async def get_daily_report_summary(request: DailyReportRequest):
    """íŠ¹ì • ë‚ ì§œì˜ ì‚¬ìš©ì í™œë™ì„ ì¢…í•©í•˜ì—¬ LLMìœ¼ë¡œ ìš”ì•½ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        # --- 1. í•„ìš”í•œ ì •ë³´ DBì—ì„œ ìˆ˜ì§‘ ---
        user_nick_nm, character_nm = await get_user_info(request.user_id)

        # a) í•´ë‹¹ ë‚ ì§œì˜ ê°€ì¥ ë†’ì€ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        start_of_day = f"{request.date}T00:00:00+00:00"
        end_of_day = f"{request.date}T23:59:59+00:00"
        
         # --- 1. ëŒ€í‘œ í´ëŸ¬ìŠ¤í„° ë° í‰ê·  ì ìˆ˜ ê³„ì‚° ---
        
        # Step 1: ê·¸ë‚ ì˜ ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        score_query = supabase.table("cluster_scores").select("cluster, score") \
            .eq("user_id", request.user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day)
        score_res = await run_in_threadpool(score_query.execute)
        
        all_scores_today = score_res.data
        if not all_scores_today:
            return {"summary": "í•´ë‹¹ ë‚ ì§œì— ëŒ€í•œ ê°ì • ê¸°ë¡ì´ ì—†ì–´ìš”."}

        # ë¨¼ì € ê°€ì¥ ë†’ì€ ë‹¨ì¼ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€í‘œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì •í•©ë‹ˆë‹¤.
        top_score_entry = max(all_scores_today, key=lambda x: x['score'])
        top_cluster_name = top_score_entry['cluster']
        
        # ì´ì œ, ëŒ€í‘œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì ìˆ˜ë¥¼ ëª¨ìë‹ˆë‹¤.
        scores_for_top_cluster = [
            item['score'] for item in all_scores_today if item['cluster'] == top_cluster_name
        ]

        # í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        average_score = sum(scores_for_top_cluster) / len(scores_for_top_cluster)
        top_score_for_llm = int(average_score * 100) # LLMì— ì „ë‹¬í•  ìµœì¢… ì ìˆ˜ (0-100)

        # --- 2. ê·¸ë‚ ì˜ ëª¨ë“  ëŒ€í™” ìš”ì•½ ê°€ì ¸ì˜¤ê¸° ---
        # ğŸ’¡ [ìˆ˜ì • 2] sessions í…Œì´ë¸”ì—ì„œ 'summary' ì»¬ëŸ¼ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        session_query = supabase.table("sessions").select("summary") \
            .eq("user_id", request.user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .not_.is_("summary", "null") # summaryê°€ nullì´ ì•„ë‹Œ ê²ƒë§Œ
        session_res = await run_in_threadpool(session_query.execute)
        dialogue_summaries = [s['summary'] for s in session_res.data if s.get('summary')]

        # --- 3. ê·¸ë‚  ì œê³µëœ ì†”ë£¨ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
        user_sessions_query = supabase.table("sessions").select("id") \
            .eq("user_id", request.user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day)
        user_sessions_res = await run_in_threadpool(user_sessions_query.execute)
        
        solution_contexts = []
        if user_sessions_res.data:
            session_ids = [s['id'] for s in user_sessions_res.data]
            log_query = supabase.table("interventions_log").select("solution_id") \
                .in_("session_id", session_ids) \
                .eq("type", "propose")
            log_res = await run_in_threadpool(log_query.execute)
            if log_res.data:
                solution_ids = list(set([log['solution_id'] for log in log_res.data])) # ì¤‘ë³µ ì œê±°
                solution_query = supabase.table("solutions").select("context").in_("solution_id", solution_ids)
                solution_res = await run_in_threadpool(solution_query.execute)
                solution_contexts = [s['context'] for s in solution_res.data if s.get('context')]

        # --- 4. ëŒ€í‘œ í´ëŸ¬ìŠ¤í„° ì¡°ì–¸ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
        advice_text = await get_mention_from_db(
            "analysis", request.language_code, cluster=top_cluster_name, level="high"
        )

        # --- 5. LLMì— ì „ë‹¬í•  ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© ---
        llm_context = {
            "user_nick_nm": user_nick_nm,
            "top_cluster_today": top_cluster_name,
            "top_score_today": top_score_for_llm, # í‰ê·  ì ìˆ˜ ì‚¬ìš©
            "user_dialogue_summary": " ".join(dialogue_summaries) or "íŠ¹ë³„í•œ ëŒ€í™”ëŠ” ì—†ì—ˆì–´ìš”.",
            "solution_context": ", ".join(solution_contexts) or "ì œê³µëœ ì†”ë£¨ì…˜ì´ ì—†ì—ˆì–´ìš”.",
            "cluster_advice": advice_text
        }
        
        system_prompt = REPORT_SUMMARY_PROMPT

        # --- 6. LLM í˜¸ì¶œ ---
        summary_json = await call_llm(
            system_prompt=system_prompt,
            user_content=json.dumps(llm_context, ensure_ascii=False),
            openai_key=OPENAI_KEY
        )

        if summary_json.get("error"):
            raise HTTPException(status_code=500, detail=f"LLM summary generation failed: {summary_json.get('error')}")

        return {"summary": summary_json.get("daily_summary", "ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”.")}

    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ UNHANDLED EXCEPTION in /report/summary: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    