# main.py
from __future__ import annotations

import asyncio
import datetime as dt
import json
import os
import re
import traceback
import random
from typing import Optional, List, Dict, Any, Tuple
import uuid

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.concurrency import run_in_threadpool
from fastapi.responses import JSONResponse
import numpy as np
from pydantic import BaseModel
from supabase import create_client, Client

from ai_moderator import moderate_text
from llm_prompts import (
    REPORT_SUMMARY_PROMPT, WEEKLY_REPORT_SUMMARY_PROMPT_STANDARD, WEEKLY_REPORT_SUMMARY_PROMPT_NEURO,
    call_llm, get_adhd_breakdown_prompt, get_system_prompt, TRIAGE_SYSTEM_PROMPT, FRIENDLY_SYSTEM_PROMPT
)
from rule_based import rule_scoring
from srj5_constants import (
    ASSESSMENT_SCORE_CAP, CLUSTER_TO_DISPLAY_NAME, CLUSTERS, DEEP_DIVE_MAX_SCORES, EMOJI_ONLY_SCORE_CAP, FINAL_FUSION_WEIGHTS_NO_ICON, ICON_TO_CLUSTER, ONBOARDING_MAPPING,
    FINAL_FUSION_WEIGHTS, FINAL_FUSION_WEIGHTS_NO_TEXT,
    W_LLM, W_RULE, 
    SAFETY_LEMMAS, SAFETY_LEMMA_COMBOS, SAFETY_REGEX, SAFETY_FIGURATIVE
)


try:
    from kiwipiepy import Kiwi
    _kiwi = Kiwi()
except ImportError:
    _kiwi = None
    print("âš ï¸ kiwipiepy is not installed. Some safety features will be disabled.")


# --- í™˜ê²½ì„¤ì • ---
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
# BIND_HOST = os.getenv("BIND_HOST", "0.0.0.0")
# PORT = int(os.getenv("PORT", "8000"))
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
# Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
supabase: Optional[Client] = None


# --- FastAPI ì•± ì´ˆê¸°í™” ë° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
app = FastAPI(title="DailyMoji API v2 (Separated Logic)")

# supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@app.on_event("startup")
def startup_event():
    global supabase
    if SUPABASE_URL and SUPABASE_KEY:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("âœ… FastAPI server started and Supabase client initialized.")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- ë°ì´í„° ëª¨ë¸ (ë¶„ë¦¬ëœ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •) ---


# ì´ì „ ëŒ€í™” ê¸°ë¡(history)ì„ ë°›ê¸° ìœ„í•œ ëª¨ë¸ ìˆ˜ì •
class HistoryItem(BaseModel):
    sender: str
    content: str

# /analyze ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AnalyzeRequest(BaseModel):
    user_id: str
    text: str
    icon: Optional[str] = None
    language_code: Optional[str] = 'ko'
    timestamp: Optional[str] = None
    onboarding: Optional[Dict[str, Any]] = None
    character_personality: Optional[str] = None
    history: Optional[List[HistoryItem]] = None 
    # ADHD ë¶„ê¸° ë¡œì§ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒíƒœ ì •ë³´
    adhd_context: Optional[Dict[str, Any]] = None


# /solutions/propose ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class SolutionRequest(BaseModel):
    user_id: str
    session_id: str
    top_cluster: str
    language_code: str = 'ko'

      
class FeedbackRequest(BaseModel):
    user_id: str
    solution_id: str
    session_id: Optional[str] = None
    solution_type: str
    feedback: str


class BackfillRequest(BaseModel):
    start_date: str  # "YYYY-MM-DD" í˜•ì‹
    end_date: str    # "YYYY-MM-DD" í˜•ì‹


# /assessment/submit ì—”ë“œí¬ì¸íŠ¸ì˜ ì…ë ¥ ëª¨ë¸
class AssessmentSubmitRequest(BaseModel):
    user_id: str
    cluster: str  # "neg_low", "neg_high" ë“± í‰ê°€í•œ í´ëŸ¬ìŠ¤í„°
    responses: Dict[str, int] # {"NGL_01": 3, "NGL_02": 2, ...} í˜•íƒœì˜ ë‹µë³€


# Flutterì˜ PresetIdsì™€ ë™ì¼í•œ êµ¬ì¡°
class PresetIds:
    FRIENDLY_REPLY = "FRIENDLY_REPLY"
    SOLUTION_PROPOSAL = "SOLUTION_PROPOSAL"
    SAFETY_CRISIS_MODAL = "SAFETY_CRISIS_MODAL"
    EMOJI_REACTION = "EMOJI_REACTION"
    ADHD_PRE_SOLUTION_QUESTION = "ADHD_PRE_SOLUTION_QUESTION"
    ADHD_AWAITING_TASK_DESCRIPTION = "ADHD_AWAITING_TASK_DESCRIPTION"
    ADHD_TASK_BREAKDOWN = "ADHD_TASK_BREAKDOWN"



# ======================================================================
# === kiwië¥¼ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ ì¥ì¹˜ ===
# ======================================================================

# --- ì•ˆì „ ì¥ì¹˜ ë¡œì§ ---

def _find_regex_matches(text: str, patterns: List[str]) -> List[str]:
    return [m.group(0) for pat in patterns for m in re.finditer(pat, text, flags=re.IGNORECASE)]

def _kiwi_detect_safety_lemmas(text: str) -> List[str]:
    if not _kiwi: return []
    try:
        tokens = _kiwi.tokenize(text)
        all_lemmas_in_text = {t.lemma for t in tokens}
        hits = {token.lemma for token in tokens if token.lemma in SAFETY_LEMMAS}
        for combo in SAFETY_LEMMA_COMBOS:
            if combo.issubset(all_lemmas_in_text):
                hits.update(combo)
        return list(hits)
    except Exception as e:
        print(f"Kiwi safety check error: {e}")
        return []

def is_safety_text(text: str, llm_json:  Optional[dict], debug_log: dict) -> Tuple[bool, dict]:
    """
    ì ìˆ˜ ì²´ê³„ ë° ì•ˆì „ ëª¨ë“œ ì„¤ëª…: ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì—ì„œ ìí•´/ìì‚´ ìœ„í—˜ì„ ë‹¤ë‹¨ê³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    - 1ë‹¨ê³„: "ì¡¸ë ¤ ì£½ê² ë‹¤"ì™€ ê°™ì´ ëª…ë°±íˆ ì•ˆì „í•œ ë¹„ìœ ì  í‘œí˜„ì„ ë¨¼ì € ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
    - 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„('ì£½ë‹¤', 'ìì‚´' ë“±)ê³¼ LLMì˜ ì˜ë„ ë¶„ì„('self_harm' í”Œë˜ê·¸)ìœ¼ë¡œ ìœ„í—˜ ì‹ í˜¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    - "ë‹¤ ë•Œë ¤ì¹˜ìš°ê³  ì‹¶ë‹¤"ì™€ ê°™ì€ ë¬¸ì¥ì€ ëª…ì‹œì ì¸ ìí•´ ë‹¨ì–´ê°€ ì—†ì–´ 1, 2ë‹¨ê³„ë¥¼ í†µê³¼í•  ìˆ˜ ìˆì§€ë§Œ,
      LLMì´ ë¬¸ì¥ì˜ ì ˆë§ì ì¸ ë‰˜ì•™ìŠ¤ë¥¼ 'self_harm: possible'ë¡œ íŒë‹¨í•˜ë©´ ì•ˆì „ ì¥ì¹˜ê°€ ë°œë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      ì´ëŸ¬í•œ ì˜¤íƒì§€ëŠ” ëª¨ë¸ì˜ ë³´ìˆ˜ì ì¸ ì•ˆì „ ì„¤ê³„ ë•Œë¬¸ì´ë©°, ì§€ì†ì ì¸ í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """

    # 1ë‹¨ê³„: ë¹„ìœ ì /ê´€ìš©ì  í‘œí˜„ ìš°ì„  í•„í„°ë§ ("ì¡¸ë ¤ ì£½ê² ë‹¤" ë“±)
    # SAFETY_FIGURATIVEì— ë§¤ì¹˜ë˜ë©´, ìœ„í—˜í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ ì¢…ë£Œ
    figurative_matches = [m.group(0) for pat in SAFETY_FIGURATIVE for m in re.finditer(pat, text, flags=re.IGNORECASE)]
    if figurative_matches:
        debug_log["safety"] = {
            "triggered": False,
            "reason": "Figurative speech detected",
            "matches": figurative_matches
        }
        return False, {}

    # 2ë‹¨ê³„: Kiwi í˜•íƒœì†Œ ë¶„ì„ ë° LLM ì˜ë„ ë¶„ì„
    kiwi_lemma_hits = []
    if _kiwi:
        try:
            tokens = _kiwi.tokenize(text)
            lemmas_in_text = {t.lemma for t in tokens}
            hits = {t.lemma for t in tokens if t.lemma in SAFETY_LEMMAS}
            for combo in SAFETY_LEMMA_COMBOS:
                if combo.issubset(lemmas_in_text):
                    hits.update(combo)
            kiwi_lemma_hits = list(hits)
        except Exception as e:
            print(f"Kiwi safety check error: {e}")

    safety_llm_flag = (llm_json or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}
    
    # Kiwi ë˜ëŠ” LLM ì¤‘ í•˜ë‚˜ë¼ë„ ìœ„í—˜ ì‹ í˜¸ë¥¼ ê°ì§€í•˜ë©´ ì•ˆì „ì¥ì¹˜ ë°œë™
    triggered = bool(kiwi_lemma_hits) or safety_llm_flag

    debug_log["safety"] = {
        "kiwi_lemma_hits": kiwi_lemma_hits,
        "llm_intent_flag": safety_llm_flag,
        "triggered": triggered
    }

    if triggered:
        # ìœ„ê¸° ìƒí™©ì— ë§ëŠ” ê·¹ë‹¨ì ì¸ ì ìˆ˜ ë¶€ì—¬
        crisis_scores = {"neg_low": 0.95, "neg_high": 0.0, "adhd": 0.0, "sleep": 0.0, "positive": 0.0}
        return True, crisis_scores

    return False, {}



# ======================================================================
# === í•µì‹¬ ë¡œì§: ìŠ¤ì½”ì–´ë§ ë° ìœµí•© (Scoring & Fusion) ===
# ======================================================================

def calculate_final_scores(
    text_scores: dict,
    assessment_scores: dict,
    icon_scores: dict,
    has_icon: bool
) -> Tuple[dict, dict]:
    """í…ìŠ¤íŠ¸, ë§ˆìŒ ì ê²€, ì•„ì´ì½˜ ì ìˆ˜ë¥¼ ì¤‘ì•™ì—ì„œ ìœµí•©í•©ë‹ˆë‹¤."""
    if not has_icon:
        # CASE 1: í…ìŠ¤íŠ¸ë§Œ ì…ë ¥ ì‹œ -> ì•„ì´ì½˜ ê°€ì¤‘ì¹˜ë¥¼ ë¹„ë¡€ ë°°ë¶„
        w = FINAL_FUSION_WEIGHTS_NO_ICON
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": 0.0}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment']
        ) for c in CLUSTERS}
    else:
        # CASE 2: í…ìŠ¤íŠ¸ + ì•„ì´ì½˜ ì…ë ¥ ì‹œ -> ëª¨ë“  ê°€ì¤‘ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        w = FINAL_FUSION_WEIGHTS
        weights_used = {"text": w['text'], "assessment": w['assessment'], "icon": w['icon']}
        final_scores = {c: clip01(
            text_scores.get(c, 0.0) * w['text'] +
            assessment_scores.get(c, 0.0) * w['assessment'] +
            icon_scores.get(c, 0.0) * w['icon']
        ) for c in CLUSTERS}

    return final_scores, weights_used

def calculate_text_scores(text: str, llm_json: Optional[dict]) -> dict:
    """Rule-based ì ìˆ˜ì™€ LLM ì ìˆ˜ë¥¼ ìœµí•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì¢… ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    rule_scores, _, _ = rule_scoring(text)
    
    text_if = {c: 0.0 for c in CLUSTERS}
    if llm_json and not llm_json.get("error"):
        I, F = llm_json.get("intensity", {}), llm_json.get("frequency", {})
        for c in CLUSTERS:
            In = clip01((I.get(c, 0.0) or 0.0) / 3.0)
            Fn = clip01((F.get(c, 0.0) or 0.0) / 3.0)
            text_if[c] = clip01(0.6 * In + 0.4 * Fn + 0.1 * rule_scores.get(c, 0.0))
    
    fused_scores = {c: clip01(W_RULE * rule_scores.get(c, 0.0) + W_LLM * text_if.get(c, 0.0)) for c in CLUSTERS}
    return fused_scores

# ======================================================================
# === í—¬í¼ í•¨ìˆ˜ (Helpers) ===
# ======================================================================

def _format_scores_for_print(scores: dict) -> str:
    """ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜"""
    if not isinstance(scores, dict):
        return str(scores)
    return json.dumps({k: round(v, 2) if isinstance(v, float) else v for k, v in scores.items()})

def clip01(x: float) -> float: return max(0.0, min(1.0, float(x)))

def g_score(final_scores: dict) -> float:
    w = {"neg_high": 1.0, "neg_low": 0.9, "sleep": 0.7, "adhd": 0.6, "positive": -0.3}
    g = sum(final_scores.get(k, 0.0) * w.get(k, 0.0) for k in CLUSTERS)
    return round(clip01((g + 1.0) / 2.0), 3)

def pick_profile(final_scores: dict, llm: Optional[dict]) -> int: # surveys íŒŒë¼ë¯¸í„° ì œê±°
    if (llm or {}).get("intent", {}).get("self_harm") in {"possible", "likely"}: return 1
    if max(final_scores.get("neg_low", 0), final_scores.get("neg_high", 0)) > 0.85: return 1
    # if (surveys and (surveys.get("phq9", 0) >= 10 or surveys.get("gad7", 0) >= 10)) or max(final_scores.values()) > 0.60: return 2
    if max(final_scores.values()) > 0.60: return 2 # surveys í•„ë“œ ì œê±°
    if max(final_scores.values()) > 0.30: return 3
    return 0

def calculate_baseline_scores(onboarding_scores: Optional[Dict[str, int]]) -> Dict[str, float]:
    if not onboarding_scores: return {c: 0.0 for c in CLUSTERS}
    baseline = {c: 0.0 for c in CLUSTERS}
    for q_key, score in onboarding_scores.items():
        processed_score = 3 - score if q_key == 'q7' else score
        if q_key in ONBOARDING_MAPPING:
            normalized_score = processed_score / 3.0
            for mapping in ONBOARDING_MAPPING[q_key]:
                baseline[mapping["cluster"]] += normalized_score * mapping["w"]
    for c in CLUSTERS:
        baseline[c] = clip01(baseline.get(c, 0.0))
    return baseline

# ======================================================================
# === DB ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ ===
# ======================================================================

async def get_user_info(user_id: str) -> Tuple[str, str]:
    """ì‚¬ìš©ì ë‹‰ë„¤ì„ê³¼ ìºë¦­í„° ì´ë¦„ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not supabase: return "ì‚¬ìš©ì", "ëª¨ì§€"
    try:
        res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("user_nick_nm, character_nm")
            .eq("id", user_id).single().execute
        )
        if res.data:
            return res.data.get("user_nick_nm", "ì‚¬ìš©ì"), res.data.get("character_nm", "ëª¨ì§€")
    except Exception:
        pass
    return "ì‚¬ìš©ì", "ëª¨ì§€"


# ëª¨ë“  ë©˜íŠ¸ ì¡°íšŒë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def get_mention_from_db(mention_type: str, language_code: str, **kwargs) -> str:
    """DBì—ì„œ ì§€ì •ëœ íƒ€ì…ê³¼ ì¡°ê±´ì— ë§ëŠ” ìºë¦­í„° ë©˜íŠ¸ë¥¼ ëœë¤ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    print(f"--- âœ… get_mention_from_db í˜¸ì¶œë¨ (mention_type: {mention_type}) âœ… ---")

    default_messages = {
        "analysis": "ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
        "reaction": "ì–´ë–¤ ì¼ ë•Œë¬¸ì— ê·¸ë ‡ê²Œ ëŠë¼ì…¨ë‚˜ìš”?",
        "propose": "ì´ëŸ° í™œë™ì€ ì–´ë– ì„¸ìš”?",
        "home": "ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        "followup_user_closed": "ê´œì°®ì•„ìš”. ëŒ€í™”ë¥¼ ì´ì–´ë‚˜ê°ˆê¹Œìš”?",
        "followup_video_ended": "ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        "decline_solution": "ì•Œê² ìŠµë‹ˆë‹¤. í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”."
    }
    default_message = default_messages.get(mention_type, "...")

    # .format()ì— ì‚¬ìš©ë  ì¸ìë“¤ì„ ë¯¸ë¦¬ ì¤€ë¹„í•©ë‹ˆë‹¤.
    format_args = kwargs.get("format_kwargs", kwargs)

    def _safe_format(text: str) -> str:
        """KeyError ì—†ì´ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            # format_argsê°€ Noneì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ dictìœ¼ë¡œ ì²˜ë¦¬
            return text.format(**(format_args or {}))
        except KeyError:
            # í¬ë§·íŒ…ì— ì‹¤íŒ¨í•˜ë©´ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            return text.replace("{user_nick_nm}", "ì¹œêµ¬")

    if not supabase:
        return _safe_format(default_message)


    # if not supabase: return default_message.format(**kwargs) if kwargs else default_message
    try:
        query = supabase.table("character_mentions").select("text").eq("mention_type", mention_type).eq("language_code", language_code)
        
        valid_filter_keys = ["personality", "cluster", "level", "solution_variant"]
        for key, value in kwargs.items():
            if key in valid_filter_keys and value:
                query = query.eq(key, value)

        response = await run_in_threadpool(query.execute)
        scripts = [row['text'] for row in response.data]
        
        if not scripts:
            return _safe_format(default_message)
        
        selected_script = random.choice(scripts)
        # DBì—ì„œ ê°€ì ¸ì˜¨ ë©˜íŠ¸ë„ ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…
        return _safe_format(selected_script)

    except Exception as e:
        print(f"âŒ get_mention_from_db Error: {e}")
        return _safe_format(default_message)


# # ìˆ˜ì¹˜ë¥¼ ì£¼ê¸°ë³´ë‹¤ëŠ”, ì‹¬ê°ë„ 3ë‹¨ê³„ì— ë”°ë¼ ë©”ì‹œì§€ í•´ì„í•´ì£¼ëŠ”ê²Œ ë‹¬ë¼ì§(ìˆ˜ì¹˜í˜•x, ëŒ€í™”í˜•o)
# async def get_analysis_message(
#     scores: dict, 
#     personality: Optional[str], 
#     language_code: str
# ) -> str:
#     if not scores: return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ë” ë“¤ì—¬ë‹¤ë³´ê³  ìˆì–´ìš”."
#     top_cluster = max(scores, key=scores.get)
#     score_val = scores[top_cluster]
    
#     level = "low"
#     if score_val > 0.7: level = "high"
#     elif score_val > 0.4: level = "mid"
    
#     return await get_mention_from_db(
#         mention_type="analysis",
#         personality=personality,
#         language_code=language_code,
#         cluster=top_cluster,
#         level=level,
#         default_message="ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ ë§ˆìŒì€ íŠ¹ë³„í•œ ìƒ‰ì„ ë ê³  ìˆë„¤ìš”.",
#         format_kwargs={"emotion": top_cluster, "score": int(score_val * 100)}
#     )
    


async def save_analysis_to_supabase(
    payload: AnalyzeRequest, profile: int, g: float,
    intervention: dict, debug_log: dict, final_scores: dict
) -> Optional[str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not supabase: return None
    try:
        user_id = payload.user_id

        # ì„¸ì…˜ì„ ì €ì¥í•˜ê¸° ì „, user_profilesì— í•´ë‹¹ ìœ ì €ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        profile_query = supabase.table("user_profiles").select("id").eq("id", user_id)
        profile_response = await run_in_threadpool(profile_query.execute)
        
        if not profile_response.data:
            print(f"âš ï¸ User profile for {user_id} not found. Creating a new one.")
            insert_query = supabase.table("user_profiles").insert({
                "id": user_id, 
                "user_nick_nm": "ì‚¬ìš©ì" 
            })
            await run_in_threadpool(insert_query.execute)


        session_row = {
            "user_id": user_id, "text": payload.text, "icon": payload.icon,
            "profile": int(profile), "g_score": float(g),
            "intervention": json.dumps(intervention, ensure_ascii=False),
            "debug_log": json.dumps(debug_log, ensure_ascii=False),
            "summary": (debug_log.get("llm") or {}).get("summary", ""),
        }
        
        session_insert_query = supabase.table("sessions").insert(session_row)
        response = await run_in_threadpool(session_insert_query.execute)
        
        session_data = response.data[0] if response.data else {}
        new_session_id = session_data.get('id')

        if not new_session_id:
            print("ğŸš¨ ERROR: Failed to save session, no ID returned.")
            return None
        
        print(f"âœ… Session saved successfully. session_id: {new_session_id}")

        if final_scores:
            score_rows = [
                {"session_id": new_session_id, "user_id": payload.user_id, "cluster": c, "score": v}
                for c, v in final_scores.items()
            ]
            if score_rows:
                for row in score_rows:
                    row["session_text"] = payload.text
                
                # .executeë¥¼ ë¶„ë¦¬
                scores_insert_query = supabase.table("cluster_scores").insert(score_rows)
                await run_in_threadpool(scores_insert_query.execute)
        
        return new_session_id
    except Exception as e:
        print(f"ğŸš¨ Supabase save failed: {e}")
        traceback.print_exc()
        return None


# RIN: ADHD ì§ˆë¬¸ì— ëŒ€í•œ ì‚¬ìš©ì ë‹µë³€ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
async def _handle_adhd_response(payload: AnalyzeRequest, debug_log: dict):
    user_response = payload.text
    adhd_context = payload.adhd_context or {}
    current_step = adhd_context.get("step")

    # --- ì‹œë‚˜ë¦¬ì˜¤ 1: "ìˆì–´!" / "ì—†ì–´!" ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ ---
    if current_step == "awaiting_choice":
        # "ìˆì–´!" / "ì—†ì–´!" ë²„íŠ¼ì— ëŒ€í•œ ì‘ë‹µ ì²˜ë¦¬
        if "adhd_has_task" in user_response:
            # ë‹¤ìŒ ë‹¨ê³„: í•  ì¼ì´ ë¬´ì—‡ì¸ì§€ ë¬¼ì–´ë³´ê¸°
            question_text = await get_mention_from_db(
                mention_type="adhd_ask_task",
                language_code=payload.language_code,
                personality=payload.character_personality,
                format_kwargs={"user_nick_nm": (await get_user_info(payload.user_id))[0]}
            )
            return {
                "intervention": {
                    "preset_id": PresetIds.ADHD_AWAITING_TASK_DESCRIPTION,
                    "text": question_text,
                    "adhd_context": {"step": "awaiting_task_description"}
                }
            }
        else: # "adhd_no_task"
         # "ì—†ì–´!"ë¥¼ ëˆ„ë¥¸ ê²½ìš° -> í˜¸í¡ ë° ì§‘ì¤‘ë ¥ í›ˆë ¨ ì†”ë£¨ì…˜ ì œì•ˆ
            
            # 1. 'ì§‘ì¤‘ë ¥ í›ˆë ¨' ì†”ë£¨ì…˜ì„ DBì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
            focus_solution_query = supabase.table("solutions").select("solution_id, solution_type").eq("cluster", "adhd").eq("solution_variant", "focus_training").limit(1)
            focus_solution_res = await run_in_threadpool(focus_solution_query.execute)
            focus_solution_data = focus_solution_res.data[0] if focus_solution_res.data else {}

            # 2. 'í˜¸í¡' ì†”ë£¨ì…˜ - í”„ë¡ íŠ¸ì—”ë“œ ë¼ìš°íŒ…ì„ ìœ„í•´ì„œ!
            breathing_solution_data = {
            "solution_id": "breathing_default", 
            "solution_type": "breathing"
            }
            
              # 3. ì œì•ˆ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            proposal_text = await get_mention_from_db(
                "propose", 
                payload.language_code, 
                cluster="adhd", 
                personality=payload.character_personality
            )     

            # ì†”ë£¨ì…˜ ì œì•ˆ ì‹œì ì— session ìƒì„±
            intervention_for_db = { "preset_id": PresetIds.SOLUTION_PROPOSAL, "proposal_text": proposal_text}
            session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention_for_db, debug_log, {})
        

            return {
                "intervention": { "preset_id": PresetIds.SOLUTION_PROPOSAL, "proposal_text": proposal_text,
                "options": [
                    { "label": "í˜¸í¡í•˜ëŸ¬ ê°€ê¸°", "action": "accept_solution", "solution_id": breathing_solution_data.get("solution_id"), "solution_type": "breathing" },
                    { "label": "ì§‘ì¤‘ë ¥ í›ˆë ¨í•˜ê¸°", "action": "accept_solution", "solution_id": focus_solution_data.get("solution_id"), "solution_type": focus_solution_data.get("solution_type") },
                ],
                "session_id": session_id 
                }
            }

        
            # --- ì‹œë‚˜ë¦¬ì˜¤ 2: ì‚¬ìš©ìê°€ í•  ì¼ì„ ì…ë ¥í–ˆì„ ë•Œ ---
    elif current_step == "awaiting_task_description":
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í•  ì¼ ë‚´ìš©ì„ ë°›ì•„ ì²˜ë¦¬
        user_nick_nm, _ = await get_user_info(payload.user_id)
        
        # ì„±ê²©ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        prompt_template = get_adhd_breakdown_prompt(payload.character_personality)
        
        # ê°€ì ¸ì˜¨ í…œí”Œë¦¿ì— ë³€ìˆ˜ë¥¼ ì±„ì›Œ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.
        final_prompt = prompt_template.format(user_nick_nm=user_nick_nm, user_message=user_response)
        
        breakdown_result = await call_llm(
            system_prompt=final_prompt, # ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ system_promptë¡œ ì‚¬ìš©
            user_content="", # user_contentëŠ” ë¹„ì›Œë‘ê¸°
            openai_key=OPENAI_KEY, 
            expect_json=True
        )
        
        coaching_text = breakdown_result.get("coaching_text", "ì¢‹ì•„ìš”, í•¨ê»˜ ì‹œì‘í•´ë´ìš”!")
        mission_text = breakdown_result.get("mission_text", "ê°€ì¥ ì‘ì€ ì¼ë¶€í„° ì‹œì‘í•´ë³´ì„¸ìš”.")
        
         # ë½€ëª¨ë„ë¡œ ì†”ë£¨ì…˜ ì •ë³´ ì¡°íšŒ
        solution_query = supabase.table("solutions").select("solution_id, solution_type").eq("cluster", "adhd").eq("solution_variant", "pomodoro").limit(1)
        solution_res = await run_in_threadpool(solution_query.execute)
        solution_data = solution_res.data[0] if solution_res.data else {}

        # DBì— ì €ì¥í•  intervention ê°ì²´ ë¨¼ì € ìƒì„±
        intervention_for_db = { 
            "preset_id": PresetIds.ADHD_TASK_BREAKDOWN, 
            "coaching_text": coaching_text, 
            "mission_text": mission_text 
        }

        # ë½€ëª¨ë„ë¡œ ì œì•ˆ ì‹œì ì— session ìƒì„±
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention_for_db, debug_log, {})

        # intervention ê°ì²´ ì•ˆì— optionsì™€ session_idë¥¼ í¬í•¨ì‹œì¼œ í•œë²ˆì— ë°˜í™˜í•©ë‹ˆë‹¤.
        intervention_for_client = intervention_for_db.copy()
        intervention_for_client["options"] = [
            { 
                "label": "ë½€ëª¨ë„ë¡œì™€ í•¨ê»˜ ë¯¸ì…˜í•˜ëŸ¬ ê°€ê¸°", 
                "action": "accept_solution", 
                "solution_id": solution_data.get("solution_id"), 
                "solution_type": solution_data.get("solution_type") 
            }
        ]
        intervention_for_client["session_id"] = session_id

        return { "intervention": intervention_for_client }

# ---------- API Endpoints (ë¶„ë¦¬ëœ êµ¬ì¡°) ----------


# ======================================================================
# === /analyze ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ë¡œì§ (ë¶„ë¦¬ëœ í•¨ìˆ˜ë“¤) ===
# ======================================================================

async def _handle_moderation(text: str) -> bool:
    """OpenAI Moderation APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ í•´ ì½˜í…ì¸ ë¥¼ í™•ì¸í•˜ê³  ì°¨ë‹¨ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    is_flagged, categories = await moderate_text(text, OPENAI_KEY)
    if not is_flagged:
        return False

    allowed_categories = {'self-harm', 'self-harm/intent', 'self-harm/instructions', 'hate', 'harassment', 'violence'}
    should_block = any(cat not in allowed_categories and triggered for cat, triggered in categories.items())
    
    if should_block:
        print(f"ğŸš¨ [BLOCKED] Inappropriate content: '{text}', Categories: {categories}")
    else:
        print(f"âš ï¸ [PASSED] Delegating to internal safety check: '{text}', Categories: {categories}")
    
    return should_block

async def _handle_emoji_only_case(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """ì•„ì´ì½˜ë§Œ ì…ë ¥ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "EMOJI_REACTION"
    print("\n--- ğŸ§ EMOJI-ONLY ANALYSIS ---")

    selected_cluster = ICON_TO_CLUSTER.get(payload.icon.lower(), "neg_low")
    
    if selected_cluster == "neutral":
        intervention = {"preset_id": PresetIds.EMOJI_REACTION, "text": "ì˜¤ëŠ˜ì€ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”?", "top_cluster": "neutral"}
        session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
        return {"session_id": session_id, "intervention": intervention}
    
    assessment_scores = calculate_baseline_scores(payload.onboarding) # ì´ëª¨ì§€ë§Œ ìˆì„ ë• ì˜¨ë³´ë”© ì ìˆ˜ ì‚¬ìš©
    icon_scores = {c: 1.0 if c == selected_cluster else 0.0 for c in CLUSTERS}
   
    w = FINAL_FUSION_WEIGHTS_NO_TEXT
    fused_scores = {c: clip01(assessment_scores.get(c, 0.0) * w['assessment'] + icon_scores.get(c, 0.0) * w['icon']) for c in CLUSTERS}
    
    # ì ìˆ˜ ìƒí•œì„ (Cap) ì ìš©
    final_scores = fused_scores.copy()
    if selected_cluster in final_scores:
        final_scores[selected_cluster] = min(final_scores[selected_cluster], EMOJI_ONLY_SCORE_CAP)
   
    g, profile = g_score(final_scores), pick_profile(final_scores, None)
    
    user_nick_nm, _ = await get_user_info(payload.user_id)
    reaction_text = await get_mention_from_db("reaction", payload.language_code, personality=payload.character_personality, cluster=selected_cluster, user_nick_nm=user_nick_nm)
    intervention = {"preset_id": PresetIds.EMOJI_REACTION, "top_cluster": selected_cluster, "empathy_text": reaction_text}
    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)
    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}


async def _handle_friendly_mode(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ì¹œêµ¬ ëª¨ë“œ'ì¼ ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "FRIENDLY"
    print(f"\n--- ğŸ‘‹ FRIENDLY MODE: '{payload.text}' ---")

    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    system_prompt = get_system_prompt(
        mode='FRIENDLY', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )
    # ì´ì „ ëŒ€í™” ê¸°ì–µ: ì¹œêµ¬ ëª¨ë“œì—ì„œë„ ëŒ€í™” ê¸°ë¡ì„ user_contentì— í¬í•¨
    history_str = "\n".join([f"{h.sender}: {h.content}" for h in payload.history]) if payload.history else ""
    user_content = f"Previous conversation:\n{history_str}\n\nCurrent message: {payload.text}"

    llm_response = await call_llm(system_prompt, user_content, OPENAI_KEY, expect_json=False)

    # LLM í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì—ëŸ¬ì¸ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
    final_text = llm_response if not (isinstance(llm_response, dict) and 'error' in llm_response) else "ìŒ... ì§€ê¸ˆì€ ì ì‹œ ìƒê°í•  ì‹œê°„ì´ í•„ìš”í•´ìš”!ğŸ¥¹"
    intervention = {"preset_id": PresetIds.FRIENDLY_REPLY, "text": final_text}
    
    session_id = await save_analysis_to_supabase(payload, 0, 0.5, intervention, debug_log, {})
    return {"session_id": session_id, "intervention": intervention}


async def _run_analysis_pipeline(payload: AnalyzeRequest, debug_log: dict) -> dict:
    """Triage ê²°ê³¼ê°€ 'ë¶„ì„ ëª¨ë“œ'ì¼ ê²½ìš°ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    debug_log["mode"] = "ANALYSIS"
    print(f"\n--- ğŸ§ ANALYSIS MODE: '{payload.text}' ---")

     # 1. ì‚¬ìš©ìì˜ ìµœì‹  í‰ê°€ ì ìˆ˜(assessment_scores)ë¥¼ DBì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #    ì´ ì ìˆ˜ëŠ” ì˜¨ë³´ë”©ìœ¼ë¡œ ì‹œì‘í•´ì„œ, ì‹¬ì¸µ ë¶„ì„ì„ í•  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
    user_nick_nm, character_nm = await get_user_info(payload.user_id)
    
    profile_res = await run_in_threadpool(
        supabase.table("user_profiles")
        .select("latest_assessment_scores")
        .eq("id", payload.user_id).single().execute
    )
    
    assessment_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
    if not assessment_scores or not isinstance(assessment_scores, dict):
        # ìµœì‹  í‰ê°€ ì ìˆ˜ê°€ ì—†ìœ¼ë©´(ì˜ˆ: ì²« ì‚¬ìš©ì), ì˜¨ë³´ë”© ì ìˆ˜ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.
        print("âš ï¸ Latest assessment scores not found, using onboarding scores as baseline.")
        assessment_scores = calculate_baseline_scores(payload.onboarding)
    # assessment_scoresì— ìƒí•œì„ (Cap)ì„ ì ìš©
    for cluster in assessment_scores:
        assessment_scores[cluster] = min(assessment_scores.get(cluster, 0.0), ASSESSMENT_SCORE_CAP)

    # --------------------------------------------------------------------------
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ 
    system_prompt = get_system_prompt(
        mode='ANALYSIS', personality=payload.character_personality, language_code=payload.language_code,
        user_nick_nm=user_nick_nm, character_nm=character_nm
    )

   # ì´ì „ ëŒ€í™” ê¸°ì–µ: ë¶„ì„ ëª¨ë“œì—ì„œë„ LLM í˜¸ì¶œ ì‹œ historyë¥¼ í¬í•¨
    history_for_llm = [h.dict() for h in payload.history] if payload.history else []
    llm_payload = {"user_message": payload.text, "baseline_scores": assessment_scores, "history": history_for_llm}
   
    # 2. LLM í˜¸ì¶œ ë° 2ì°¨ ì•ˆì „ ì¥ì¹˜
    llm_json = await call_llm(system_prompt, json.dumps(llm_payload, ensure_ascii=False), OPENAI_KEY)
    debug_log["llm"] = llm_json

    is_crisis, crisis_scores = is_safety_text(payload.text, llm_json, debug_log)
    if is_crisis:
        print(f"ğŸš¨ 2nd Safety Check Triggered: '{payload.text}'")
        g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
        intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "cluster": top_cluster,"solution_id": f"{top_cluster}_crisis_01"}
        session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
        return {"session_id": session_id, "final_scores": crisis_scores, "g_score": g, "profile": profile, "intervention": intervention}


    # 3. ëª¨ë“  ì ìˆ˜ ê³„ì‚° ë° ìœµí•©
    text_scores = calculate_text_scores(payload.text, llm_json)
    
    has_icon = payload.icon and ICON_TO_CLUSTER.get(payload.icon.lower()) != "neutral"
    icon_scores = {c: 0.0 for c in CLUSTERS}
    if has_icon:
        icon_scores[ICON_TO_CLUSTER.get(payload.icon.lower())] = 1.0


    final_scores, weights_used = calculate_final_scores(text_scores, assessment_scores, icon_scores, has_icon)
    debug_log["scores"] = {"weights_used": weights_used, "assessment_base": assessment_scores, "text": text_scores, "icon": icon_scores, "final": final_scores}
    print(f"Scores -> Assessment Base: {_format_scores_for_print(assessment_scores)}, Text: {_format_scores_for_print(text_scores)}, Icon: {_format_scores_for_print(icon_scores)}")
    print(f"Weights: {_format_scores_for_print(weights_used)} -> Final Scores: {_format_scores_for_print(final_scores)}")


    # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
    g, profile = g_score(final_scores), pick_profile(final_scores, llm_json)
    top_cluster = max(final_scores, key=final_scores.get, default="neg_low")
    print(f"G-Score: {g:.2f}, Profile: {profile}")
    
    empathy_text = (llm_json or {}).get("empathy_response", "ë§ˆìŒì„ ì‚´í”¼ëŠ” ì¤‘ì´ì—ìš”...")
    score_val = final_scores[top_cluster]
    level = "high" if score_val > 0.7 else "mid" if score_val > 0.4 else "low"
    
    # 'analysis' íƒ€ì…ì˜ ë©˜íŠ¸ë¥¼ DBì—ì„œ ê°€ì ¸ì˜´
    analysis_text = await get_mention_from_db(
        "analysis", 
        payload.language_code, 
        personality=payload.character_personality, 
        cluster=top_cluster, 
        level=level, 
        format_kwargs={"emotion": CLUSTER_TO_DISPLAY_NAME.get(top_cluster),"user_nick_nm": user_nick_nm}
    )
    
    # intervention ê°ì²´ ìƒì„± ë° DB ì €ì¥
    intervention = {
        "preset_id": PresetIds.SOLUTION_PROPOSAL, 
        "top_cluster": top_cluster, 
        "empathy_text": empathy_text, 
        "analysis_text": analysis_text
    }

    session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, final_scores)
    return {"session_id": session_id, "final_scores": final_scores, "g_score": g, "profile": profile, "intervention": intervention}



# ======================================================================
# ===          ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
 
@app.post("/analyze")
async def analyze_emotion(payload: AnalyzeRequest):
    """ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ì™€ ì•„ì´ì½˜ì„ ë°›ì•„ ê°ì •ì„ ë¶„ì„í•˜ê³  ìŠ¤ì½”ì–´ë§ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    text = (payload.text or "").strip()
    debug_log: Dict[str, Any] = {"input": payload.dict()}

    try:
        # --- íŒŒì´í”„ë¼ì¸ 0: ìœ í•´ ì½˜í…ì¸  ê²€ì—´ ---
        if await _handle_moderation(text):
            return JSONResponse(status_code=400, content={"error": "Inappropriate content detected."})

        # ADHD ì»¨í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ë©´, ë‹¤ë¥¸ ëª¨ë“  ë¶„ì„ì„ ê±´ë„ˆë›°ê³  ADHD ë‹µë³€ ì²˜ë¦¬ ë¡œì§ìœ¼ë¡œ ë°”ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        if payload.adhd_context and "step" in payload.adhd_context:
            return await _handle_adhd_response(payload, debug_log)


        # --- íŒŒì´í”„ë¼ì¸ 1: ğŸŒ¸ CASE 2 - ì´ëª¨ì§€ë§Œ ìˆëŠ” ê²½ìš° ---
        if payload.icon and not text:
            debug_log["mode"] = "EMOJI_REACTION"
            return await _handle_emoji_only_case(payload, debug_log)

        # --- íŒŒì´í”„ë¼ì¸ 2: 1ì°¨ ì•ˆì „ ì¥ì¹˜ (LLM í˜¸ì¶œ ì „) ---
        is_crisis, crisis_scores = is_safety_text(text, None, debug_log)
        if is_crisis:
            print(f"ğŸš¨ 1st Safety Check Triggered: '{text}'")
            g, profile, top_cluster = g_score(crisis_scores), 1, "neg_low"
            intervention = {"preset_id": PresetIds.SAFETY_CRISIS_MODAL, "analysis_text": "ë§ì´ í˜ë“œì‹œêµ°ìš”. ì§€ê¸ˆ ë„ì›€ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”.", "cluster": top_cluster,"solution_id": f"{top_cluster}_crisis_01"}
            session_id = await save_analysis_to_supabase(payload, profile, g, intervention, debug_log, crisis_scores)
            return {"session_id": session_id, "intervention": intervention}

        # --- íŒŒì´í”„ë¼ì¸ 3: Triage (ì¹œêµ¬ ëª¨ë“œ / ë¶„ì„ ëª¨ë“œ ë¶„ê¸°) ---
        rule_scores, _, _ = rule_scoring(text)
        is_simple_text = len(text) < 10 and max(rule_scores.values() or [0.0]) < 0.1
        
        if is_simple_text:
            triage_mode = 'FRIENDLY'
            debug_log["triage_decision"] = "Rule-based: Simple text"
        else:
            triage_mode = await call_llm(TRIAGE_SYSTEM_PROMPT, text, OPENAI_KEY, expect_json=False)
            debug_log["triage_decision"] = f"LLM Triage: {triage_mode}"

        # --- íŒŒì´í”„ë¼ì¸ 4: Triage ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ ---
        if triage_mode == 'FRIENDLY':
            return await _handle_friendly_mode(payload, debug_log)
        else: # ANALYSIS
            analysis_result = await _run_analysis_pipeline(payload, debug_log)
            
            intervention = analysis_result.get("intervention", {})
            top_cluster = intervention.get("top_cluster")
            empathy_text = intervention.get("empathy_text", "")
            user_nick_nm, _ = await get_user_info(payload.user_id)
            
            
            # ë§Œì•½ ë¶„ì„ ê²°ê³¼ top_clusterê°€ ADHDë¼ë©´, ì†”ë£¨ì…˜ì„ ë°”ë¡œ ì œì•ˆí•˜ì§€ ì•Šê³  ì§ˆë¬¸ì„ ë˜ì§
            if top_cluster == "adhd":
                print("ğŸ§  ADHD cluster detected. Switching to pre-solution question flow.")
                
                question_text_template = await get_mention_from_db(
                    mention_type="adhd_question",
                    language_code=payload.language_code,
                    personality=payload.character_personality,
                    format_kwargs={"user_nick_nm": user_nick_nm}
                )

                final_question_text = f"{empathy_text} {question_text_template}"

                
                # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì§ˆë¬¸ê³¼ ë‹¤ìŒ ìš”ì²­ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
                analysis_result["intervention"] = {
                    "preset_id": PresetIds.ADHD_PRE_SOLUTION_QUESTION,
                    "text": final_question_text.strip(), # ìµœì¢… ì¡°í•©ëœ í…ìŠ¤íŠ¸
                    "options": [
                        {"label": "ìˆì–´! ë­ë¶€í„° í•˜ë©´ ì¢‹ì„ê¹Œ?", "action": "adhd_has_task"},
                        {"label": "ì—†ì–´! ì§‘ì¤‘ë ¥ í›ˆë ¨ í• ë˜", "action": "adhd_no_task"}
                    ],
                    "adhd_context": { "step": "awaiting_choice" }
                }

            return analysis_result

    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ UNHANDLED EXCEPTION in /analyze: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})

# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ (ë§ˆìŒ ì ê²€) ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    if not supabase: raise HTTPException(status_code=500, detail="Supabase client not initialized")
    try:
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score: raise HTTPException(status_code=400, detail=f"Invalid cluster: {payload.cluster}")
        
        normalized_score = clip01(total_score / max_score)
        
        profile_res = await run_in_threadpool(supabase.table("user_profiles").select("latest_assessment_scores").eq("id", payload.user_id).single().execute)
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}
        
        latest_scores[payload.cluster] = normalized_score
        
        await run_in_threadpool(supabase.table("user_profiles").update({"latest_assessment_scores": latest_scores}).eq("id", payload.user_id).execute)
        
        history_row = {"user_id": payload.user_id, "assessment_type": f"deep_dive_{payload.cluster}", "scores": {payload.cluster: normalized_score}, "raw_responses": payload.responses}
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)
        
        return {"message": "Assessment submitted successfully", "updated_scores": latest_scores}
    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})



# ======================================================================
# ===          ì†”ë£¨ì…˜ ì œì•ˆ ë° ìƒì„¸ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================
   

@app.post("/solutions/propose")
async def propose_solution(payload: SolutionRequest): 
    """
    ë¶„ì„ ê²°ê³¼(top_cluster)ì— ë§ëŠ” í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì œì•ˆí•  ì†”ë£¨ì…˜ íƒ€ì… ëª©ë¡ì„ ëª…í™•íˆ ì •ì˜í•˜ê³ , í•´ë‹¹ íƒ€ì…ì˜ ì†”ë£¨ì…˜ë§Œ ì°¾ì•„ 
    ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì˜µì…˜ ëª©ë¡ê³¼, ëŒ€í‘œ ì œì•ˆ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    neg_low, sleep: í˜¸í¡, ì˜ìƒ, í–‰ë™ë¯¸ì…˜
    neg_high, positive: í˜¸í¡, ì˜ìƒë§Œ
    adhdëŠ” í• ê±° ìˆëƒì—†ëƒ ë¬¼ì–´ë³´ê³  ìˆìœ¼ë©´ ë½€ëª¨ë„ë¡œ, ì—†ìœ¼ë©´ í˜¸í¡, ì˜ìƒ
    """    
    if not supabase: raise HTTPException(status_code=500, detail="Supabase client not initialized")
        
    try:
        user_nick_nm, _ = await get_user_info(payload.user_id)
        top_cluster = payload.top_cluster

         # 0. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì œì•ˆí•  ì†”ë£¨ì…˜ íƒ€ì… ëª©ë¡ì„ ì •ì˜í•´ì•¼í•¨
        solution_types_by_cluster = {
            "neg_low": ["breathing", "video", "action"],
            "sleep": ["breathing", "video", "action"],
            "neg_high": ["breathing", "video"],
            "positive": ["breathing", "video"],
            # ADHDëŠ” ë³„ë„ íë¦„ì„ íƒ€ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ë§Œ ì •ì˜
            "adhd": ["breathing", "video"] 
        }
        
        # í˜„ì¬ top_clusterì— í•´ë‹¹í•˜ëŠ” ì†”ë£¨ì…˜ íƒ€ì… ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        target_solution_types = solution_types_by_cluster.get(top_cluster, ["video"])


        # 1. ì‚¬ìš©ìì˜ ê±°ë¶€ íƒœê·¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        profile_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("negative_tags")
            .eq("id", payload.user_id)
            .single().execute
        )
        negative_tags = (profile_res.data or {}).get("negative_tags", [])

        # 2. ì œì•ˆí•  í›„ë³´ ì†”ë£¨ì…˜ ì „ì²´ë¥¼ DBì—ì„œ ê°€ì ¸ì˜¤ê¸°
        all_candidates_res = await run_in_threadpool(
            supabase.table("solutions")
            .select("*")
            .eq("cluster", top_cluster)
            .execute
        )
        all_candidates = all_candidates_res.data
        
        if not all_candidates:
            return {"proposal_text": "ì§€ê¸ˆì€ ì œì•ˆí•´ë“œë¦´ íŠ¹ë³„í•œ í™œë™ì´ ì—†ë„¤ìš”.", "options": []}

        # # 3. ê±°ë¶€ íƒœê·¸ê°€ í¬í•¨ëœ ì†”ë£¨ì…˜ì€ í›„ë³´ì—ì„œ ì œì™¸
        # if negative_tags:
        #     filtered_candidates = [
        #         sol for sol in all_candidates
        #         if not any(tag in (sol.get("tags") or []) for tag in negative_tags)
        #     ]
        # else:
        #     filtered_candidates = all_candidates

        # 3. í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ì†”ë£¨ì…˜ í•„í„°ë§(1/3 í™•ë¥ ë¡œ ë‚˜ì˜¤ë„ë¡!)
        probabilistically_filtered_candidates = []
        if negative_tags:
            for sol in all_candidates:
                solution_tags = set(sol.get("tags") or [])
                # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                if not solution_tags.isdisjoint(negative_tags):
                    # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ìˆë‹¤ë©´, 1/3 í™•ë¥ ë¡œë§Œ ëª©ë¡ì— ì¶”ê°€
                    if random.random() < (1/3):
                        probabilistically_filtered_candidates.append(sol)
                else:
                    # ê²¹ì¹˜ëŠ” íƒœê·¸ê°€ ì—†ë‹¤ë©´, ë¬´ì¡°ê±´ ëª©ë¡ì— ì¶”ê°€
                    probabilistically_filtered_candidates.append(sol)
        else:
            # negative_tagsê°€ ì—†ìœ¼ë©´ ëª¨ë“  í›„ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            probabilistically_filtered_candidates = all_candidates
        
        # í•„í„°ë§ í›„ í›„ë³´êµ°ì´ ì—†ìœ¼ë©´ ëª¨ë“  í›„ë³´ë¥¼ ë‹¤ì‹œ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
        if not probabilistically_filtered_candidates:
            probabilistically_filtered_candidates = all_candidates


        # 4. ê° ì†”ë£¨ì…˜ íƒ€ì…ë³„ë¡œ ëŒ€í‘œ ì†”ë£¨ì…˜ì„ í•˜ë‚˜ì”© ëœë¤ ì„ íƒ
        options = []
        labels = {"breathing": "í˜¸í¡í•˜ëŸ¬ ê°€ê¸°", "video": "ì˜ìƒ ë³´ëŸ¬ê°€ê¸°", "action": "ë¯¸ì…˜ í•˜ëŸ¬ê°€ê¸°"}
        
        # í…ìŠ¤íŠ¸ ì¡°í•©ì„ ìœ„í•´ ì²« ë²ˆì§¸ ì†”ë£¨ì…˜ì˜ ì„¤ëª…ì„ ì €ì¥í•  ë³€ìˆ˜
        first_solution_text = ""

        for sol_type in target_solution_types:

            # 'breathing' íƒ€ì…ì€ DB ì¡°íšŒ ì—†ì´ ê³ ì •ëœ ì˜µì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            if sol_type == 'breathing':
                options.append({
                    "label": labels.get(sol_type),
                    "action": "accept_solution",
                    "solution_id": "breathing_default",
                    "solution_type": "breathing"
                })
                continue

            # 'sleep' í´ëŸ¬ìŠ¤í„°ì˜ 'action' íƒ€ì…ì€ ìˆ˜ë©´ìœ„ìƒ íŒìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
            elif top_cluster == 'sleep' and sol_type == 'action':
                options.append({
                    "label": labels.get(sol_type), "action": "accept_solution",
                    "solution_id": "sleep_hygiene_tip_random", "solution_type": "action"
                })
                continue
            
            # ê·¸ ì™¸ ëª¨ë“  ê²½ìš°ëŠ” DBì—ì„œ ì†”ë£¨ì…˜ì„ ì°¾ìŠµë‹ˆë‹¤.
            type_candidates = [s for s in probabilistically_filtered_candidates if s.get("solution_type") == sol_type]
            if type_candidates:
                chosen_solution = random.choice(type_candidates)
                
                # 4-1. í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬í•  ë²„íŠ¼ ì˜µì…˜ ëª©ë¡
                options.append({
                    "label": labels.get(sol_type, "ì†”ë£¨ì…˜ ë³´ê¸°"),
                    "action": "accept_solution",
                    "solution_id": chosen_solution["solution_id"],
                    "solution_type": chosen_solution["solution_type"]
                })

                # 4-2. ì²« ë²ˆì§¸ë¡œ ì„ íƒëœ ì†”ë£¨ì…˜ì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì €ì¥ 
                if not first_solution_text:
                    first_solution_text = chosen_solution.get("text", "")

        if not options:
            return {"proposal_text": "ì§€ê¸ˆ ì œì•ˆí•´ë“œë¦´ ë§Œí•œ ë§ì¶¤ í™œë™ì´ ì—†ë„¤ìš”. ëŒ€í™”ë¥¼ ë” ë‚˜ëˆ ë³¼ê¹Œìš”?", "options": []}

        # 5. ì œì•ˆ ë©˜íŠ¸ì™€ ëŒ€í‘œ ì†”ë£¨ì…˜ ì„¤ëª…ì„ ì¡°í•©í•˜ì—¬ ìµœì¢… ì œì•ˆ í…ìŠ¤íŠ¸ ìƒì„±
        proposal_script = await get_mention_from_db(
            mention_type="propose",
            language_code=payload.language_code,
            cluster=top_cluster,
            user_nick_nm=user_nick_nm
        )
        final_text = f"{proposal_script} {first_solution_text}".strip()
      
        # 6. ë¡œê·¸ ì €ì¥ ë° ìµœì¢… ê²°ê³¼ ë°˜í™˜
        log_entry = {
            "session_id": payload.session_id, 
            "type": "propose", 
            "solution_id": f"multiple_options_{top_cluster}"
        }
        await run_in_threadpool(supabase.table("interventions_log").insert(log_entry).execute)

        return {"proposal_text": final_text, "options": options}

    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})

# ======================================================================
# ===          ì†”ë£¨ì…˜ ì˜ìƒ ì—”ë“œí¬ì¸íŠ¸         ===
# ======================================================================

    # SolutionPageì—ì„œ ì˜ìƒ ë¡œë“œ
    # í•˜ë“œì½”ë”©ëœ SOLUTION_DETAILS_LIBRARYë¥¼ DB ì¡°íšŒë¡œ ëŒ€ì²´í–ˆìŒ!!
@app.get("/solutions/{solution_id}")
async def get_solution_details(solution_id: str):
    print(f"RIN: âœ… ì†”ë£¨ì…˜ ìƒì„¸ ì •ë³´ ìš”ì²­ ë°›ìŒ: {solution_id}")
    
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")
    
    try:
        # solutions í…Œì´ë¸”ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¡°íšŒ
        response = await run_in_threadpool(
            supabase.table("solutions")
            .select("url, start_at, end_at, text") 
            .eq("solution_id", solution_id)
            .single()
            .execute
        )
        
        
        if not response.data:
            raise HTTPException(status_code=404, detail="Solution not found")

        # ìœ íŠœë¸Œ ë¶ˆëŸ¬ì˜¬ë•Œ startAt, endAt (camelCase)ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ í‚¤ë¥¼ ë³€í™˜í•´ì¤Œ
        # supabaseëŠ” snake_caseë¡œ ì €ì¥í•´ì•¼ í•œë‹¤ê³  í•¨.
        return {
            'url': response.data.get('url'), 
            'startAt': response.data.get('start_at'), 
            'endAt': response.data.get('end_at'),
            'text': response.data.get('text')
            }
        
    except Exception as e:
        print(f"RIN: âŒ í•´ë‹¹ ì†”ë£¨ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {solution_id}, ì—ëŸ¬: {e}")
        raise HTTPException(status_code=404, detail="Solution not found")
    

# ======================================================================
# ===          ìƒí™©ë³„ ëŒ€ì‚¬ ì œê³µ ì—”ë“œí¬ì¸íŠ¸ (`/dialogue/*`)         ===
# ======================================================================
@app.get("/dialogue/home")
async def get_home_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko',
    emotion: Optional[str] = None 
):
    """í™ˆ í™”ë©´ì— í‘œì‹œí•  ëŒ€ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if emotion:
        # ì´ëª¨ì§€ê°€ ì„ íƒëœ ê²½ìš°: 'reaction' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "reaction"
        cluster = ICON_TO_CLUSTER.get(emotion.lower(), "common")
    else:
        # ì´ëª¨ì§€ê°€ ì—†ëŠ” ì´ˆê¸° ìƒíƒœ: 'home' ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        mention_type = "home"
        cluster = "common"

    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        language_code=language_code,
        personality=personality,
        cluster=cluster,
        default_message=f"ì•ˆë…•, {user_nick_nm}! ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë•Œ?",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}
    
#  ì†”ë£¨ì…˜ ì™„ë£Œ í›„ í›„ì† ì§ˆë¬¸ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸  
@app.get("/dialogue/solution-followup")
async def get_solution_followup_dialogue(
    reason: str, # 'user_closed' ë˜ëŠ” 'video_ended'
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ì´ ëë‚œ í›„ì˜ ìƒí™©(reason)ê³¼ ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” í›„ì† ì§ˆë¬¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # ì´ìœ (reason)ì— ë”°ë¼ DBì—ì„œ ì¡°íšŒí•  mention_typeì„ ê²°ì •í•©ë‹ˆë‹¤.
    if reason == 'user_closed':
        mention_type = "followup_user_closed"
    else: # 'video_ended' ë˜ëŠ” ê¸°íƒ€
        mention_type = "followup_video_ended"

    # get_mention_from_db í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©˜íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    dialogue_text = await get_mention_from_db(
        mention_type=mention_type,
        personality=personality,
        language_code=language_code,
        cluster="common", 
        default_message="ì–´ë•Œìš”? ì¢€ ì¢‹ì•„ì§„ ê²ƒ ê°™ì•„ìš”?ğŸ˜Š",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí–ˆì„ ë•Œì˜ ë©˜íŠ¸ë¥¼ ì„±í–¥ë³„ë¡œ ì£¼ê¸° 
@app.get("/dialogue/decline-solution")
async def get_decline_solution_dialogue(
    personality: Optional[str] = None, 
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ì†”ë£¨ì…˜ ì œì•ˆì„ ê±°ì ˆí•˜ê³  ëŒ€í™”ë¥¼ ì´ì–´ê°€ê³  ì‹¶ì–´í•  ë•Œì˜ ë°˜ì‘ ë©˜íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    dialogue_text = await get_mention_from_db(
        mention_type="decline_solution",
        personality=personality,
        language_code=language_code,
        cluster="common",
        default_message="ì•Œê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ìš”. ì €ì—ê²Œ í¸ì•ˆí•˜ê²Œ í„¸ì–´ë†“ìœ¼ì„¸ìš”. ê·€ ê¸°ìš¸ì—¬ ë“£ê³  ìˆì„ê²Œìš”.",
        format_kwargs={"user_nick_nm": user_nick_nm}
    )
    
    return {"dialogue": dialogue_text}


# ======================================================================
# ===     ë¦¬í¬íŠ¸ ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================
class DailyReportRequest(BaseModel):
    user_id: str
    date: str # "YYYY-MM-DD" í˜•ì‹
    language_code: str = 'ko'


async def create_and_save_summary_for_user(user_id: str, date_str: str):
    """
    ê·¸ë‚ ì˜ 'ìµœê³ ì  ê°ì •'ê³¼ 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„ì˜ ê°ì •'ì„ ëª¨ë‘ ì°¾ì•„ LLMì— ì „ë‹¬í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìŠ¤ì¼€ì¤„ë§ëœ ì‘ì—…(/tasks/generate-summaries)ì— ì˜í•´ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    print(f"----- [Daily Summary Job Start] User: {user_id}, Date: {date_str} -----")
    
    # Supabase ë˜ëŠ” OpenAI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.
    if not supabase or not OPENAI_KEY:
        print("Error: Supabase or OpenAI key not set.")
        return

    try:
        start_of_day = f"{date_str}T00:00:00+00:00"
        end_of_day = f"{date_str}T23:59:59+00:00"

        # --- 1. 'ê·¸ë‚  ê°€ì¥ ë†’ì•˜ë˜ ë‹¨ì¼ ê°ì •' ì°¾ê¸° (ê¸°ì¤€ì  1) ---
        top_score_query = supabase.table("cluster_scores") \
            .select("cluster, score, sessions(summary)") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .order("score", desc=True) \
            .limit(1)
        top_score_res = await run_in_threadpool(top_score_query.execute)

        if not top_score_res.data:
            print(f"Info: No cluster scores for user {user_id} on {date_str}. Skipping.")
            return

        top_score_entry = top_score_res.data[0]
        headline_cluster = top_score_entry['cluster']
        headline_score = int(top_score_entry['score'] * 100)
        headline_summary = (top_score_entry.get('sessions') or {}).get('summary', "íŠ¹ë³„í•œ ëŒ€í™”ëŠ” ì—†ì—ˆì–´ìš”.")

        # --- 2. 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„(g_score ìµœê³ ì )ì˜ ê°ì •' ì°¾ê¸° (ê¸°ì¤€ì  2) ---
        top_g_score_session_query = supabase.table("sessions") \
            .select("id, summary, g_score, cluster_scores(cluster, score)") \
            .eq("user_id", user_id) \
            .gte("created_at", start_of_day) \
            .lte("created_at", end_of_day) \
            .order("g_score", desc=True) \
            .limit(1)
        top_g_score_res = await run_in_threadpool(top_g_score_session_query.execute)
        
        difficult_moment_context = None
        if top_g_score_res.data:
            top_g_score_session = top_g_score_res.data[0]
            if top_g_score_session.get('cluster_scores'):
                # í•´ë‹¹ ì„¸ì…˜ ë‚´ì—ì„œ ê°€ì¥ ë†’ì•˜ë˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                top_cluster_in_g_session = max(top_g_score_session['cluster_scores'], key=lambda x: x['score'])
                
                # 'ìµœê³ ì  ê°ì •'ê³¼ 'ê°€ì¥ í˜ë“¤ì—ˆë˜ ìˆœê°„ì˜ ê°ì •'ì´ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ì¶”ê°€ ì •ë³´ êµ¬ì„±
                if top_cluster_in_g_session['cluster'] != headline_cluster:
                    difficult_moment_context = {
                        "cluster_name": CLUSTER_TO_DISPLAY_NAME.get(top_cluster_in_g_session['cluster']),
                        "score": int(top_cluster_in_g_session['score'] * 100),
                        "reason": "ì´ ê°ì •ì€ í•˜ë£¨ ì¤‘ ê°€ì¥ í˜ë“¤ì—ˆë˜(ì¢…í•© ì ìˆ˜ê°€ ë†’ì•˜ë˜) ìˆœê°„ì˜ ì£¼ìš” ê°ì •ì…ë‹ˆë‹¤."
                    }

        # --- 3. LLMì— ì „ë‹¬í•  ì •ë³´ êµ¬ì„± ---
        user_nick_nm, _ = await get_user_info(user_id)
        llm_context = {
            "user_nick_nm": user_nick_nm,
            "headline_emotion": {
                "cluster_name": CLUSTER_TO_DISPLAY_NAME.get(headline_cluster),
                "score": headline_score,
                "dialogue_summary": headline_summary
            },
            "difficult_moment": difficult_moment_context # Noneì¼ ìˆ˜ë„ ìˆìŒ
        }
        
        recent_summaries_query = supabase.table("daily_summaries").select("summary_text").eq("user_id", user_id).order("date", desc=True).limit(5)
        recent_summaries_res = await run_in_threadpool(recent_summaries_query.execute)
        llm_context["previous_summaries"] = [s['summary_text'] for s in recent_summaries_res.data]

        # --- LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ë¬¸ ìƒì„± ---
        summary_json = await call_llm(
            system_prompt=REPORT_SUMMARY_PROMPT,
            user_content=json.dumps(llm_context, ensure_ascii=False),
            openai_key=OPENAI_KEY
        )
        
        daily_summary_text = summary_json.get("daily_summary")
        if not daily_summary_text:
            print(f"Warning: LLM failed to generate summary for user {user_id} on {date_str}.")
            return


        # --- 8. ìƒì„±ëœ ìš”ì•½ë¬¸ì„ `daily_summaries` í…Œì´ë¸”ì— ì €ì¥ (Upsert) ---
        summary_data = {
            "user_id": user_id,
            "date": date_str,
            "summary_text": daily_summary_text,
            "top_cluster": headline_cluster,
            "top_score": headline_score 
        }
        await run_in_threadpool(supabase.table("daily_summaries").upsert(summary_data, on_conflict="user_id,date").execute)

        # upsert: user_idì™€ dateê°€ ë™ì¼í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        upsert_query = supabase.table("daily_summaries").upsert(summary_data, on_conflict="user_id,date")
        await run_in_threadpool(upsert_query.execute)
        
        print(f"Success: Saved daily summary for user {user_id} on {date_str}.")

    except Exception as e:
                print(f"Error in create_and_save_summary_for_user: {e}"); traceback.print_exc()
    finally:
        print(f"----- [Job End] User: {user_id}, Date: {date_str} -----")


# 2ì£¼ ì°¨íŠ¸ ìš”ì•½ ìƒì„± í•¨ìˆ˜
async def create_and_save_weekly_summary_for_user(user_id: str, date_str: str):
    print(f"----- [Weekly Summary Job Start] User: {user_id}, Date: {date_str} -----")
    if not supabase or not OPENAI_KEY: return

    try:
        # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ìš”ì¼ í™•ì¸
        today_dt = dt.datetime.strptime(date_str, '%Y-%m-%d')
        # (ì›”ìš”ì¼=0, í™”ìš”ì¼=1, ..., ì¼ìš”ì¼=6)
        is_sunday = today_dt.weekday() == 6

        # ìš”ì¼ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if is_sunday:
            system_prompt = WEEKLY_REPORT_SUMMARY_PROMPT_NEURO
            print(f"    Info: ì¼ìš”ì¼ì´ë¯€ë¡œ 'ë‡Œê³¼í•™ ë¦¬í¬íŠ¸'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        else:
            system_prompt = WEEKLY_REPORT_SUMMARY_PROMPT_STANDARD
            print(f"    Info: ì¼ë°˜ 2ì£¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            
        today = dt.datetime.strptime(date_str, '%Y-%m-%d').replace(tzinfo=dt.timezone.utc)
        start_date = today - dt.timedelta(days=13)
        end_date = today + dt.timedelta(days=1)

        # 14ì¼ê°„ì˜ ì„¸ì…˜ ë° í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ë°ì´í„° í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
        sessions_res = supabase.table("sessions").select("id, created_at, g_score").eq("user_id", user_id).gte("created_at", start_date.isoformat()).lt("created_at", end_date.isoformat()).execute()
        if not sessions_res.data:
            print(f"Info: No session data found for weekly summary for user {user_id}. Skipping.")
            return # ë°ì´í„° ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
        
        # ê¸°ë¡ì´ ìˆëŠ” ë‚ ì§œ ìˆ˜ ê³„ì‚°
        recorded_days = set()
        for session in sessions_res.data:
            try:
                # íƒ€ì„ì¡´ ì •ë³´ ì œê±°í•˜ê³  ë‚ ì§œë§Œ ì¶”ì¶œ
                day_str = dt.datetime.fromisoformat(session['created_at'].split('+')[0]).strftime('%Y-%m-%d')
                recorded_days.add(day_str)
            except Exception as e:
                print(f"Warning: Could not parse date {session['created_at']} for user {user_id}. Error: {e}")
                continue # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ì„¸ì…˜ ê±´ë„ˆë›°ê¸°

        MIN_DAYS_REQUIRED = 3 # ìµœì†Œ í•„ìš” ì¼ìˆ˜
        if len(recorded_days) < MIN_DAYS_REQUIRED:
            print(f"Info: Insufficient data ({len(recorded_days)} days found, requires {MIN_DAYS_REQUIRED}) for weekly summary for user {user_id}. Skipping.")
            # ë°ì´í„° ë¶€ì¡± ì‹œ, DBì— placeholder ì €ì¥í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ ì¢…ë£Œ
            return
        # [ìˆ˜ì • ë] ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰



        
        session_ids = [s['id'] for s in sessions_res.data]
        scores_res = supabase.table("cluster_scores").select("session_id, created_at, cluster, score").in_("session_id", session_ids).execute()
        
        sessions_with_scores = []
        scores_by_session_id = {sid: [] for sid in session_ids}
        for score in scores_res.data:
            scores_by_session_id.setdefault(score['session_id'], []).append(score)

        for session in sessions_res.data:
            session['cluster_scores'] = scores_by_session_id.get(session['id'], [])
            sessions_with_scores.append(session)

        if not session:
            print(f"Info: No data for weekly summary for user {user_id}. Skipping.")
            return


 # --- ë°ì´í„° ê°€ê³µ: íŠ¸ë Œë“œ ë¶„ì„ ë¡œì§ ì‹œì‘ ---

        # 1. ì¼ë³„ ë°ì´í„° êµ¬ì¡°í™”
        daily_data = {}
        # 14ì¼ê°„ì˜ ëª¨ë“  ë‚ ì§œ í‚¤ë¥¼ ë¯¸ë¦¬ ìƒì„±
        for i in range(14):
            day_key = (start_date + dt.timedelta(days=i)).strftime('%Y-%m-%d')
            daily_data[day_key] = {'g_scores': [], 'clusters': {c: [] for c in CLUSTERS}}
        for session in sessions_with_scores:
            created_at_str = session['created_at'].split('+')[0]
            try: day = dt.datetime.strptime(created_at_str, "%Y-%m-%dT%H:%M:%S.%f").strftime('%Y-%m-%d')
            except ValueError: day = dt.datetime.strptime(created_at_str, "%Y-%m-%dT%H:%M:%S").strftime('%Y-%m-%d')
            if day in daily_data:
                if session['g_score'] is not None: daily_data[day]['g_scores'].append(session['g_score'])
                for score_data in session.get('cluster_scores', []):
                    if score_data['cluster'] in daily_data[day]['clusters']: daily_data[day]['clusters'][score_data['cluster']].append(score_data['score'])
       
        # 2. í†µê³„ ì§€í‘œ ê³„ì‚°
        g_scores = [np.mean(day['g_scores']) for day in daily_data.values() if day['g_scores']]
        
        cluster_stats = {}
        all_scores = []
        for c in CLUSTERS:
            # í•˜ë£¨ì— ì—¬ëŸ¬ ê¸°ë¡ì´ ìˆìœ¼ë©´ í‰ê· ì„ ë‚´ì–´ ê·¸ë‚ ì˜ ëŒ€í‘œ ì ìˆ˜ë¡œ ì‚¬ìš©
            daily_avgs = [np.mean(day['clusters'][c]) for day in daily_data.values() if day['clusters'][c]]
            
            if not daily_avgs: cluster_stats[c] = {"avg": 0, "std": 0, "trend": "stable"}; continue
            all_scores.extend([(c, s) for s in daily_avgs])
           
            # ì¶”ì„¸ ë¶„ì„ (ê°„ë‹¨í•œ ê¸°ìš¸ê¸° ê³„ì‚°)
            x = np.arange(len(daily_avgs)); slope = np.polyfit(x, daily_avgs, 1)[0] if len(daily_avgs) > 1 else 0
            # í•˜ë£¨ í‰ê·  5ì ì”© ì ìˆ˜ê°€ ìƒìŠ¹/í•˜ë½ í•˜ëŠ” ì¶”ì„¸ì¼ ë•Œ í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë³€í™”ë¡œ ì¹¨
            trend = "increasing" if slope > 0.05 else "decreasing" if slope < -0.05 else "stable"

            cluster_stats[c] = {
                "avg": int(np.mean(daily_avgs) * 100), 
                "std": int(np.std(daily_avgs) * 100), 
                "trend": trend
                }

        # 3. ì£¼ìš” í´ëŸ¬ìŠ¤í„° ë° ìƒê´€ê´€ê³„ ë¶„ì„

        # ìƒê´€ê´€ê³„ ë¶„ì„ ë¡œì§ (ëª¨ë“  í´ëŸ¬ìŠ¤í„° ëŒ€ìƒ)
        correlations = []
            #  "í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ 2ì£¼ í‰ê·  ì ìˆ˜ê°€ 'ë‚®ìŒ' ìˆ˜ì¤€ì„ ë„˜ì–´, 'ì¤‘ê°„' ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ê¾¸ì¤€íˆ ë‚˜íƒ€ë‚¬ë‹¤"
        
        # [ê¸ì •ì  ìƒê´€ê´€ê³„: Aê°€ ë†’ì„ ë•Œ Bë„ ë†’ìŒ]
        if cluster_stats['sleep']['avg'] > 40 and cluster_stats['neg_low']['avg'] > 40:
            correlations.append("ìˆ˜ë©´ì˜ ì§ˆ ì €í•˜ì™€ ìš°ìš¸/ë¬´ê¸°ë ¥ê°ì´ í•¨ê»˜ ë†’ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì‹¬ë¦¬ì  ì—ë„ˆì§€ë¥¼ ì†Œëª¨ì‹œí‚¤ëŠ” ìš”ì¸ì´ ë  ìˆ˜ ìˆì–´, ë‘ ê°ì •ì˜ ê´€ê³„ë¥¼ í•¨ê»˜ ì‚´í´ë³´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if cluster_stats['neg_high']['avg'] > 40 and cluster_stats['sleep']['avg'] > 40:
            correlations.append("ë¶ˆì•ˆ/ê¸´ì¥ê°ì´ ë†’ì€ ë‚ , ìˆ˜ë©´ ë¬¸ì œë„ í•¨ê»˜ ì¦ê°€í•˜ëŠ” íŒ¨í„´ì´ ë³´ì…ë‹ˆë‹¤. ê³¼ë„í•œ ê°ì„± ìƒíƒœê°€ í¸ì•ˆí•œ íœ´ì‹ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë‹ˆ, ë¶ˆì•ˆ/ê¸´ì¥ê³¼ ìˆ˜ë©´ì˜ ì—°ê´€ì„±ì„ ëŒì•„ë³´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if cluster_stats['adhd']['avg'] > 50 and cluster_stats['neg_high']['avg'] > 50:
            correlations.append("ì§‘ì¤‘ë ¥ ì €í•˜ ë¬¸ì œì™€ ë¶ˆì•ˆê°ì´ ëª¨ë‘ ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì£¼ì˜ë¥¼ í†µì œí•˜ë ¤ëŠ” ë…¸ë ¥ì´ ê³¼ë„í•œ ì •ì‹ ì  ê¸´ì¥ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” íŒ¨í„´ì´ ê´€ì°°ë©ë‹ˆë‹¤. ì§‘ì¤‘ë ¥ê³¼ ë¶ˆì•ˆê° ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # [ë¶€ì •ì /ë°˜ë¹„ë¡€ ìƒê´€ê´€ê³„: Aê°€ ë†’ì„ ë•Œ BëŠ” ë‚®ìŒ]
        if cluster_stats['neg_low']['avg'] > 50 and cluster_stats['positive']['avg'] < 30:
            correlations.append("ìš°ìš¸/ë¬´ê¸°ë ¥ê°ì´ ë†’ì€ ì‹œê¸°ì—ëŠ” ê¸ì •ì  ê°ì •ì„ ëŠë¼ëŠ” ì •ë„ê°€ í˜„ì €íˆ ë‚®ì•„ì§€ëŠ” íŒ¨í„´ì´ ëšœë ·í•©ë‹ˆë‹¤. ì´ëŠ” ê°ì • íšŒë³µì„ ìœ„í•œ ì¸ì§€ì  ìì›ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì‹ í˜¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        if cluster_stats['neg_high']['avg'] > 50 and cluster_stats['positive']['avg'] < 30:
            correlations.append("ë¶ˆì•ˆ/ë¶„ë…¸ ê°ì •ì´ ë†’ì•„ì§ˆ ë•Œ, í‰ì˜¨/íšŒë³µ ì ìˆ˜ëŠ” ë°˜ëŒ€ë¡œ ë‚®ì•„ì§€ëŠ” ê²½í–¥ì´ ê´€ì°°ë©ë‹ˆë‹¤. ì´ ë‘ ê°ì • ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³´ë©° ì •ì„œì  ì•ˆì •ì„±ì„ ìœ„í•œ ìì‹ ë§Œì˜ ë°©ë²•ì„ ì°¾ì•„ë³´ëŠ” ê²ƒë„ ì¢‹ê² ìŠµë‹ˆë‹¤.")

        # [ì¶”ì„¸ ê¸°ë°˜ ë°˜ë¹„ë¡€ ìƒê´€ê´€ê³„: Aê°€ ê°œì„ ë  ë•Œ Bë„ ê°œì„ ë¨]
        if cluster_stats['sleep']['trend'] == 'decreasing' and cluster_stats['neg_low']['trend'] == 'decreasing':
            correlations.append("ë§¤ìš° ê¸ì •ì ì¸ ì‹ í˜¸ì…ë‹ˆë‹¤! ìµœê·¼ 2ì£¼ê°„ ìˆ˜ë©´ì˜ ì§ˆì´ ê°œì„ ë˜ë©´ì„œ, ìš°ìš¸/ë¬´ê¸°ë ¥ê° ë˜í•œ í•¨ê»˜ ê°ì†Œí•˜ëŠ” ì„ ìˆœí™˜ì´ ë§Œë“¤ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        if cluster_stats['neg_low']['trend'] == 'decreasing' and cluster_stats['positive']['trend'] == 'increasing':
            correlations.append("íšŒë³µíƒ„ë ¥ì„±ì´ ê°•í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ìš¸ê°ì´ ì ì°¨ ì¤„ì–´ë“¤ë©´ì„œ ê·¸ ìë¦¬ë¥¼ ê¸ì •ì ì´ê³  í‰ì˜¨í•œ ê°ì •ì´ ì±„ì›Œë‚˜ê°€ê³  ìˆëŠ” ëª¨ìŠµì´ ì¸ìƒì ì…ë‹ˆë‹¤.")

        # 4. ì£¼ìš” í´ëŸ¬ìŠ¤í„° ì‹ë³„
        # ì§€ë‚œ 2ì£¼ê°„ ë°œìƒí•œ ëª¨ë“  ê°ì • ê¸°ë¡ ì¤‘ì—ì„œ, ì ìˆ˜ê°€ ê°€ì¥ ë†’ì•˜ë˜ ìˆœê°„ Top 2ë¥¼ ì°¾ì•„ë‚´ë¼
        dominant_clusters_keys = list(set([item[0] for item in sorted(all_scores, key=lambda item: item[1], reverse=True)[:2]]))        
        # í´ëŸ¬ìŠ¤í„° ì´ë¦„ ë³€í™˜
        dominant_clusters_display = [CLUSTER_TO_DISPLAY_NAME.get(c, c) for c in dominant_clusters_keys]

        # ìµœì¢… LLM ì „ë‹¬ ë°ì´í„° êµ¬ì¡°
        trend_data = {
            "g_score_stats": {"avg": int(np.mean(g_scores)*100) if g_scores else 0, 
                              "std": int(np.std(g_scores)*100) if g_scores else 0}, 
            "cluster_stats": cluster_stats, 
            "dominant_clusters": dominant_clusters_display, 
            "correlations": correlations
            }

        # 5. LLM í˜¸ì¶œ ë° ê²°ê³¼ ì €ì¥
        # ë¶„ì„í•œ íŠ¸ë Œë“œ llmì— ë„£ê¸°
        user_nick_nm, _ = await get_user_info(user_id)
        llm_context = { "user_nick_nm": user_nick_nm, "trend_data": trend_data }
        summary_json = await call_llm(system_prompt, json.dumps(llm_context, ensure_ascii=False), OPENAI_KEY)

        if not summary_json or "error" in summary_json:
            print(f"Warning: LLM failed to generate weekly summary for user {user_id}.")
            return
            
        summary_data = { "user_id": user_id, "summary_date": date_str, **summary_json }
        await run_in_threadpool(supabase.table("weekly_summaries").upsert(summary_data, on_conflict="user_id,summary_date").execute)
        print(f"Success: Saved weekly summary for user {user_id} on {date_str}.")
    except Exception as e:
        print(f"Error in create_and_save_weekly_summary_for_user: {e}"); traceback.print_exc()




#  ------- daily_summaries í…Œì´ë¸”ì—ì„œ ìš”ì•½ë¬¸ ê°„ë‹¨íˆ ì¡°íšŒ ---------
# ëª¨ì§€ ë‹¬ë ¥ì—ì„œ íŠ¹ì • ë‚ ì§œë¥¼ íƒ­í–ˆì„ ë•Œ, í•´ë‹¹ ë‚ ì§œì˜ 'ì¼ì¼ ìš”ì•½ë¬¸' í•˜ë‚˜ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ëŠ” ì—­í• 
@app.post("/report/summary")
async def get_daily_report_summary(request: DailyReportRequest):
    """ë¯¸ë¦¬ ìƒì„±ëœ ì¼ì¼ ìš”ì•½ë¬¸ì„ DBì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        query = supabase.table("daily_summaries").select("summary_text") \
            .eq("user_id", request.user_id) \
            .eq("date", request.date) \
            .limit(1)
            
        response = await run_in_threadpool(query.execute)

        if response.data:
            summary = response.data[0].get("summary_text", "ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"summary": summary}
        else:
            return {"summary": "í•´ë‹¹ ë‚ ì§œì˜ ìš”ì•½ ê¸°ë¡ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ì–´ìš”."}

    except Exception as e:
        print(f"ğŸ”¥ EXCEPTION in /report/summary (read): {e}")
        raise HTTPException(status_code=500, detail="ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
# --- 2ì£¼ ì°¨íŠ¸ ìš”ì•½ë¬¸ì„ í”„ë¡ íŠ¸ì—”ë“œì— ì œê³µí•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ ---
# ëª¨ì§€ ì°¨íŠ¸ í˜ì´ì§€ì— ë“¤ì–´ê°”ì„ ë•Œ, '2ì£¼ ë¶„ì„ ë¦¬í¬íŠ¸' ì „ì²´(ì¢…í•©, í´ëŸ¬ìŠ¤í„°ë³„)ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì—­í• 
class WeeklyReportRequest(BaseModel):
    user_id: str

@app.post("/report/weekly-summary")
async def get_weekly_report_summary(request: WeeklyReportRequest):
    if not supabase: raise HTTPException(500, "Supabase client not initialized")
    
    # ê¸°ë³¸ í”Œë ˆì´ìŠ¤í™€ë” ë©”ì‹œì§€ ì •ì˜
    placeholder_no_data = "ì•„ì§ 2ì£¼ ë¦¬í¬íŠ¸ë¥¼ ë§Œë“¤ê¸°ì— ê¸°ë¡ì´ ì¡°ê¸ˆ ë¶€ì¡±í•´ìš”. 3ì¼ ì´ìƒ ê¾¸ì¤€íˆ ê¸°ë¡í•´ì£¼ì‹œë©´ ë” ìì„¸í•œ ë¦¬í¬íŠ¸ë¥¼ ë°›ì•„ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”!"
    placeholder_error = "ë¦¬í¬íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    try:
        # Supabase ì¿¼ë¦¬ ê°ì²´ ìƒì„±
        query = (
            supabase.table("weekly_summaries")
            .select("*")
            .eq("user_id", request.user_id)
            .order("summary_date", desc=True)
            .limit(1)
            .maybe_single()
        )

        # â­ [ìˆ˜ì •] query ê°ì²´ì˜ execute ë©”ì„œë“œ ìì²´ë¥¼ ì „ë‹¬ (ê´„í˜¸ ì—†ìŒ!)
        response = await run_in_threadpool(query.execute)

        # â­ response.dataê°€ Noneì´ ì•„ë‹ˆê³ , ë‚´ìš©ì´ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
        if response and response.data and response.data.get("overall_summary"):
            print(f"âœ… Found weekly summary for user {request.user_id}")
            return response.data # ì •ìƒ ë°ì´í„° ë°˜í™˜
        else:
            print(f"âš ï¸ No weekly summary data found for user {request.user_id}. Returning placeholder.")
            return {
                "overall_summary": placeholder_no_data,
                "neg_low_summary": placeholder_no_data,
                "neg_high_summary": placeholder_no_data,
                "adhd_summary": placeholder_no_data,
                "sleep_summary": placeholder_no_data,
                "positive_summary": placeholder_no_data
            }

    except Exception as e:
        print(f"ğŸ”¥ EXCEPTION in /report/weekly-summary: {e}")
        traceback.print_exc()
        return {
            "overall_summary": placeholder_error,
            "neg_low_summary": placeholder_error,
            "neg_high_summary": placeholder_error,
            "adhd_summary": placeholder_error,
            "sleep_summary": placeholder_error,
            "positive_summary": placeholder_error
        }
    
# ======================================================================
# ===     ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ë§ ì‘ì—…ìš© ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/tasks/generate-summaries")
async def handle_generate_summaries_task():
    """
    Supabase Cron Jobì— ì˜í•´ í˜¸ì¶œë  ì—”ë“œí¬ì¸íŠ¸.
    ì–´ì œ í™œë™í•œ ëª¨ë“  ì‚¬ìš©ìì˜ ì¼ì¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # ì–´ì œ ë‚ ì§œ ê³„ì‚° (UTC ê¸°ì¤€)
    yesterday = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=1)
    yesterday_str = yesterday.strftime('%Y-%m-%d')
    
    start_of_yesterday = f"{yesterday_str}T00:00:00+00:00"
    end_of_yesterday = f"{yesterday_str}T23:59:59+00:00"

    print(f"Starting daily summary generation task for date: {yesterday_str}")

    # ì–´ì œ í™œë™í•œ ìœ ì € ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
    active_users_query = supabase.table("sessions").select("user_id", count='exact') \
        .gte("created_at", start_of_yesterday) \
        .lte("created_at", end_of_yesterday)
    
    active_users_res = await run_in_threadpool(active_users_query.execute)
    # ì–´ì œ ì•±ì„ ì‚¬ìš©í•œ ìœ ì €ê°€ ë‹¨ í•œ ëª…ë„ ì—†ë‹¤ë©´, ì¦‰ì‹œ "ì–´ì œ í™œë™í•œ ìœ ì € ì—†ìŒ" ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì‘ì—…ì„ ì¢…ë£Œ
    if not active_users_res.data:
        message = "No active users yesterday. Task finished."
        print(message)
        return {"message": message}

    # í™œë™ ìœ ì €ê°€ ìˆì„ ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰
    user_ids = list(set([item['user_id'] for item in active_users_res.data]))
    
    print(f"Found {len(user_ids)} active users. Starting summary generation for each user...")

    # ê° ìœ ì €ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì•½ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    for user_id in user_ids:
        await create_and_save_summary_for_user(user_id, yesterday_str)
        await create_and_save_weekly_summary_for_user(user_id, yesterday_str)

    message = f"Summary generation task complete for {len(user_ids)} users."
    print(message)
    return {"message": message}


# ======================================================================
# ===     ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ ì œì¶œ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/assessment/submit")
async def submit_assessment(payload: AssessmentSubmitRequest):
    """ì£¼ê¸°ì  ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ì•„ DBì— ì €ì¥í•˜ê³ , ì‚¬ìš©ìì˜ ìµœì‹  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        # 1. ì œì¶œëœ ë‹µë³€ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° ë° ì •ê·œí™”
        total_score = sum(payload.responses.values())
        max_score = DEEP_DIVE_MAX_SCORES.get(payload.cluster)
        if not max_score:
            raise HTTPException(status_code=400, detail=f"Invalid cluster: {payload.cluster}")
        
        normalized_score = clip01(total_score / max_score)

        # 2. user_profiles í…Œì´ë¸”ì—ì„œ í˜„ì¬ ìµœì‹  ì ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
        profile_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .select("latest_assessment_scores")
            .eq("id", payload.user_id).single().execute
        )
        
        latest_scores = (profile_res.data or {}).get("latest_assessment_scores", {})
        if not isinstance(latest_scores, dict): latest_scores = {}


        # 3. ì´ë²ˆì— í‰ê°€í•œ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ë§Œ ì—…ë°ì´íŠ¸
        latest_scores[payload.cluster] = normalized_score
        
        # 4. ì—…ë°ì´íŠ¸ëœ ì ìˆ˜ë¥¼ ë‹¤ì‹œ user_profiles í…Œì´ë¸”ì— ì €ì¥
        update_res = await run_in_threadpool(
            supabase.table("user_profiles")
            .update({"latest_assessment_scores": latest_scores})
            .eq("id", payload.user_id).execute
        )

        # assessment_history í…Œì´ë¸”ì— ì›ë³¸ ê¸°ë¡ ì €ì¥ (ì¶”í›„ ìƒì„¸ ë¶„ì„ìš©)
        history_row = {
            "user_id": payload.user_id,
            "assessment_type": f"deep_dive_{payload.cluster}",
            "scores": {payload.cluster: normalized_score},
            "raw_responses": payload.responses,
        }
        await run_in_threadpool(supabase.table("assessment_history").insert(history_row).execute)

        return {"message": "Assessment submitted successfully", "updated_scores": latest_scores}

    except Exception as e:
        tb = traceback.format_exc()
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    


# ======================================================================
# ===     ìˆ˜ë©´ìœ„ìƒ íŒ ì œê³µ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.get("/dialogue/sleep-tip")
async def get_sleep_tip(
    personality: Optional[str] = None,
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ìºë¦­í„° ì„±í–¥ì— ë§ëŠ” ìˆ˜ë©´ìœ„ìƒ íŒì„ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # get_mention_from_db ëŒ€ì‹  ì§ì ‘ ì¿¼ë¦¬ (ë³„ë„ í…Œì´ë¸”ì´ë¯€ë¡œ)
    if not supabase:
        return {"tip": "ê·œì¹™ì ì¸ ìˆ˜ë©´ ìŠµê´€ì„ ê°€ì ¸ë³´ì„¸ìš”."}
    try:
        query = supabase.table("sleep_hygiene_tips").select("text").eq("language_code", language_code)
        if personality:
            query = query.eq("personality", personality)
        
        # SQLì˜ ORDER BY random() LIMIT 1ê³¼ ìœ ì‚¬í•œ íš¨ê³¼
        response = await run_in_threadpool(query.execute)
        tips = [row['text'] for row in response.data]
        
        if not tips:
            # í•´ë‹¹ ì„±ê²©ì˜ íŒì´ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒ ë°˜í™˜
            fallback_res = await run_in_threadpool(supabase.table("sleep_hygiene_tips").select("text").eq("personality", "prob_solver").execute)
            tips = [row['text'] for row in fallback_res.data]

        selected_tip = random.choice(tips) if tips else "ìˆ˜ë©´ ìœ„ìƒë²•ì„ ì°¸ê³ í•´ë³´ì„¸ìš”."
        
        # user_nick_nm í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì±„ì›Œì„œ ë°˜í™˜
        return {"tip": selected_tip.format(user_nick_nm=user_nick_nm)}

    except Exception as e:
        print(f"âŒ get_sleep_tip Error: {e}")
        return {"tip": "ìˆ˜ë©´ ìœ„ìƒë²•ì„ ì°¸ê³ í•´ë³´ì„¸ìš”."}
    


# ======================================================================
# ===     í–‰ë™ í™œì„±í™” ë¯¸ì…˜ ì œê³µ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.get("/dialogue/action-mission")
async def get_action_mission(
    personality: Optional[str] = None,
    user_nick_nm: Optional[str] = "ì¹œêµ¬",
    language_code: Optional[str] = 'ko'
):
    """ìš°ìš¸(neg_low) í´ëŸ¬ìŠ¤í„°ë¥¼ ìœ„í•œ í–‰ë™ ë¯¸ì…˜ì„ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not supabase:
        return {"mission": "ì°½ë¬¸ì„ ì—´ê³  1ë¶„ê°„ ë°”ê¹¥ ê³µê¸°ë¥¼ ì¬ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"}
    try:
        query = supabase.table("action_solutions").select("text").eq("language_code", language_code)
        if personality:
            query = query.eq("personality", personality)
        
        response = await run_in_threadpool(query.execute)
        missions = [row['text'] for row in response.data]
        
        if not missions:
            fallback_res = await run_in_threadpool(supabase.table("action_solutions").select("text").eq("personality", "prob_solver").execute)
            missions = [row['text'] for row in fallback_res.data]

        selected_mission = random.choice(missions) if missions else "ì ì‹œ ìë¦¬ì—ì„œ ì¼ì–´ë‚˜ êµ³ì€ ëª¸ì„ í’€ì–´ì£¼ì„¸ìš”."
        
        return {"mission": selected_mission.format(user_nick_nm=user_nick_nm)}

    except Exception as e:
        print(f"âŒ get_action_mission Error: {e}")
        return {"mission": "ì ì‹œ ìë¦¬ì—ì„œ ì¼ì–´ë‚˜ êµ³ì€ ëª¸ì„ í’€ì–´ì£¼ì„¸ìš”."}




# ======================================================================
# ===     í”¼ë“œë°± ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸     ===
# ======================================================================

@app.post("/solutions/feedback")
async def handle_solution_feedback(payload: FeedbackRequest):
    """
    ì†”ë£¨ì…˜ì— ëŒ€í•œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ ,
    'not_helpful'ì¸ ê²½ìš° negative_tagsë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    if not supabase:
        raise HTTPException(status_code=500, detail="Supabase client not initialized")

    try:
        # 1. ë¨¼ì € solution_feedback í…Œì´ë¸”ì— í”¼ë“œë°± ê¸°ë¡ì„ ì‚½ì…í•©ë‹ˆë‹¤.
        feedback_insert_query = supabase.table("solution_feedback").insert({
            "user_id": payload.user_id,
            "solution_id": payload.solution_id,
            "session_id": payload.session_id,
            "solution_type": payload.solution_type,
            "feedback": payload.feedback
        })
        await run_in_threadpool(feedback_insert_query.execute)

        # 2. ë§Œì•½ í”¼ë“œë°±ì´ 'not_helpful'ì´ë¼ë©´, íƒœê·¸ ì—…ë°ì´íŠ¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        if payload.feedback == 'not_helpful':
            # 2-1. ì‹«ì–´ìš” ëˆ„ë¥¸ ì†”ë£¨ì…˜ì˜ íƒœê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            solution_query = supabase.table("solutions").select("tags").eq("solution_id", payload.solution_id).single()
            solution_res = await run_in_threadpool(solution_query.execute)
            
            if solution_res.data and solution_res.data.get("tags"):
                solution_tags = solution_res.data["tags"]
                
                # 2-2. ì‚¬ìš©ìì˜ í˜„ì¬ negative_tagsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                profile_query = supabase.table("user_profiles").select("negative_tags").eq("id", payload.user_id).single()
                profile_res = await run_in_threadpool(profile_query.execute)
                
                current_tags = []
                if profile_res.data and profile_res.data.get("negative_tags"):
                    current_tags = profile_res.data["negative_tags"]
                
                # 2-3. ê¸°ì¡´ íƒœê·¸ì™€ ìƒˆë¡œìš´ íƒœê·¸ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
                updated_tags = list(set(current_tags) | set(solution_tags))
                
                # 2-4. user_profiles í…Œì´ë¸”ì— ì—…ë°ì´íŠ¸ëœ íƒœê·¸ ëª©ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.
                update_query = supabase.table("user_profiles").update({"negative_tags": updated_tags}).eq("id", payload.user_id)
                await run_in_threadpool(update_query.execute)
                
                print(f"âœ… User {payload.user_id} negative_tags updated: {updated_tags}")

        return {"message": "Feedback submitted successfully"}

    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ EXCEPTION in /solutions/feedback: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    
@app.post("/jobs/backfill")
async def run_backfill(payload: BackfillRequest):
    """
    ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì— ëŒ€í•´ ëª¨ë“  ì‚¬ìš©ìì˜ ì¼ì¼/ì£¼ê°„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        payload: BackfillRequest
            - start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
            - end_date: ë ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
    
    Returns:
        dict: ë°±í•„ ì‘ì—… ê²°ê³¼
    """
    try:
        from backfill_summaries import run_backfill as backfill_function
        result = await backfill_function(payload.start_date, payload.end_date)
        return result
    except Exception as e:
        tb = traceback.format_exc()
        print(f"ğŸ”¥ EXCEPTION in /jobs/backfill: {e}\n{tb}")
        raise HTTPException(status_code=500, detail={"error": str(e), "trace": tb})
    


